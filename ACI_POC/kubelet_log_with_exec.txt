Flag --non-masquerade-cidr has been deprecated, will be removed in a future version
Flag --keep-terminated-pod-volumes has been deprecated, will be removed in a future version
Flag --non-masquerade-cidr has been deprecated, will be removed in a future version
I1207 08:34:29.184431   13809 feature_gate.go:156] feature gates: map[Accelerators:true MountPropagation:true]
I1207 08:34:29.184717   13809 mount_linux.go:190] Detected OS without systemd
I1207 08:34:29.184736   13809 client.go:75] Connecting to docker on unix:///var/run/docker.sock
I1207 08:34:29.184745   13809 client.go:95] Start docker client with request timeout=2m0s
W1207 08:34:29.185620   13809 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
I1207 08:34:29.190455   13809 iptables.go:564] couldn't get iptables-restore version; assuming it doesn't support --wait
I1207 08:34:29.191820   13809 plugins.go:101] No cloud provider specified.
I1207 08:34:29.191844   13809 server.go:299] No cloud provider specified: "" from the config file: ""
I1207 08:34:29.192984   13809 loader.go:357] Config loaded from file /var/lib/kubelet/kubeconfig
I1207 08:34:29.197899   13809 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct"
W1207 08:34:29.215140   13809 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused
W1207 08:34:29.215280   13809 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
I1207 08:34:29.240106   13809 fs.go:139] Filesystem UUIDs: map[880a2828-910d-4022-a871-c1290ff1c84c:/dev/sdb1 b6adc449-5e3d-4331-ba6b-6e99a75fa48e:/dev/sda1]
I1207 08:34:29.240128   13809 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/sys/fs/cgroup major:0 minor:76 fsType:tmpfs blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:54 fsType:tmpfs blockSize:0} /dev/sda1:{mountpoint:/var/lib/docker/overlay2 major:8 minor:1 fsType:ext4 blockSize:0}]
I1207 08:34:29.241594   13809 manager.go:216] Machine: {NumCores:4 CpuFrequency:2095190 MemoryCapacity:16820432896 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:833e0926ee21aed71ec075d726cbcfe0 SystemUUID:B5D8B0ED-6A85-984A-93F2-380AE5B69544 BootID:784cf38e-b192-4511-91f4-38326e124304 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:76 Capacity:8410214400 Type:vfs Inodes:2053275 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:54 Capacity:67108864 Type:vfs Inodes:2053275 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:51976970240 Type:vfs Inodes:6400000 HasInodes:true} {Device:overlay DeviceMajor:0 DeviceMinor:46 Capacity:51976970240 Type:vfs Inodes:6400000 HasInodes:true}] DiskMap:map[8:16:{Name:sdb Major:8 Minor:16 Size:107374182400 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:53687091200 Scheduler:none}] NetworkDevices:[{Name:cbr0 MacAddress:0a:58:0a:f4:23:01 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:22:48:05:8d:6c Speed:50000 Mtu:1500}] Topology:[{Id:0 Memory:16820432896 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:1048576 Type:Unified Level:2}]} {Id:1 Threads:[2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:1048576 Type:Unified Level:2}]}] Caches:[{Size:37486592 Type:Unified Level:3}]}] CloudProvider:Azure InstanceType:Unknown InstanceID:B5D8B0ED-6A85-984A-93F2-380AE5B69544}
I1207 08:34:29.242417   13809 manager.go:222] Version: {KernelVersion:4.15.0-1113-azure ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.13.1 DockerAPIVersion:1.26 CadvisorVersion: CadvisorRevision:}
I1207 08:34:29.243003   13809 server.go:229] Sending events to api server.
I1207 08:34:29.243061   13809 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
I1207 08:34:29.247181   13809 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
I1207 08:34:29.247214   13809 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
I1207 08:34:29.247357   13809 container_manager_linux.go:288] Creating device plugin handler: false
I1207 08:34:29.247410   13809 oom_linux.go:65] attempting to set "/proc/self/oom_score_adj" to "-999"
I1207 08:34:29.247476   13809 server.go:686] Using root directory: /var/lib/kubelet
I1207 08:34:29.247522   13809 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
I1207 08:34:29.247584   13809 file.go:52] Watching path "/etc/kubernetes/manifests"
I1207 08:34:29.247599   13809 kubelet.go:283] Watching apiserver
I1207 08:34:29.247841   13809 config.go:282] Setting pods for source file
I1207 08:34:29.247922   13809 reflector.go:202] Starting reflector *v1.Pod (0s) from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47
I1207 08:34:29.248081   13809 reflector.go:240] Listing and watching *v1.Pod from k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47
I1207 08:34:29.248175   13809 reflector.go:202] Starting reflector *v1.Service (0s) from k8s.io/kubernetes/pkg/kubelet/kubelet.go:413
I1207 08:34:29.248201   13809 reflector.go:240] Listing and watching *v1.Service from k8s.io/kubernetes/pkg/kubelet/kubelet.go:413
I1207 08:34:29.248791   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-agentpool4-a14krc0l-17&resourceVersion=0
I1207 08:34:29.248812   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.248821   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.248831   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.250064   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/services?resourceVersion=0
I1207 08:34:29.250084   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.250092   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.250100   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.250251   13809 reflector.go:202] Starting reflector *v1.Node (0s) from k8s.io/kubernetes/pkg/kubelet/kubelet.go:422
I1207 08:34:29.250270   13809 reflector.go:240] Listing and watching *v1.Node from k8s.io/kubernetes/pkg/kubelet/kubelet.go:422
I1207 08:34:29.250446   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-agentpool4-a14krc0l-17&resourceVersion=0
I1207 08:34:29.250461   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.250479   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.250488   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.255036   13809 iptables.go:564] couldn't get iptables-restore version; assuming it doesn't support --wait
I1207 08:34:29.255728   13809 kubelet.go:517] Hairpin mode set to "promiscuous-bridge"
I1207 08:34:29.255974   13809 kubenet_linux.go:138] Using interface eth0 MTU 1500 as bridge MTU
I1207 08:34:29.257736   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubenet: SNAT for outbound traffic from cluster -m addrtype ! --dst-type LOCAL ! -d 10.0.0.0/8 -j MASQUERADE]
I1207 08:34:29.259651   13809 plugins.go:187] Loaded network plugin "kubenet"
W1207 08:34:29.260873   13809 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
I1207 08:34:29.264543   13809 iptables.go:564] couldn't get iptables-restore version; assuming it doesn't support --wait
I1207 08:34:29.265321   13809 kubenet_linux.go:138] Using interface eth0 MTU 1500 as bridge MTU
I1207 08:34:29.267003   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubenet: SNAT for outbound traffic from cluster -m addrtype ! --dst-type LOCAL ! -d 10.0.0.0/8 -j MASQUERADE]
I1207 08:34:29.268762   13809 plugins.go:187] Loaded network plugin "kubenet"
I1207 08:34:29.268789   13809 docker_service.go:207] Docker cri networking managed by kubenet
I1207 08:34:29.287337   13809 docker_service.go:212] Docker Info: &{ID:26DM:PJNH:6HPX:ZBHO:X65D:I2PC:PEDU:SZ47:CQ5F:CFI3:6IZU:3JPW Containers:13 ContainersRunning:8 ContainersPaused:0 ContainersStopped:5 Images:31 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[]} MemoryLimit:true SwapLimit:false KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:61 OomKillDisable:true NGoroutines:64 SystemTime:2021-12-07T08:34:29.275226662Z LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:1 KernelVersion:4.15.0-1113-azure OperatingSystem:Ubuntu 16.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc4202ae150 NCPU:4 MemTotal:16820432896 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:k8s-agentpool4-a14krc0l-17 Labels:[] ExperimentalBuild:false ServerVersion:1.13.1 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:0xc420884a00} LiveRestoreEnabled:true Isolation: InitBinary:docker-init ContainerdCommit:{ID:aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1 Expected:aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1} RuncCommit:{ID:N/A Expected:9df8b306d01f59d3a8029be411de015b7304dd8f} InitCommit:{ID:949e6fa Expected:949e6fa} SecurityOptions:[name=apparmor name=seccomp,profile=default]}
I1207 08:34:29.287451   13809 docker_service.go:225] Setting cgroupDriver to cgroupfs
I1207 08:34:29.291036   13809 docker_legacy.go:200] Unable to convert legacy container {ID:f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c Names:[] Image:runjivu/hyperkube-amd64:v1.8.4 ImageID:sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c Command:/hyperkube kubelet --enable-server --node-labels=kubernetes.io/role=agent,agentpool=agentpool4,kubernetes.azure.com/cluster=CAAS-PROD-KOREACENTRAL-LINUX-14 --v=7 --non-masquerade-cidr=10.0.0.0/8 --volume-plugin-dir=/etc/kubernetes/volumeplugins --image-pull-progress-deadline=30m --address=0.0.0.0 --allow-privileged=true --authorization-mode=AlwaysAllow --azure-container-registry-config= --cadvisor-port=0 --cgroups-per-qos=true --cloud-config= --cloud-provider= --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --enforce-node-allocatable=pods --event-qps=0 --eviction-hard=memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5% --feature-gates=Accelerators=true,MountPropagation=true --image-gc-high-threshold=85 --image-gc-low-threshold=80 --keep-terminated-pod-volumes=false --kubeconfig=/var/lib/kubelet/kubeconfig --max-pods=110 --network-plugin=kubenet --node-status-update-frequency=10s --non-masquerade-cidr=10.0.0.0/8 --pod-infra-container-image=k8s-gcrio.azureedge.net/pause-amd64:3.1 --pod-manifest-path=/etc/kubernetes/manifests   Created:1638866068 Ports:[] SizeRw:0 SizeRootFs:0 Labels:map[] State:running Status:Up Less than a second HostConfig:{NetworkMode:host} NetworkSettings:0xc42127e1c0 Mounts:[{Type:bind Name: Source:/srv/kubernetes Destination:/srv/kubernetes Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/var/lib/docker Destination:/var/lib/docker Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/containers Destination:/var/lib/containers Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/waagent/ManagedIdentity-Settings Destination:/var/lib/waagent/ManagedIdentity-Settings Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/dev Destination:/dev Driver: Mode: RW:true Propagation:} {Type:bind Name: Source:/var/lib/kubelet Destination:/var/lib/kubelet Driver: Mode:shared RW:true Propagation:shared} {Type:bind Name: Source:/sys Destination:/sys Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/etc/kubernetes/volumeplugins Destination:/etc/kubernetes/volumeplugins Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/etc/kubernetes Destination:/etc/kubernetes Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/var/log Destination:/var/log Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/run Destination:/var/run Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/sbin/d Destination:/sbin/d Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/tmp Destination:/tmp Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/cni Destination:/var/lib/cni Driver: Mode:rw RW:true Propagation:}]}: failed to parse Docker container name "laughing_swanson" into parts
I1207 08:34:29.293949   13809 docker_legacy.go:255] Unable to convert legacy container {ID:f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c Names:[] Image:runjivu/hyperkube-amd64:v1.8.4 ImageID:sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c Command:/hyperkube kubelet --enable-server --node-labels=kubernetes.io/role=agent,agentpool=agentpool4,kubernetes.azure.com/cluster=CAAS-PROD-KOREACENTRAL-LINUX-14 --v=7 --non-masquerade-cidr=10.0.0.0/8 --volume-plugin-dir=/etc/kubernetes/volumeplugins --image-pull-progress-deadline=30m --address=0.0.0.0 --allow-privileged=true --authorization-mode=AlwaysAllow --azure-container-registry-config= --cadvisor-port=0 --cgroups-per-qos=true --cloud-config= --cloud-provider= --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --enforce-node-allocatable=pods --event-qps=0 --eviction-hard=memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5% --feature-gates=Accelerators=true,MountPropagation=true --image-gc-high-threshold=85 --image-gc-low-threshold=80 --keep-terminated-pod-volumes=false --kubeconfig=/var/lib/kubelet/kubeconfig --max-pods=110 --network-plugin=kubenet --node-status-update-frequency=10s --non-masquerade-cidr=10.0.0.0/8 --pod-infra-container-image=k8s-gcrio.azureedge.net/pause-amd64:3.1 --pod-manifest-path=/etc/kubernetes/manifests   Created:1638866068 Ports:[] SizeRw:0 SizeRootFs:0 Labels:map[] State:running Status:Up Less than a second HostConfig:{NetworkMode:host} NetworkSettings:0xc42127e298 Mounts:[{Type:bind Name: Source:/var/log Destination:/var/log Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/run Destination:/var/run Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/sbin/d Destination:/sbin/d Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/tmp Destination:/tmp Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/cni Destination:/var/lib/cni Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/dev Destination:/dev Driver: Mode: RW:true Propagation:} {Type:bind Name: Source:/var/lib/kubelet Destination:/var/lib/kubelet Driver: Mode:shared RW:true Propagation:shared} {Type:bind Name: Source:/sys Destination:/sys Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/srv/kubernetes Destination:/srv/kubernetes Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/var/lib/docker Destination:/var/lib/docker Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/containers Destination:/var/lib/containers Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/var/lib/waagent/ManagedIdentity-Settings Destination:/var/lib/waagent/ManagedIdentity-Settings Driver: Mode:ro RW:false Propagation:} {Type:bind Name: Source:/etc/kubernetes/volumeplugins Destination:/etc/kubernetes/volumeplugins Driver: Mode:rw RW:true Propagation:} {Type:bind Name: Source:/etc/kubernetes Destination:/etc/kubernetes Driver: Mode:ro RW:false Propagation:}]}: failed to parse Docker container name "laughing_swanson" into parts
I1207 08:34:29.294040   13809 docker_legacy.go:151] No legacy containers found, stop performing legacy cleanup.
I1207 08:34:29.294075   13809 kubelet.go:605] RemoteRuntimeEndpoint: "unix:///var/run/dockershim.sock", RemoteImageEndpoint: "unix:///var/run/dockershim.sock"
I1207 08:34:29.294103   13809 kubelet.go:606] Starting the GRPC server for the docker CRI shim.
I1207 08:34:29.294193   13809 docker_server.go:51] Start dockershim grpc server
I1207 08:34:29.299253   13809 container_manager_linux.go:827] attempting to apply oom_score_adj of -999 to pid 1444
I1207 08:34:29.299335   13809 oom_linux.go:65] attempting to set "/proc/1444/oom_score_adj" to "-999"
I1207 08:34:29.299441   13809 container_manager_linux.go:827] attempting to apply oom_score_adj of -999 to pid 1950
I1207 08:34:29.299528   13809 oom_linux.go:65] attempting to set "/proc/1950/oom_score_adj" to "-999"
I1207 08:34:29.302139   13809 round_trippers.go:439] Response Status: 200 OK in 51 milliseconds
I1207 08:34:29.302825   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-agentpool4-a14krc0l-17&resourceVersion=1094485854&timeoutSeconds=527&watch=true
I1207 08:34:29.303011   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.303171   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.303283   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.305534   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:34:29.314838   13809 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
I1207 08:34:29.314927   13809 remote_image.go:40] Connecting to image service unix:///var/run/dockershim.sock
I1207 08:34:29.315061   13809 plugins.go:56] Registering credential provider: .dockercfg
I1207 08:34:29.325937   13809 round_trippers.go:439] Response Status: 200 OK in 75 milliseconds
I1207 08:34:29.326268   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/services?resourceVersion=1087296592&timeoutSeconds=393&watch=true
I1207 08:34:29.326285   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.326293   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.326300   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.328879   13809 round_trippers.go:439] Response Status: 200 OK in 80 milliseconds
I1207 08:34:29.329284   13809 config.go:282] Setting pods for source api
I1207 08:34:29.329331   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-agentpool4-a14krc0l-17&resourceVersion=1094485784&timeoutSeconds=348&watch=true
I1207 08:34:29.329343   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.329350   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.329357   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.329558   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:29.329927   13809 config.go:404] Receiving a new pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.329953   13809 config.go:404] Receiving a new pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.329962   13809 config.go:404] Receiving a new pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:34:29.329977   13809 config.go:404] Receiving a new pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.330789   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:34:29.332387   13809 azure_credentials.go:80] Azure config unspecified, disabling
I1207 08:34:29.333494   13809 kuberuntime_manager.go:178] Container runtime docker initialized, version: 1.13.1, apiVersion: 1.26.0
I1207 08:34:29.333918   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/aws-ebs"
I1207 08:34:29.333940   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/empty-dir"
I1207 08:34:29.333951   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/gce-pd"
I1207 08:34:29.333962   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/git-repo"
I1207 08:34:29.333979   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/host-path"
I1207 08:34:29.333997   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/nfs"
I1207 08:34:29.334018   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/secret"
I1207 08:34:29.334034   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/iscsi"
I1207 08:34:29.334048   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/glusterfs"
I1207 08:34:29.334060   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/rbd"
I1207 08:34:29.334079   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/cinder"
I1207 08:34:29.334087   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/quobyte"
I1207 08:34:29.334098   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/cephfs"
I1207 08:34:29.334113   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/downward-api"
I1207 08:34:29.334122   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/fc"
I1207 08:34:29.334130   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/flocker"
I1207 08:34:29.334148   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/azure-file"
I1207 08:34:29.334165   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/configmap"
I1207 08:34:29.334176   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/vsphere-volume"
I1207 08:34:29.334191   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/azure-disk"
I1207 08:34:29.334201   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/photon-pd"
I1207 08:34:29.334214   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/projected"
I1207 08:34:29.334225   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/portworx-volume"
I1207 08:34:29.334241   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/scaleio"
I1207 08:34:29.334305   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/local-volume"
I1207 08:34:29.334319   13809 plugins.go:420] Loaded volume plugin "kubernetes.io/storageos"
I1207 08:34:29.335057   13809 server.go:718] Started kubelet v1.8.4
E1207 08:34:29.335120   13809 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
I1207 08:34:29.335162   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.335211   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.335226   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.335232   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.335242   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.335248   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.335266   13809 server.go:128] Starting to listen on 0.0.0.0:10250
I1207 08:34:29.335314   13809 healthz.go:74] Installing healthz checkers:"ping", "syncloop"
I1207 08:34:29.335665   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'Starting' Starting kubelet.
I1207 08:34:29.335949   13809 server.go:148] Starting to listen read-only on 0.0.0.0:10255
I1207 08:34:29.336001   13809 healthz.go:74] Installing healthz checkers:"ping", "syncloop"
I1207 08:34:29.336459   13809 server.go:296] Adding debug handlers to kubelet server.
I1207 08:34:29.337196   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:29.337287   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01c927c050", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:335031888, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:335031888, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.343221   13809 mount_linux.go:600] Directory /var/lib/kubelet is already on a shared mount
I1207 08:34:29.343249   13809 kubelet.go:1222] Container garbage collection succeeded
I1207 08:34:29.348630   13809 node_container_manager.go:70] Attempting to enforce Node Allocatable with config: {KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]}
E1207 08:34:29.348730   13809 kubelet.go:1292] Failed to start gpuManager stat /dev/nvidiactl: no such file or directory
I1207 08:34:29.348761   13809 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
I1207 08:34:29.348779   13809 status_manager.go:140] Starting to sync pod status with apiserver
I1207 08:34:29.348837   13809 container_manager_linux.go:440] [ContainerManager]: Adding periodic tasks for docker CRI integration
I1207 08:34:29.348923   13809 container_manager_linux.go:446] [ContainerManager]: Discovered runtime cgroups name: /system.slice/docker.service
I1207 08:34:29.348950   13809 qos_container_manager_linux.go:320] [ContainerManager]: Updated QoS cgroup configuration
I1207 08:34:29.348960   13809 container_manager_linux.go:827] attempting to apply oom_score_adj of -999 to pid 13809
I1207 08:34:29.348969   13809 oom_linux.go:65] attempting to set "/proc/13809/oom_score_adj" to "-999"
I1207 08:34:29.349011   13809 volume_manager.go:244] The desired_state_of_world populator starts
I1207 08:34:29.349022   13809 volume_manager.go:246] Starting Kubelet Volume Manager
I1207 08:34:29.349051   13809 kubelet.go:1768] Starting kubelet main sync loop.
I1207 08:34:29.349340   13809 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
I1207 08:34:29.349308   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeAllocatableEnforced' Updated Node Allocatable limit across pods
I1207 08:34:29.349466   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.349497   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.349512   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.349520   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.349528   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
E1207 08:34:29.349168   13809 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
I1207 08:34:29.349272   13809 iptables.go:396] running iptables -N [KUBE-MARK-DROP -t nat]
I1207 08:34:29.349731   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:29.351554   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:29.351692   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01ca009e22", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:349244450, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:349244450, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.352782   13809 iptables.go:396] running iptables -C [KUBE-MARK-DROP -t nat -j MARK --set-xmark 0x00008000/0x00008000]
I1207 08:34:29.353494   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR
E1207 08:34:29.353818   13809 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR
I1207 08:34:29.358530   13809 iptables.go:396] running iptables -N [KUBE-FIREWALL -t filter]
I1207 08:34:29.360331   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47: non-existent -> exited
I1207 08:34:29.360365   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864: non-existent -> running
I1207 08:34:29.360371   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731: non-existent -> running
I1207 08:34:29.360377   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14: non-existent -> exited
I1207 08:34:29.360382   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479: non-existent -> exited
I1207 08:34:29.360387   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b: non-existent -> running
I1207 08:34:29.360393   13809 generic.go:146] GenericPLEG: 7df95486-5737-11ec-9d1e-002248057bde/efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863: non-existent -> running
I1207 08:34:29.360399   13809 generic.go:146] GenericPLEG: 7df95486-5737-11ec-9d1e-002248057bde/e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826: non-existent -> running
I1207 08:34:29.360405   13809 generic.go:146] GenericPLEG: 83d280aa-8ff5-11ea-a482-002248057bde/a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844: non-existent -> exited
I1207 08:34:29.360410   13809 generic.go:146] GenericPLEG: 83d280aa-8ff5-11ea-a482-002248057bde/a9ec477cf43e44591814f19311555d74e884af3490302c58f069cfdfbe31c732: non-existent -> exited
I1207 08:34:29.360416   13809 generic.go:146] GenericPLEG: 7c988aa2-5737-11ec-9d1e-002248057bde/74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c: non-existent -> running
I1207 08:34:29.360421   13809 generic.go:146] GenericPLEG: 7c988aa2-5737-11ec-9d1e-002248057bde/b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c: non-existent -> running
I1207 08:34:29.362999   13809 iptables.go:396] running iptables -C [KUBE-FIREWALL -t filter -m comment --comment kubernetes firewall for dropping marked packets -m mark --mark 0x00008000/0x00008000 -j DROP]
I1207 08:34:29.364421   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.370511   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd, container logger uses image mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd(sha256:bb32f6f7f4c64ffefd4cbb6dcf092316bc3aaadcdd4c71b4adcfa598631636be)
I1207 08:34:29.370532   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod, container c1 uses image runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc(sha256:d84cbef41193e9e1cfd9e14852331048b5c83425a17de43cc8a9e62bb17df2b9)
I1207 08:34:29.370538   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod, container c5 uses image runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0(sha256:e4733421cd8c24d2fc70b62a227ed80a1817308645719b03c9749c90927cf17e)
I1207 08:34:29.370545   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod, container c4 uses image runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668(sha256:aab4d062ed6fa55bdf3f79ab03e9c5a0a5bf46d6a933fc30ab39f588f657bc36)
I1207 08:34:29.370551   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod, container c3 uses image k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded(sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c)
I1207 08:34:29.370556   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod, container c2 uses image k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded(sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c)
I1207 08:34:29.370562   13809 image_gc_manager.go:211] Pod caas-20d222f17dd4489fb180b51031b406f6/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf, container infra uses image mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5(sha256:c6146c587a3a64c05fdfd3f4c0384174d2f575158985d8e962003dea4fadb4c7)
I1207 08:34:29.370567   13809 image_gc_manager.go:211] Pod kube-system/kube-proxy-wqgxc, container kube-proxy uses image k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded(sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c)
I1207 08:34:29.370579   13809 image_gc_manager.go:222] Adding image ID sha256:d84cbef41193e9e1cfd9e14852331048b5c83425a17de43cc8a9e62bb17df2b9 to currentImages
I1207 08:34:29.370588   13809 image_gc_manager.go:227] Image ID sha256:d84cbef41193e9e1cfd9e14852331048b5c83425a17de43cc8a9e62bb17df2b9 is new
I1207 08:34:29.370599   13809 image_gc_manager.go:235] Setting Image ID sha256:d84cbef41193e9e1cfd9e14852331048b5c83425a17de43cc8a9e62bb17df2b9 lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.370618   13809 image_gc_manager.go:239] Image ID sha256:d84cbef41193e9e1cfd9e14852331048b5c83425a17de43cc8a9e62bb17df2b9 has size 516247996
I1207 08:34:29.370624   13809 image_gc_manager.go:222] Adding image ID sha256:c6146c587a3a64c05fdfd3f4c0384174d2f575158985d8e962003dea4fadb4c7 to currentImages
I1207 08:34:29.370631   13809 image_gc_manager.go:227] Image ID sha256:c6146c587a3a64c05fdfd3f4c0384174d2f575158985d8e962003dea4fadb4c7 is new
I1207 08:34:29.370638   13809 image_gc_manager.go:235] Setting Image ID sha256:c6146c587a3a64c05fdfd3f4c0384174d2f575158985d8e962003dea4fadb4c7 lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.370645   13809 image_gc_manager.go:239] Image ID sha256:c6146c587a3a64c05fdfd3f4c0384174d2f575158985d8e962003dea4fadb4c7 has size 31742437
I1207 08:34:29.370651   13809 image_gc_manager.go:222] Adding image ID sha256:f09bfe1b1fb485d1a698a6a521058430fa92ac0b417d196999dd2db9e272aba6 to currentImages
I1207 08:34:29.370657   13809 image_gc_manager.go:227] Image ID sha256:f09bfe1b1fb485d1a698a6a521058430fa92ac0b417d196999dd2db9e272aba6 is new
I1207 08:34:29.370662   13809 image_gc_manager.go:239] Image ID sha256:f09bfe1b1fb485d1a698a6a521058430fa92ac0b417d196999dd2db9e272aba6 has size 31740535
I1207 08:34:29.370666   13809 image_gc_manager.go:222] Adding image ID sha256:c69fdb7ffe6926deca043211e8963f9e561951da7bf2c5d493e5481528c92bcf to currentImages
I1207 08:34:29.370672   13809 image_gc_manager.go:227] Image ID sha256:c69fdb7ffe6926deca043211e8963f9e561951da7bf2c5d493e5481528c92bcf is new
I1207 08:34:29.370677   13809 image_gc_manager.go:239] Image ID sha256:c69fdb7ffe6926deca043211e8963f9e561951da7bf2c5d493e5481528c92bcf has size 31740517
I1207 08:34:29.370681   13809 image_gc_manager.go:222] Adding image ID sha256:e178428d4a3c008f455a7a7c44a8a39cfac5790b7abbecd24f3c63e611e84d34 to currentImages
I1207 08:34:29.370687   13809 image_gc_manager.go:227] Image ID sha256:e178428d4a3c008f455a7a7c44a8a39cfac5790b7abbecd24f3c63e611e84d34 is new
I1207 08:34:29.370692   13809 image_gc_manager.go:239] Image ID sha256:e178428d4a3c008f455a7a7c44a8a39cfac5790b7abbecd24f3c63e611e84d34 has size 283076574
I1207 08:34:29.370696   13809 image_gc_manager.go:222] Adding image ID sha256:a8858586bbe94f541b3f76f0e65bce27922a80bbd81040df455137a0c065b7ec to currentImages
I1207 08:34:29.370702   13809 image_gc_manager.go:227] Image ID sha256:a8858586bbe94f541b3f76f0e65bce27922a80bbd81040df455137a0c065b7ec is new
I1207 08:34:29.370707   13809 image_gc_manager.go:239] Image ID sha256:a8858586bbe94f541b3f76f0e65bce27922a80bbd81040df455137a0c065b7ec has size 17035279
I1207 08:34:29.370711   13809 image_gc_manager.go:222] Adding image ID sha256:a241444465052b309895b4d5b77e2de17d19a36a9fb69aeb5cb1d934de09f589 to currentImages
I1207 08:34:29.370717   13809 image_gc_manager.go:227] Image ID sha256:a241444465052b309895b4d5b77e2de17d19a36a9fb69aeb5cb1d934de09f589 is new
I1207 08:34:29.370722   13809 image_gc_manager.go:239] Image ID sha256:a241444465052b309895b4d5b77e2de17d19a36a9fb69aeb5cb1d934de09f589 has size 225280876
I1207 08:34:29.370727   13809 image_gc_manager.go:222] Adding image ID sha256:46c1b808c34b512ac222034df3df543a00a69a93009c82941b3957e9762085b8 to currentImages
I1207 08:34:29.370732   13809 image_gc_manager.go:227] Image ID sha256:46c1b808c34b512ac222034df3df543a00a69a93009c82941b3957e9762085b8 is new
I1207 08:34:29.370737   13809 image_gc_manager.go:239] Image ID sha256:46c1b808c34b512ac222034df3df543a00a69a93009c82941b3957e9762085b8 has size 207502991
I1207 08:34:29.370742   13809 image_gc_manager.go:222] Adding image ID sha256:3a203bc9f6a4c9450275860dfd1a03c9862887a48d542709d37ae825c0ff7d3e to currentImages
I1207 08:34:29.370749   13809 image_gc_manager.go:227] Image ID sha256:3a203bc9f6a4c9450275860dfd1a03c9862887a48d542709d37ae825c0ff7d3e is new
I1207 08:34:29.370756   13809 image_gc_manager.go:239] Image ID sha256:3a203bc9f6a4c9450275860dfd1a03c9862887a48d542709d37ae825c0ff7d3e has size 189657744
I1207 08:34:29.370760   13809 image_gc_manager.go:222] Adding image ID sha256:7560c294effe4f208123231e1d611cb03b04d92604e08ab1fe8d0a8d2275339f to currentImages
I1207 08:34:29.370766   13809 image_gc_manager.go:227] Image ID sha256:7560c294effe4f208123231e1d611cb03b04d92604e08ab1fe8d0a8d2275339f is new
I1207 08:34:29.370771   13809 image_gc_manager.go:239] Image ID sha256:7560c294effe4f208123231e1d611cb03b04d92604e08ab1fe8d0a8d2275339f has size 682696
I1207 08:34:29.370775   13809 image_gc_manager.go:222] Adding image ID sha256:bb32f6f7f4c64ffefd4cbb6dcf092316bc3aaadcdd4c71b4adcfa598631636be to currentImages
I1207 08:34:29.370780   13809 image_gc_manager.go:227] Image ID sha256:bb32f6f7f4c64ffefd4cbb6dcf092316bc3aaadcdd4c71b4adcfa598631636be is new
I1207 08:34:29.370789   13809 image_gc_manager.go:235] Setting Image ID sha256:bb32f6f7f4c64ffefd4cbb6dcf092316bc3aaadcdd4c71b4adcfa598631636be lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.370797   13809 image_gc_manager.go:239] Image ID sha256:bb32f6f7f4c64ffefd4cbb6dcf092316bc3aaadcdd4c71b4adcfa598631636be has size 236786136
I1207 08:34:29.370801   13809 image_gc_manager.go:222] Adding image ID sha256:adafef2e596ef06ec2112bc5a9663c6a4f59a3dfd4243c9cabe06c8748e7f288 to currentImages
I1207 08:34:29.370806   13809 image_gc_manager.go:227] Image ID sha256:adafef2e596ef06ec2112bc5a9663c6a4f59a3dfd4243c9cabe06c8748e7f288 is new
I1207 08:34:29.370811   13809 image_gc_manager.go:239] Image ID sha256:adafef2e596ef06ec2112bc5a9663c6a4f59a3dfd4243c9cabe06c8748e7f288 has size 73858282
I1207 08:34:29.370816   13809 image_gc_manager.go:222] Adding image ID sha256:74435f89ab7825e19cf8c92c7b5c5ebd73ae2d0a2be16f49b3fb81c9062ab303 to currentImages
I1207 08:34:29.370821   13809 image_gc_manager.go:227] Image ID sha256:74435f89ab7825e19cf8c92c7b5c5ebd73ae2d0a2be16f49b3fb81c9062ab303 is new
I1207 08:34:29.370827   13809 image_gc_manager.go:239] Image ID sha256:74435f89ab7825e19cf8c92c7b5c5ebd73ae2d0a2be16f49b3fb81c9062ab303 has size 73856440
I1207 08:34:29.370831   13809 image_gc_manager.go:222] Adding image ID sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e to currentImages
I1207 08:34:29.370838   13809 image_gc_manager.go:227] Image ID sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e is new
I1207 08:34:29.370848   13809 image_gc_manager.go:239] Image ID sha256:a24bb4013296f61e89ba57005a7b3e52274d8edd3ae2077d04395f806b63d83e has size 5574537
I1207 08:34:29.370853   13809 image_gc_manager.go:222] Adding image ID sha256:725d4a62140d2702ea936a5f19ca685811c56687d5afe54aea59665da9aa304d to currentImages
I1207 08:34:29.370858   13809 image_gc_manager.go:227] Image ID sha256:725d4a62140d2702ea936a5f19ca685811c56687d5afe54aea59665da9aa304d is new
I1207 08:34:29.370865   13809 image_gc_manager.go:239] Image ID sha256:725d4a62140d2702ea936a5f19ca685811c56687d5afe54aea59665da9aa304d has size 1050662074
I1207 08:34:29.370870   13809 image_gc_manager.go:222] Adding image ID sha256:45fd62493f100cbf6fae0fd72c6662240eead0f16f5b1b63127f195c024d5bc8 to currentImages
I1207 08:34:29.370876   13809 image_gc_manager.go:227] Image ID sha256:45fd62493f100cbf6fae0fd72c6662240eead0f16f5b1b63127f195c024d5bc8 is new
I1207 08:34:29.370883   13809 image_gc_manager.go:239] Image ID sha256:45fd62493f100cbf6fae0fd72c6662240eead0f16f5b1b63127f195c024d5bc8 has size 15187302
I1207 08:34:29.370888   13809 image_gc_manager.go:222] Adding image ID sha256:e49827d3e4b8b6cec8b3a481fe71af360b5a5e799860206d9590d986512d2a1f to currentImages
I1207 08:34:29.370894   13809 image_gc_manager.go:227] Image ID sha256:e49827d3e4b8b6cec8b3a481fe71af360b5a5e799860206d9590d986512d2a1f is new
I1207 08:34:29.370899   13809 image_gc_manager.go:239] Image ID sha256:e49827d3e4b8b6cec8b3a481fe71af360b5a5e799860206d9590d986512d2a1f has size 246312714
I1207 08:34:29.370904   13809 image_gc_manager.go:222] Adding image ID sha256:ea9aa31b6428318f063b09cb4c192c3638cbff593a5350c26b0c60050133c5df to currentImages
I1207 08:34:29.370909   13809 image_gc_manager.go:227] Image ID sha256:ea9aa31b6428318f063b09cb4c192c3638cbff593a5350c26b0c60050133c5df is new
I1207 08:34:29.370915   13809 image_gc_manager.go:239] Image ID sha256:ea9aa31b6428318f063b09cb4c192c3638cbff593a5350c26b0c60050133c5df has size 10544350
I1207 08:34:29.370919   13809 image_gc_manager.go:222] Adding image ID sha256:e497dabd8450c0e1dd0ad318367cc83bc7c109780904ea16f24d03d81d4b90f6 to currentImages
I1207 08:34:29.370925   13809 image_gc_manager.go:227] Image ID sha256:e497dabd8450c0e1dd0ad318367cc83bc7c109780904ea16f24d03d81d4b90f6 is new
I1207 08:34:29.370929   13809 image_gc_manager.go:239] Image ID sha256:e497dabd8450c0e1dd0ad318367cc83bc7c109780904ea16f24d03d81d4b90f6 has size 917985267
I1207 08:34:29.370937   13809 image_gc_manager.go:222] Adding image ID sha256:f32a97de94e13d29835a19851acd6cbc7979d1d50f703725541e44bb89a1ce91 to currentImages
I1207 08:34:29.370943   13809 image_gc_manager.go:227] Image ID sha256:f32a97de94e13d29835a19851acd6cbc7979d1d50f703725541e44bb89a1ce91 is new
I1207 08:34:29.370948   13809 image_gc_manager.go:239] Image ID sha256:f32a97de94e13d29835a19851acd6cbc7979d1d50f703725541e44bb89a1ce91 has size 25779561
I1207 08:34:29.370953   13809 image_gc_manager.go:222] Adding image ID sha256:e4733421cd8c24d2fc70b62a227ed80a1817308645719b03c9749c90927cf17e to currentImages
I1207 08:34:29.370958   13809 image_gc_manager.go:227] Image ID sha256:e4733421cd8c24d2fc70b62a227ed80a1817308645719b03c9749c90927cf17e is new
I1207 08:34:29.370964   13809 image_gc_manager.go:235] Setting Image ID sha256:e4733421cd8c24d2fc70b62a227ed80a1817308645719b03c9749c90927cf17e lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.370971   13809 image_gc_manager.go:239] Image ID sha256:e4733421cd8c24d2fc70b62a227ed80a1817308645719b03c9749c90927cf17e has size 656817138
I1207 08:34:29.370976   13809 image_gc_manager.go:222] Adding image ID sha256:efa6f1f5535708120a1798f630ee62831f89a7cb2bd44e3f6e3841e7561fab92 to currentImages
I1207 08:34:29.370982   13809 image_gc_manager.go:227] Image ID sha256:efa6f1f5535708120a1798f630ee62831f89a7cb2bd44e3f6e3841e7561fab92 is new
I1207 08:34:29.370987   13809 image_gc_manager.go:239] Image ID sha256:efa6f1f5535708120a1798f630ee62831f89a7cb2bd44e3f6e3841e7561fab92 has size 1727945252
I1207 08:34:29.370991   13809 image_gc_manager.go:222] Adding image ID sha256:273654f07fb521d95969e15d4906a901949dfd3ce3d9d0b4df3fd53ff8ee3e8c to currentImages
I1207 08:34:29.370997   13809 image_gc_manager.go:227] Image ID sha256:273654f07fb521d95969e15d4906a901949dfd3ce3d9d0b4df3fd53ff8ee3e8c is new
I1207 08:34:29.371001   13809 image_gc_manager.go:239] Image ID sha256:273654f07fb521d95969e15d4906a901949dfd3ce3d9d0b4df3fd53ff8ee3e8c has size 1466472365
I1207 08:34:29.371006   13809 image_gc_manager.go:222] Adding image ID sha256:70c1124a9a02572960e8d48a3f793226828d09349fedc6521cb05112f6c5ba7d to currentImages
I1207 08:34:29.371011   13809 image_gc_manager.go:227] Image ID sha256:70c1124a9a02572960e8d48a3f793226828d09349fedc6521cb05112f6c5ba7d is new
I1207 08:34:29.371017   13809 image_gc_manager.go:239] Image ID sha256:70c1124a9a02572960e8d48a3f793226828d09349fedc6521cb05112f6c5ba7d has size 254608226
I1207 08:34:29.371023   13809 image_gc_manager.go:222] Adding image ID sha256:179ac5ebc664e765d379a508fb2a96ce4e487a3a925604829af5a40475723aee to currentImages
I1207 08:34:29.371028   13809 image_gc_manager.go:227] Image ID sha256:179ac5ebc664e765d379a508fb2a96ce4e487a3a925604829af5a40475723aee is new
I1207 08:34:29.371033   13809 image_gc_manager.go:239] Image ID sha256:179ac5ebc664e765d379a508fb2a96ce4e487a3a925604829af5a40475723aee has size 180183766
I1207 08:34:29.371037   13809 image_gc_manager.go:222] Adding image ID sha256:72ca45cd8858169f347945cdbe4f1f27e6038d16384f432c196dd93da48f70e9 to currentImages
I1207 08:34:29.371043   13809 image_gc_manager.go:227] Image ID sha256:72ca45cd8858169f347945cdbe4f1f27e6038d16384f432c196dd93da48f70e9 is new
I1207 08:34:29.371047   13809 image_gc_manager.go:239] Image ID sha256:72ca45cd8858169f347945cdbe4f1f27e6038d16384f432c196dd93da48f70e9 has size 667718926
I1207 08:34:29.371052   13809 image_gc_manager.go:222] Adding image ID sha256:aab4d062ed6fa55bdf3f79ab03e9c5a0a5bf46d6a933fc30ab39f588f657bc36 to currentImages
I1207 08:34:29.371060   13809 image_gc_manager.go:227] Image ID sha256:aab4d062ed6fa55bdf3f79ab03e9c5a0a5bf46d6a933fc30ab39f588f657bc36 is new
I1207 08:34:29.371071   13809 image_gc_manager.go:235] Setting Image ID sha256:aab4d062ed6fa55bdf3f79ab03e9c5a0a5bf46d6a933fc30ab39f588f657bc36 lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.371079   13809 image_gc_manager.go:239] Image ID sha256:aab4d062ed6fa55bdf3f79ab03e9c5a0a5bf46d6a933fc30ab39f588f657bc36 has size 651557048
I1207 08:34:29.371085   13809 image_gc_manager.go:222] Adding image ID sha256:7aa3602ab41ea3384904197455e66f6435cb0261bd62a06db1d8e76cb8960c42 to currentImages
I1207 08:34:29.371091   13809 image_gc_manager.go:227] Image ID sha256:7aa3602ab41ea3384904197455e66f6435cb0261bd62a06db1d8e76cb8960c42 is new
I1207 08:34:29.371097   13809 image_gc_manager.go:239] Image ID sha256:7aa3602ab41ea3384904197455e66f6435cb0261bd62a06db1d8e76cb8960c42 has size 114808794
I1207 08:34:29.371102   13809 image_gc_manager.go:222] Adding image ID sha256:da579b235e92c861790f69d621d4601f54892bef262fa3fc38e3637cce406126 to currentImages
I1207 08:34:29.371108   13809 image_gc_manager.go:227] Image ID sha256:da579b235e92c861790f69d621d4601f54892bef262fa3fc38e3637cce406126 is new
I1207 08:34:29.371113   13809 image_gc_manager.go:239] Image ID sha256:da579b235e92c861790f69d621d4601f54892bef262fa3fc38e3637cce406126 has size 4027841
I1207 08:34:29.371118   13809 image_gc_manager.go:222] Adding image ID sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e to currentImages
I1207 08:34:29.371123   13809 image_gc_manager.go:227] Image ID sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e is new
I1207 08:34:29.371128   13809 image_gc_manager.go:239] Image ID sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e has size 742472
I1207 08:34:29.371133   13809 image_gc_manager.go:222] Adding image ID sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c to currentImages
I1207 08:34:29.371138   13809 image_gc_manager.go:227] Image ID sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c is new
I1207 08:34:29.371143   13809 image_gc_manager.go:235] Setting Image ID sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c lastUsed to 2021-12-07 08:34:29.370574343 +0000 UTC
I1207 08:34:29.371149   13809 image_gc_manager.go:239] Image ID sha256:9c7535a15d66b895fb6312a1414d59048e7771ce76375cb6f68890fb0e9c8e0c has size 512188575
I1207 08:34:29.371411   13809 iptables.go:396] running iptables -C [OUTPUT -t filter -j KUBE-FIREWALL]
I1207 08:34:29.388582   13809 factory.go:355] Registering Docker factory
W1207 08:34:29.388619   13809 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused
W1207 08:34:29.388780   13809 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
I1207 08:34:29.388801   13809 factory.go:54] Registering systemd factory
I1207 08:34:29.391105   13809 factory.go:86] Registering Raw factory
I1207 08:34:29.395369   13809 manager.go:1140] Started watching for new ooms in manager
I1207 08:34:29.395455   13809 factory.go:116] Factory "docker" was unable to handle container "/"
I1207 08:34:29.395514   13809 factory.go:105] Error trying to work out if we can handle /: / not handled by systemd handler
I1207 08:34:29.395523   13809 factory.go:116] Factory "systemd" was unable to handle container "/"
I1207 08:34:29.395535   13809 factory.go:112] Using factory "raw" for container "/"
I1207 08:34:29.395953   13809 manager.go:932] Added container: "/" (aliases: [], namespace: "")
I1207 08:34:29.396218   13809 handler.go:325] Added event &{/ 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.396278   13809 manager.go:311] Starting recovery of all containers
I1207 08:34:29.397511   13809 container.go:409] Start housekeeping for container "/"
I1207 08:34:29.399948   13809 iptables.go:396] running iptables -C [INPUT -t filter -j KUBE-FIREWALL]
I1207 08:34:29.402398   13809 iptables.go:396] running iptables -N [KUBE-MARK-MASQ -t nat]
I1207 08:34:29.405074   13809 iptables.go:396] running iptables -N [KUBE-POSTROUTING -t nat]
I1207 08:34:29.413427   13809 iptables.go:396] running iptables -C [KUBE-MARK-MASQ -t nat -j MARK --set-xmark 0x00004000/0x00004000]
I1207 08:34:29.419330   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]
I1207 08:34:29.421622   13809 iptables.go:396] running iptables -C [KUBE-POSTROUTING -t nat -m comment --comment kubernetes service traffic requiring SNAT -m mark --mark 0x00004000/0x00004000 -j MASQUERADE]
I1207 08:34:29.423814   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc42083c8c0), (*container.ContainerStatus)(0xc420a62000), (*container.ContainerStatus)(0xc420a620e0), (*container.ContainerStatus)(0xc420a621c0), (*container.ContainerStatus)(0xc42083c9a0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4215cd6d0)}} (err: <nil>)
I1207 08:34:29.425306   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826"] for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.433052   13809 generic.go:345] PLEG: Write status for caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7df95486-5737-11ec-9d1e-002248057bde", Name:"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.253", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc42083cc40)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4210f1220)}} (err: <nil>)
I1207 08:34:29.434008   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["a9ec477cf43e44591814f19311555d74e884af3490302c58f069cfdfbe31c732"] for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:34:29.437952   13809 generic.go:345] PLEG: Write status for kube-proxy-wqgxc/kube-system: &container.PodStatus{ID:"83d280aa-8ff5-11ea-a482-002248057bde", Name:"kube-proxy-wqgxc", Namespace:"kube-system", IP:"", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4212e2380)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc420737360)}} (err: <nil>)
I1207 08:34:29.438651   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c"] for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:29.443341   13809 generic.go:345] PLEG: Write status for shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c988aa2-5737-11ec-9d1e-002248057bde", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc42083d180)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc420015400)}} (err: <nil>)
I1207 08:34:29.449147   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.449163   13809 kubelet_node_status.go:280] Setting node annotation to enable volume controller attach/detach
I1207 08:34:29.455170   13809 kubelet_node_status.go:443] Recording NodeHasSufficientDisk event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.455209   13809 kubelet_node_status.go:443] Recording NodeHasSufficientMemory event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.455239   13809 kubelet_node_status.go:443] Recording NodeHasNoDiskPressure event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.455274   13809 kubelet_node_status.go:83] Attempting to register node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.455469   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/nodes
I1207 08:34:29.455483   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.455492   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.455492   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientDisk' Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientDisk
I1207 08:34:29.455593   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientMemory' Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientMemory
I1207 08:34:29.455656   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasNoDiskPressure' Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasNoDiskPressure
I1207 08:34:29.455558   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.455742   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.455481   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.455828   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.455837   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.455845   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.455866   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.457716   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:29.457806   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d0513db3", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455191475, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455191475, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.457962   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.457973   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.457978   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.457983   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.457988   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.459317   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:29.459524   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d051d2ed", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455229677, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455229677, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.459665   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.459694   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.459707   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.459730   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.459759   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.461164   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
I1207 08:34:29.461239   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/docker.service"
I1207 08:34:29.461291   13809 factory.go:105] Error trying to work out if we can handle /system.slice/docker.service: /system.slice/docker.service not handled by systemd handler
I1207 08:34:29.461307   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/docker.service"
I1207 08:34:29.461315   13809 factory.go:112] Using factory "raw" for container "/system.slice/docker.service"
E1207 08:34:29.461335   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d0525247", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455262279, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455262279, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.461527   13809 manager.go:932] Added container: "/system.slice/docker.service" (aliases: [], namespace: "")
I1207 08:34:29.461679   13809 handler.go:325] Added event &{/system.slice/docker.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.461706   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:34:29.461725   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:34:29.461736   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:34:29.461776   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/snapd.apparmor.service"
I1207 08:34:29.461798   13809 factory.go:105] Error trying to work out if we can handle /system.slice/snapd.apparmor.service: /system.slice/snapd.apparmor.service not handled by systemd handler
I1207 08:34:29.461815   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/snapd.apparmor.service"
I1207 08:34:29.461829   13809 factory.go:112] Using factory "raw" for container "/system.slice/snapd.apparmor.service"
I1207 08:34:29.461906   13809 container.go:409] Start housekeeping for container "/system.slice/docker.service"
I1207 08:34:29.462021   13809 manager.go:932] Added container: "/system.slice/snapd.apparmor.service" (aliases: [], namespace: "")
I1207 08:34:29.462205   13809 handler.go:325] Added event &{/system.slice/snapd.apparmor.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.462243   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:34:29.462274   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:34:29.462300   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:34:29.462340   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.462364   13809 container.go:409] Start housekeeping for container "/system.slice/snapd.apparmor.service"
I1207 08:34:29.462366   13809 factory.go:105] Error trying to work out if we can handle /kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde: /kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde not handled by systemd handler
I1207 08:34:29.462509   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.462525   13809 factory.go:112] Using factory "raw" for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.462733   13809 manager.go:932] Added container: "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde" (aliases: [], namespace: "")
I1207 08:34:29.462913   13809 handler.go:325] Added event &{/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde 2021-12-07 08:27:19.028356349 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.462944   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mdadm.service"
I1207 08:34:29.462966   13809 factory.go:105] Error trying to work out if we can handle /system.slice/mdadm.service: /system.slice/mdadm.service not handled by systemd handler
I1207 08:34:29.462983   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/mdadm.service"
I1207 08:34:29.462989   13809 factory.go:112] Using factory "raw" for container "/system.slice/mdadm.service"
I1207 08:34:29.463050   13809 container.go:409] Start housekeeping for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.463157   13809 manager.go:932] Added container: "/system.slice/mdadm.service" (aliases: [], namespace: "")
I1207 08:34:29.463357   13809 handler.go:325] Added event &{/system.slice/mdadm.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.463393   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.463416   13809 factory.go:105] Error trying to work out if we can handle /kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde: /kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde not handled by systemd handler
I1207 08:34:29.463435   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.463452   13809 factory.go:112] Using factory "raw" for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.463538   13809 container.go:409] Start housekeeping for container "/system.slice/mdadm.service"
I1207 08:34:29.463712   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde" (aliases: [], namespace: "")
I1207 08:34:29.463900   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde 2021-12-07 08:27:16.688212721 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.463926   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:34:29.463947   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:34:29.463967   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:34:29.463976   13809 factory.go:116] Factory "docker" was unable to handle container "/user.slice"
I1207 08:34:29.464001   13809 factory.go:105] Error trying to work out if we can handle /user.slice: /user.slice not handled by systemd handler
I1207 08:34:29.464012   13809 factory.go:116] Factory "systemd" was unable to handle container "/user.slice"
I1207 08:34:29.464018   13809 factory.go:112] Using factory "raw" for container "/user.slice"
I1207 08:34:29.464113   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.464217   13809 manager.go:932] Added container: "/user.slice" (aliases: [], namespace: "")
I1207 08:34:29.464348   13809 handler.go:325] Added event &{/user.slice 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.464389   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:34:29.464402   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:34:29.464411   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:34:29.464428   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-udevd.service"
I1207 08:34:29.464453   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-udevd.service: /system.slice/systemd-udevd.service not handled by systemd handler
I1207 08:34:29.464473   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-udevd.service"
I1207 08:34:29.464493   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-udevd.service"
I1207 08:34:29.464532   13809 container.go:409] Start housekeeping for container "/user.slice"
I1207 08:34:29.464729   13809 manager.go:932] Added container: "/system.slice/systemd-udevd.service" (aliases: [], namespace: "")
I1207 08:34:29.464887   13809 handler.go:325] Added event &{/system.slice/systemd-udevd.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.464915   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cloud-final.service"
I1207 08:34:29.464924   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cloud-final.service: /system.slice/cloud-final.service not handled by systemd handler
I1207 08:34:29.464949   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cloud-final.service"
I1207 08:34:29.464985   13809 factory.go:112] Using factory "raw" for container "/system.slice/cloud-final.service"
I1207 08:34:29.465062   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-udevd.service"
I1207 08:34:29.465000   13809 round_trippers.go:439] Response Status: 409 Conflict in 9 milliseconds
I1207 08:34:29.465210   13809 manager.go:932] Added container: "/system.slice/cloud-final.service" (aliases: [], namespace: "")
I1207 08:34:29.465321   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17
I1207 08:34:29.465403   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.465433   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.465463   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.465410   13809 handler.go:325] Added event &{/system.slice/cloud-final.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.465559   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:34:29.465573   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:34:29.465581   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:34:29.465613   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/lvm2-monitor.service"
I1207 08:34:29.465630   13809 factory.go:105] Error trying to work out if we can handle /system.slice/lvm2-monitor.service: /system.slice/lvm2-monitor.service not handled by systemd handler
I1207 08:34:29.465649   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/lvm2-monitor.service"
I1207 08:34:29.465665   13809 factory.go:112] Using factory "raw" for container "/system.slice/lvm2-monitor.service"
I1207 08:34:29.465680   13809 container.go:409] Start housekeeping for container "/system.slice/cloud-final.service"
I1207 08:34:29.465891   13809 manager.go:932] Added container: "/system.slice/lvm2-monitor.service" (aliases: [], namespace: "")
I1207 08:34:29.466042   13809 handler.go:325] Added event &{/system.slice/lvm2-monitor.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.466071   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:34:29.466095   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:34:29.466115   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:34:29.466142   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/burstable"
I1207 08:34:29.466156   13809 factory.go:105] Error trying to work out if we can handle /kubepods/burstable: /kubepods/burstable not handled by systemd handler
I1207 08:34:29.466162   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/burstable"
I1207 08:34:29.466167   13809 factory.go:112] Using factory "raw" for container "/kubepods/burstable"
I1207 08:34:29.466267   13809 container.go:409] Start housekeeping for container "/system.slice/lvm2-monitor.service"
I1207 08:34:29.466399   13809 manager.go:932] Added container: "/kubepods/burstable" (aliases: [], namespace: "")
I1207 08:34:29.466575   13809 handler.go:325] Added event &{/kubepods/burstable 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.466611   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/apparmor.service"
I1207 08:34:29.466642   13809 factory.go:105] Error trying to work out if we can handle /system.slice/apparmor.service: /system.slice/apparmor.service not handled by systemd handler
I1207 08:34:29.466655   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/apparmor.service"
I1207 08:34:29.466662   13809 factory.go:112] Using factory "raw" for container "/system.slice/apparmor.service"
I1207 08:34:29.466698   13809 container.go:409] Start housekeeping for container "/kubepods/burstable"
I1207 08:34:29.466886   13809 manager.go:932] Added container: "/system.slice/apparmor.service" (aliases: [], namespace: "")
I1207 08:34:29.467074   13809 handler.go:325] Added event &{/system.slice/apparmor.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.467112   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-random-seed.service"
I1207 08:34:29.467125   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-random-seed.service: /system.slice/systemd-random-seed.service not handled by systemd handler
I1207 08:34:29.467131   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-random-seed.service"
I1207 08:34:29.467157   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-random-seed.service"
I1207 08:34:29.467331   13809 manager.go:932] Added container: "/system.slice/systemd-random-seed.service" (aliases: [], namespace: "")
I1207 08:34:29.467355   13809 container.go:409] Start housekeeping for container "/system.slice/apparmor.service"
I1207 08:34:29.467477   13809 handler.go:325] Added event &{/system.slice/systemd-random-seed.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.467512   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-udev-trigger.service"
I1207 08:34:29.467524   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-udev-trigger.service: /system.slice/systemd-udev-trigger.service not handled by systemd handler
I1207 08:34:29.467529   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-udev-trigger.service"
I1207 08:34:29.467542   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-udev-trigger.service"
I1207 08:34:29.467602   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-random-seed.service"
I1207 08:34:29.467750   13809 manager.go:932] Added container: "/system.slice/systemd-udev-trigger.service" (aliases: [], namespace: "")
I1207 08:34:29.467926   13809 handler.go:325] Added event &{/system.slice/systemd-udev-trigger.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.467957   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:34:29.467965   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:34:29.467991   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:34:29.468023   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:34:29.468057   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:34:29.468091   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:34:29.468137   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-udev-trigger.service"
I1207 08:34:29.469192   13809 factory.go:112] Using factory "docker" for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826"
I1207 08:34:29.469992   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:29.470310   13809 manager.go:932] Added container: "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826" (aliases: [k8s_POD_caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6_7df95486-5737-11ec-9d1e-002248057bde_0 e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826], namespace: "docker")
I1207 08:34:29.470510   13809 handler.go:325] Added event &{/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826 2021-12-07 08:27:19.904410114 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.470548   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/openvswitch-nonetwork.service"
I1207 08:34:29.470562   13809 factory.go:105] Error trying to work out if we can handle /system.slice/openvswitch-nonetwork.service: /system.slice/openvswitch-nonetwork.service not handled by systemd handler
I1207 08:34:29.470567   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/openvswitch-nonetwork.service"
I1207 08:34:29.470572   13809 factory.go:112] Using factory "raw" for container "/system.slice/openvswitch-nonetwork.service"
I1207 08:34:29.470678   13809 container.go:409] Start housekeeping for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826"
I1207 08:34:29.470860   13809 manager.go:932] Added container: "/system.slice/openvswitch-nonetwork.service" (aliases: [], namespace: "")
I1207 08:34:29.471093   13809 handler.go:325] Added event &{/system.slice/openvswitch-nonetwork.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.471888   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cloud-init-local.service"
I1207 08:34:29.472126   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cloud-init-local.service: /system.slice/cloud-init-local.service not handled by systemd handler
I1207 08:34:29.472146   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cloud-init-local.service"
I1207 08:34:29.472155   13809 factory.go:112] Using factory "raw" for container "/system.slice/cloud-init-local.service"
I1207 08:34:29.472461   13809 kubelet_node_status.go:134] Node k8s-agentpool4-a14krc0l-17 was previously registered
I1207 08:34:29.472501   13809 kubelet_node_status.go:86] Successfully registered node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.472709   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:34:29.472721   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.472728   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.472734   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.472837   13809 manager.go:932] Added container: "/system.slice/cloud-init-local.service" (aliases: [], namespace: "")
I1207 08:34:29.473077   13809 container.go:409] Start housekeeping for container "/system.slice/openvswitch-nonetwork.service"
I1207 08:34:29.473206   13809 handler.go:325] Added event &{/system.slice/cloud-init-local.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.473263   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/polkitd.service"
I1207 08:34:29.473276   13809 factory.go:105] Error trying to work out if we can handle /system.slice/polkitd.service: /system.slice/polkitd.service not handled by systemd handler
I1207 08:34:29.473281   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/polkitd.service"
I1207 08:34:29.473287   13809 factory.go:112] Using factory "raw" for container "/system.slice/polkitd.service"
I1207 08:34:29.473563   13809 manager.go:932] Added container: "/system.slice/polkitd.service" (aliases: [], namespace: "")
I1207 08:34:29.473785   13809 handler.go:325] Added event &{/system.slice/polkitd.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.473813   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ifup@eth0.service"
I1207 08:34:29.473823   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ifup@eth0.service: /system.slice/ifup@eth0.service not handled by systemd handler
I1207 08:34:29.473829   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ifup@eth0.service"
I1207 08:34:29.473836   13809 factory.go:112] Using factory "raw" for container "/system.slice/ifup@eth0.service"
I1207 08:34:29.473924   13809 container.go:409] Start housekeeping for container "/system.slice/cloud-init-local.service"
I1207 08:34:29.474074   13809 manager.go:932] Added container: "/system.slice/ifup@eth0.service" (aliases: [], namespace: "")
I1207 08:34:29.474342   13809 handler.go:325] Added event &{/system.slice/ifup@eth0.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.474390   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/irqbalance.service"
I1207 08:34:29.474400   13809 factory.go:105] Error trying to work out if we can handle /system.slice/irqbalance.service: /system.slice/irqbalance.service not handled by systemd handler
I1207 08:34:29.474405   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/irqbalance.service"
I1207 08:34:29.474412   13809 factory.go:112] Using factory "raw" for container "/system.slice/irqbalance.service"
I1207 08:34:29.474560   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:34:29.474726   13809 manager.go:932] Added container: "/system.slice/irqbalance.service" (aliases: [], namespace: "")
I1207 08:34:29.474954   13809 handler.go:325] Added event &{/system.slice/irqbalance.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.474981   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/iscsid.service"
I1207 08:34:29.474991   13809 factory.go:105] Error trying to work out if we can handle /system.slice/iscsid.service: /system.slice/iscsid.service not handled by systemd handler
I1207 08:34:29.474996   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/iscsid.service"
I1207 08:34:29.475003   13809 factory.go:112] Using factory "raw" for container "/system.slice/iscsid.service"
I1207 08:34:29.475039   13809 kuberuntime_manager.go:899] updating runtime config through cri with podcidr 10.244.35.0/24
I1207 08:34:29.474616   13809 container.go:409] Start housekeeping for container "/system.slice/polkitd.service"
I1207 08:34:29.475186   13809 container.go:409] Start housekeeping for container "/system.slice/ifup@eth0.service"
I1207 08:34:29.475265   13809 manager.go:932] Added container: "/system.slice/iscsid.service" (aliases: [], namespace: "")
I1207 08:34:29.475904   13809 handler.go:325] Added event &{/system.slice/iscsid.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.475964   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/lvm2-lvmetad.service"
I1207 08:34:29.475995   13809 factory.go:105] Error trying to work out if we can handle /system.slice/lvm2-lvmetad.service: /system.slice/lvm2-lvmetad.service not handled by systemd handler
I1207 08:34:29.476030   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/lvm2-lvmetad.service"
I1207 08:34:29.476066   13809 factory.go:112] Using factory "raw" for container "/system.slice/lvm2-lvmetad.service"
I1207 08:34:29.475284   13809 container.go:409] Start housekeeping for container "/system.slice/irqbalance.service"
I1207 08:34:29.476341   13809 manager.go:932] Added container: "/system.slice/lvm2-lvmetad.service" (aliases: [], namespace: "")
I1207 08:34:29.476561   13809 handler.go:325] Added event &{/system.slice/lvm2-lvmetad.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.476631   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/lxd-containers.service"
I1207 08:34:29.476668   13809 factory.go:105] Error trying to work out if we can handle /system.slice/lxd-containers.service: /system.slice/lxd-containers.service not handled by systemd handler
I1207 08:34:29.476705   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/lxd-containers.service"
I1207 08:34:29.476740   13809 factory.go:112] Using factory "raw" for container "/system.slice/lxd-containers.service"
I1207 08:34:29.476998   13809 manager.go:932] Added container: "/system.slice/lxd-containers.service" (aliases: [], namespace: "")
I1207 08:34:29.475304   13809 docker_service.go:307] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.35.0/24,},}
I1207 08:34:29.477121   13809 kubenet_linux.go:257] PodCIDR is set to "10.244.35.0/24"
I1207 08:34:29.477162   13809 kubenet_linux.go:265] CNI network config set to {
  "cniVersion": "0.1.0",
  "name": "kubenet",
  "type": "bridge",
  "bridge": "cbr0",
  "mtu": 1500,
  "addIf": "eth0",
  "isGateway": true,
  "ipMasq": false,
  "hairpinMode": false,
  "ipam": {
    "type": "host-local",
    "subnet": "10.244.35.0/24",
    "gateway": "10.244.35.1",
    "routes": [
      { "dst": "0.0.0.0/0" }
    ]
  }
}
I1207 08:34:29.477219   13809 handler.go:325] Added event &{/system.slice/lxd-containers.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.477275   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cgroupfs-mount.service"
I1207 08:34:29.477307   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cgroupfs-mount.service: /system.slice/cgroupfs-mount.service not handled by systemd handler
I1207 08:34:29.477330   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cgroupfs-mount.service"
I1207 08:34:29.477354   13809 factory.go:112] Using factory "raw" for container "/system.slice/cgroupfs-mount.service"
I1207 08:34:29.477464   13809 container.go:409] Start housekeeping for container "/system.slice/iscsid.service"
I1207 08:34:29.477728   13809 container.go:409] Start housekeeping for container "/system.slice/lvm2-lvmetad.service"
I1207 08:34:29.477758   13809 container.go:409] Start housekeeping for container "/system.slice/lxd-containers.service"
I1207 08:34:29.477736   13809 manager.go:932] Added container: "/system.slice/cgroupfs-mount.service" (aliases: [], namespace: "")
I1207 08:34:29.477339   13809 kubenet_linux.go:268] CNI network config:
{
  "cniVersion": "0.1.0",
  "name": "kubenet",
  "type": "bridge",
  "bridge": "cbr0",
  "mtu": 1500,
  "addIf": "eth0",
  "isGateway": true,
  "ipMasq": false,
  "hairpinMode": false,
  "ipam": {
    "type": "host-local",
    "subnet": "10.244.35.0/24",
    "gateway": "10.244.35.1",
    "routes": [
      { "dst": "0.0.0.0/0" }
    ]
  }
}
I1207 08:34:29.478862   13809 handler.go:325] Added event &{/system.slice/cgroupfs-mount.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.478884   13809 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.35.0/24
I1207 08:34:29.478891   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dbus.service"
I1207 08:34:29.478982   13809 factory.go:105] Error trying to work out if we can handle /system.slice/dbus.service: /system.slice/dbus.service not handled by systemd handler
I1207 08:34:29.479070   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/dbus.service"
I1207 08:34:29.479086   13809 factory.go:112] Using factory "raw" for container "/system.slice/dbus.service"
I1207 08:34:29.479152   13809 container.go:409] Start housekeeping for container "/system.slice/cgroupfs-mount.service"
I1207 08:34:29.480128   13809 manager.go:932] Added container: "/system.slice/dbus.service" (aliases: [], namespace: "")
I1207 08:34:29.480757   13809 handler.go:325] Added event &{/system.slice/dbus.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.480924   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-timesyncd.service"
I1207 08:34:29.481070   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-timesyncd.service: /system.slice/systemd-timesyncd.service not handled by systemd handler
I1207 08:34:29.481126   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-timesyncd.service"
I1207 08:34:29.481158   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-timesyncd.service"
I1207 08:34:29.481457   13809 manager.go:932] Added container: "/system.slice/systemd-timesyncd.service" (aliases: [], namespace: "")
I1207 08:34:29.481684   13809 handler.go:325] Added event &{/system.slice/systemd-timesyncd.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.481751   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:34:29.481838   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:34:29.481877   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:34:29.481912   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:34:29.481944   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:34:29.481977   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:34:29.482038   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/system-serial\\x2dgetty.slice"
I1207 08:34:29.482073   13809 factory.go:105] Error trying to work out if we can handle /system.slice/system-serial\x2dgetty.slice: /system.slice/system-serial\x2dgetty.slice not handled by systemd handler
I1207 08:34:29.482103   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/system-serial\\x2dgetty.slice"
I1207 08:34:29.482134   13809 factory.go:112] Using factory "raw" for container "/system.slice/system-serial\\x2dgetty.slice"
I1207 08:34:29.482440   13809 manager.go:932] Added container: "/system.slice/system-serial\\x2dgetty.slice" (aliases: [], namespace: "")
I1207 08:34:29.482692   13809 handler.go:325] Added event &{/system.slice/system-serial\x2dgetty.slice 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.482764   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/unattended-upgrades.service"
I1207 08:34:29.482808   13809 factory.go:105] Error trying to work out if we can handle /system.slice/unattended-upgrades.service: /system.slice/unattended-upgrades.service not handled by systemd handler
I1207 08:34:29.482846   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/unattended-upgrades.service"
I1207 08:34:29.482942   13809 factory.go:112] Using factory "raw" for container "/system.slice/unattended-upgrades.service"
I1207 08:34:29.483234   13809 manager.go:932] Added container: "/system.slice/unattended-upgrades.service" (aliases: [], namespace: "")
I1207 08:34:29.483506   13809 handler.go:325] Added event &{/system.slice/unattended-upgrades.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.483579   13809 factory.go:116] Factory "docker" was unable to handle container "/init.scope"
I1207 08:34:29.483625   13809 factory.go:105] Error trying to work out if we can handle /init.scope: /init.scope not handled by systemd handler
I1207 08:34:29.483665   13809 factory.go:116] Factory "systemd" was unable to handle container "/init.scope"
I1207 08:34:29.483726   13809 factory.go:112] Using factory "raw" for container "/init.scope"
I1207 08:34:29.484123   13809 manager.go:932] Added container: "/init.scope" (aliases: [], namespace: "")
I1207 08:34:29.485172   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-timesyncd.service"
I1207 08:34:29.485967   13809 handler.go:325] Added event &{/init.scope 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.486204   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:34:29.486316   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:34:29.486337   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:34:29.486452   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/setvtrgb.service"
I1207 08:34:29.486559   13809 factory.go:105] Error trying to work out if we can handle /system.slice/setvtrgb.service: /system.slice/setvtrgb.service not handled by systemd handler
I1207 08:34:29.486575   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/setvtrgb.service"
I1207 08:34:29.486586   13809 factory.go:112] Using factory "raw" for container "/system.slice/setvtrgb.service"
I1207 08:34:29.486997   13809 container.go:409] Start housekeeping for container "/system.slice/dbus.service"
I1207 08:34:29.487276   13809 manager.go:932] Added container: "/system.slice/setvtrgb.service" (aliases: [], namespace: "")
I1207 08:34:29.487536   13809 handler.go:325] Added event &{/system.slice/setvtrgb.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.487581   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:34:29.487651   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:34:29.487681   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:34:29.487692   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:34:29.487723   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:34:29.487792   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:34:29.487804   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:34:29.487841   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:34:29.487981   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:34:29.488455   13809 container.go:409] Start housekeeping for container "/system.slice/unattended-upgrades.service"
I1207 08:34:29.488797   13809 container.go:409] Start housekeeping for container "/init.scope"
I1207 08:34:29.489759   13809 container.go:409] Start housekeeping for container "/system.slice/system-serial\\x2dgetty.slice"
I1207 08:34:29.490225   13809 container.go:409] Start housekeeping for container "/system.slice/setvtrgb.service"
I1207 08:34:29.494068   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c"
I1207 08:34:29.494249   13809 kubelet_node_status.go:443] Recording NodeHasSufficientMemory event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.494296   13809 kubelet_node_status.go:443] Recording NodeHasNoDiskPressure event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.494390   13809 kubelet_node_status.go:443] Recording NodeNotReady event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:29.494426   13809 kubelet_node_status.go:791] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2021-12-07 08:34:29.49437255 +0000 UTC LastTransitionTime:2021-12-07 08:34:29.49437255 +0000 UTC Reason:KubeletNotReady Message:container runtime is down,runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR}
I1207 08:34:29.494749   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasSufficientMemory' Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientMemory
I1207 08:34:29.494796   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeHasNoDiskPressure' Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasNoDiskPressure
I1207 08:34:29.494939   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeNotReady' Node k8s-agentpool4-a14krc0l-17 status is now: NodeNotReady
I1207 08:34:29.495750   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/default/events/k8s-agentpool4-a14krc0l-17.16be6c01d051d2ed
I1207 08:34:29.495792   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.495808   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:29.495859   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.495892   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.496750   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c" (aliases: [k8s_logger_shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6_7c988aa2-5737-11ec-9d1e-002248057bde_0 74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c], namespace: "docker")
I1207 08:34:29.497096   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c 2021-12-07 08:27:22.832589806 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.497202   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ondemand.service"
I1207 08:34:29.498455   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:34:29.498533   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.498549   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.498608   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ondemand.service: /system.slice/ondemand.service not handled by systemd handler
I1207 08:34:29.498664   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ondemand.service"
I1207 08:34:29.498699   13809 factory.go:112] Using factory "raw" for container "/system.slice/ondemand.service"
I1207 08:34:29.499211   13809 round_trippers.go:439] Response Status: 403 Forbidden in 3 milliseconds
I1207 08:34:29.499565   13809 manager.go:932] Added container: "/system.slice/ondemand.service" (aliases: [], namespace: "")
E1207 08:34:29.499428   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d051d2ed", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455229677, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:494287845, loc:(*time.Location)(0x9e22280)}}, Count:2, Type:"Normal"}': 'events "k8s-agentpool4-a14krc0l-17.16be6c01d051d2ed" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "default"' (will not retry!)
I1207 08:34:29.499866   13809 handler.go:325] Added event &{/system.slice/ondemand.service 2021-11-30 02:10:09.091256292 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.500070   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/default/events/k8s-agentpool4-a14krc0l-17.16be6c01d0525247
I1207 08:34:29.500093   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.500105   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.500164   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:29.500211   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.497265   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c"
I1207 08:34:29.498614   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:29.500635   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.501565   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864"
I1207 08:34:29.501871   13809 container.go:409] Start housekeeping for container "/system.slice/ondemand.service"
I1207 08:34:29.502417   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:29.502515   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d0525247", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:455262279, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:494362650, loc:(*time.Location)(0x9e22280)}}, Count:2, Type:"Normal"}': 'events "k8s-agentpool4-a14krc0l-17.16be6c01d0525247" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "default"' (will not retry!)
I1207 08:34:29.502651   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:29.502678   13809 round_trippers.go:421] Request Headers:
I1207 08:34:29.502703   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:29.502737   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:29.502765   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:29.504145   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864" (aliases: [k8s_c5_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_0 6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864], namespace: "docker")
I1207 08:34:29.504395   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:29.504404   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864 2021-12-07 08:28:28.816633575 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.504518   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-modules-load.service"
I1207 08:34:29.504567   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-modules-load.service: /system.slice/systemd-modules-load.service not handled by systemd handler
I1207 08:34:29.504594   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-modules-load.service"
I1207 08:34:29.504620   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-modules-load.service"
I1207 08:34:29.504401   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
I1207 08:34:29.504867   13809 manager.go:932] Added container: "/system.slice/systemd-modules-load.service" (aliases: [], namespace: "")
E1207 08:34:29.504859   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c01d2a7d31d", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeNotReady", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeNotReady", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:494420253, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462869, nsec:494420253, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:29.505093   13809 handler.go:325] Added event &{/system.slice/systemd-modules-load.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.505149   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ephemeral-disk-warning.service"
I1207 08:34:29.505184   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ephemeral-disk-warning.service: /system.slice/ephemeral-disk-warning.service not handled by systemd handler
I1207 08:34:29.505213   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ephemeral-disk-warning.service"
I1207 08:34:29.505244   13809 factory.go:112] Using factory "raw" for container "/system.slice/ephemeral-disk-warning.service"
I1207 08:34:29.506326   13809 manager.go:932] Added container: "/system.slice/ephemeral-disk-warning.service" (aliases: [], namespace: "")
I1207 08:34:29.506632   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-modules-load.service"
I1207 08:34:29.506929   13809 handler.go:325] Added event &{/system.slice/ephemeral-disk-warning.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.507044   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/hv-kvp-daemon.service"
I1207 08:34:29.507087   13809 factory.go:105] Error trying to work out if we can handle /system.slice/hv-kvp-daemon.service: /system.slice/hv-kvp-daemon.service not handled by systemd handler
I1207 08:34:29.507125   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/hv-kvp-daemon.service"
I1207 08:34:29.507157   13809 factory.go:112] Using factory "raw" for container "/system.slice/hv-kvp-daemon.service"
I1207 08:34:29.507651   13809 manager.go:932] Added container: "/system.slice/hv-kvp-daemon.service" (aliases: [], namespace: "")
I1207 08:34:29.507939   13809 handler.go:325] Added event &{/system.slice/hv-kvp-daemon.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.507978   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:34:29.508028   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:34:29.505184   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864"
I1207 08:34:29.508113   13809 container.go:409] Start housekeeping for container "/system.slice/hv-kvp-daemon.service"
I1207 08:34:29.508069   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:34:29.509075   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/networking.service"
I1207 08:34:29.509112   13809 factory.go:105] Error trying to work out if we can handle /system.slice/networking.service: /system.slice/networking.service not handled by systemd handler
I1207 08:34:29.509139   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/networking.service"
I1207 08:34:29.509161   13809 factory.go:112] Using factory "raw" for container "/system.slice/networking.service"
I1207 08:34:29.509251   13809 container.go:409] Start housekeeping for container "/system.slice/ephemeral-disk-warning.service"
I1207 08:34:29.509538   13809 manager.go:932] Added container: "/system.slice/networking.service" (aliases: [], namespace: "")
I1207 08:34:29.509855   13809 handler.go:325] Added event &{/system.slice/networking.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.509886   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/openvswitch-switch.service"
I1207 08:34:29.509920   13809 factory.go:105] Error trying to work out if we can handle /system.slice/openvswitch-switch.service: /system.slice/openvswitch-switch.service not handled by systemd handler
I1207 08:34:29.509940   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/openvswitch-switch.service"
I1207 08:34:29.509962   13809 factory.go:112] Using factory "raw" for container "/system.slice/openvswitch-switch.service"
I1207 08:34:29.510217   13809 container.go:409] Start housekeeping for container "/system.slice/networking.service"
I1207 08:34:29.510313   13809 manager.go:932] Added container: "/system.slice/openvswitch-switch.service" (aliases: [], namespace: "")
I1207 08:34:29.510742   13809 handler.go:325] Added event &{/system.slice/openvswitch-switch.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.510802   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods"
I1207 08:34:29.510840   13809 factory.go:105] Error trying to work out if we can handle /kubepods: /kubepods not handled by systemd handler
I1207 08:34:29.510874   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods"
I1207 08:34:29.510909   13809 factory.go:112] Using factory "raw" for container "/kubepods"
I1207 08:34:29.511174   13809 manager.go:932] Added container: "/kubepods" (aliases: [], namespace: "")
I1207 08:34:29.511411   13809 handler.go:325] Added event &{/kubepods 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.511485   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/atd.service"
I1207 08:34:29.511519   13809 factory.go:105] Error trying to work out if we can handle /system.slice/atd.service: /system.slice/atd.service not handled by systemd handler
I1207 08:34:29.511546   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/atd.service"
I1207 08:34:29.511575   13809 factory.go:112] Using factory "raw" for container "/system.slice/atd.service"
I1207 08:34:29.511788   13809 manager.go:932] Added container: "/system.slice/atd.service" (aliases: [], namespace: "")
I1207 08:34:29.511981   13809 handler.go:325] Added event &{/system.slice/atd.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.512040   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/system-getty.slice"
I1207 08:34:29.512072   13809 factory.go:105] Error trying to work out if we can handle /system.slice/system-getty.slice: /system.slice/system-getty.slice not handled by systemd handler
I1207 08:34:29.512101   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/system-getty.slice"
I1207 08:34:29.512130   13809 factory.go:112] Using factory "raw" for container "/system.slice/system-getty.slice"
I1207 08:34:29.513319   13809 container.go:409] Start housekeeping for container "/system.slice/openvswitch-switch.service"
I1207 08:34:29.513628   13809 manager.go:932] Added container: "/system.slice/system-getty.slice" (aliases: [], namespace: "")
I1207 08:34:29.513919   13809 handler.go:325] Added event &{/system.slice/system-getty.slice 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.513968   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-remount-fs.service"
I1207 08:34:29.514063   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-remount-fs.service: /system.slice/systemd-remount-fs.service not handled by systemd handler
I1207 08:34:29.514089   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-remount-fs.service"
I1207 08:34:29.514115   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-remount-fs.service"
I1207 08:34:29.514162   13809 container.go:409] Start housekeeping for container "/kubepods"
I1207 08:34:29.514378   13809 manager.go:932] Added container: "/system.slice/systemd-remount-fs.service" (aliases: [], namespace: "")
I1207 08:34:29.514594   13809 handler.go:325] Added event &{/system.slice/systemd-remount-fs.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.514625   13809 factory.go:116] Factory "docker" was unable to handle container "/azure.slice"
I1207 08:34:29.514662   13809 factory.go:105] Error trying to work out if we can handle /azure.slice: /azure.slice not handled by systemd handler
I1207 08:34:29.514682   13809 factory.go:116] Factory "systemd" was unable to handle container "/azure.slice"
I1207 08:34:29.514698   13809 factory.go:112] Using factory "raw" for container "/azure.slice"
I1207 08:34:29.514907   13809 manager.go:932] Added container: "/azure.slice" (aliases: [], namespace: "")
I1207 08:34:29.515072   13809 handler.go:325] Added event &{/azure.slice 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.515101   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/resolvconf.service"
I1207 08:34:29.515111   13809 factory.go:105] Error trying to work out if we can handle /system.slice/resolvconf.service: /system.slice/resolvconf.service not handled by systemd handler
I1207 08:34:29.515120   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/resolvconf.service"
I1207 08:34:29.515127   13809 factory.go:112] Using factory "raw" for container "/system.slice/resolvconf.service"
I1207 08:34:29.515275   13809 container.go:409] Start housekeeping for container "/system.slice/system-getty.slice"
I1207 08:34:29.515339   13809 manager.go:932] Added container: "/system.slice/resolvconf.service" (aliases: [], namespace: "")
I1207 08:34:29.516462   13809 handler.go:325] Added event &{/system.slice/resolvconf.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.516757   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/besteffort"
I1207 08:34:29.516782   13809 factory.go:105] Error trying to work out if we can handle /kubepods/besteffort: /kubepods/besteffort not handled by systemd handler
I1207 08:34:29.516791   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/besteffort"
I1207 08:34:29.516829   13809 factory.go:112] Using factory "raw" for container "/kubepods/besteffort"
I1207 08:34:29.517486   13809 manager.go:932] Added container: "/kubepods/besteffort" (aliases: [], namespace: "")
I1207 08:34:29.517706   13809 container.go:409] Start housekeeping for container "/system.slice/atd.service"
I1207 08:34:29.517814   13809 handler.go:325] Added event &{/kubepods/besteffort 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.517858   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:34:29.517902   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:34:29.517941   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:34:29.517962   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:34:29.517971   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:34:29.517980   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:34:29.518018   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/accounts-daemon.service"
I1207 08:34:29.518049   13809 factory.go:105] Error trying to work out if we can handle /system.slice/accounts-daemon.service: /system.slice/accounts-daemon.service not handled by systemd handler
I1207 08:34:29.518078   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/accounts-daemon.service"
I1207 08:34:29.518105   13809 factory.go:112] Using factory "raw" for container "/system.slice/accounts-daemon.service"
I1207 08:34:29.518495   13809 manager.go:932] Added container: "/system.slice/accounts-daemon.service" (aliases: [], namespace: "")
I1207 08:34:29.518782   13809 handler.go:325] Added event &{/system.slice/accounts-daemon.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.518875   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/keyboard-setup.service"
I1207 08:34:29.518894   13809 factory.go:105] Error trying to work out if we can handle /system.slice/keyboard-setup.service: /system.slice/keyboard-setup.service not handled by systemd handler
I1207 08:34:29.518901   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/keyboard-setup.service"
I1207 08:34:29.518911   13809 factory.go:112] Using factory "raw" for container "/system.slice/keyboard-setup.service"
I1207 08:34:29.519167   13809 container.go:409] Start housekeeping for container "/azure.slice"
I1207 08:34:29.519319   13809 manager.go:932] Added container: "/system.slice/keyboard-setup.service" (aliases: [], namespace: "")
I1207 08:34:29.519619   13809 handler.go:325] Added event &{/system.slice/keyboard-setup.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.519664   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-logind.service"
I1207 08:34:29.519708   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-logind.service: /system.slice/systemd-logind.service not handled by systemd handler
I1207 08:34:29.519733   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-logind.service"
I1207 08:34:29.519752   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-logind.service"
I1207 08:34:29.520067   13809 manager.go:932] Added container: "/system.slice/systemd-logind.service" (aliases: [], namespace: "")
I1207 08:34:29.520348   13809 handler.go:325] Added event &{/system.slice/systemd-logind.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.520576   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-remount-fs.service"
I1207 08:34:29.520861   13809 container.go:409] Start housekeeping for container "/system.slice/resolvconf.service"
I1207 08:34:29.521883   13809 container.go:409] Start housekeeping for container "/kubepods/besteffort"
I1207 08:34:29.522403   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731"
I1207 08:34:29.522455   13809 container.go:409] Start housekeeping for container "/system.slice/accounts-daemon.service"
I1207 08:34:29.522976   13809 container.go:409] Start housekeeping for container "/system.slice/keyboard-setup.service"
I1207 08:34:29.524949   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-logind.service"
I1207 08:34:29.526324   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731" (aliases: [k8s_c4_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_0 1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731], namespace: "docker")
I1207 08:34:29.526557   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731 2021-12-07 08:27:59.042810179 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.526911   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731"
I1207 08:34:29.530369   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c"
I1207 08:34:29.531698   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c" (aliases: [k8s_POD_shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6_7c988aa2-5737-11ec-9d1e-002248057bde_0 b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c], namespace: "docker")
I1207 08:34:29.531988   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c 2021-12-07 08:27:21.640516655 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.532025   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/lxcfs.service"
I1207 08:34:29.532051   13809 factory.go:105] Error trying to work out if we can handle /system.slice/lxcfs.service: /system.slice/lxcfs.service not handled by systemd handler
I1207 08:34:29.532074   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/lxcfs.service"
I1207 08:34:29.532093   13809 factory.go:112] Using factory "raw" for container "/system.slice/lxcfs.service"
I1207 08:34:29.532313   13809 manager.go:932] Added container: "/system.slice/lxcfs.service" (aliases: [], namespace: "")
I1207 08:34:29.532501   13809 handler.go:325] Added event &{/system.slice/lxcfs.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.532532   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-update-utmp.service"
I1207 08:34:29.533198   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-update-utmp.service: /system.slice/systemd-update-utmp.service not handled by systemd handler
I1207 08:34:29.533223   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-update-utmp.service"
I1207 08:34:29.533237   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-update-utmp.service"
I1207 08:34:29.533735   13809 manager.go:932] Added container: "/system.slice/systemd-update-utmp.service" (aliases: [], namespace: "")
I1207 08:34:29.533949   13809 handler.go:325] Added event &{/system.slice/systemd-update-utmp.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.534882   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/open-iscsi.service"
I1207 08:34:29.534899   13809 factory.go:105] Error trying to work out if we can handle /system.slice/open-iscsi.service: /system.slice/open-iscsi.service not handled by systemd handler
I1207 08:34:29.534905   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/open-iscsi.service"
I1207 08:34:29.534913   13809 factory.go:112] Using factory "raw" for container "/system.slice/open-iscsi.service"
I1207 08:34:29.535127   13809 manager.go:932] Added container: "/system.slice/open-iscsi.service" (aliases: [], namespace: "")
I1207 08:34:29.535282   13809 handler.go:325] Added event &{/system.slice/open-iscsi.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.535312   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice"
I1207 08:34:29.535336   13809 factory.go:105] Error trying to work out if we can handle /system.slice: /system.slice not handled by systemd handler
I1207 08:34:29.535356   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice"
I1207 08:34:29.535376   13809 factory.go:112] Using factory "raw" for container "/system.slice"
I1207 08:34:29.535572   13809 manager.go:932] Added container: "/system.slice" (aliases: [], namespace: "")
I1207 08:34:29.535711   13809 handler.go:325] Added event &{/system.slice 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.535790   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:34:29.535830   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:34:29.535866   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:34:29.535889   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:34:29.535898   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:34:29.535925   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:34:29.535958   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/console-setup.service"
I1207 08:34:29.535990   13809 factory.go:105] Error trying to work out if we can handle /system.slice/console-setup.service: /system.slice/console-setup.service not handled by systemd handler
I1207 08:34:29.536018   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/console-setup.service"
I1207 08:34:29.536046   13809 factory.go:112] Using factory "raw" for container "/system.slice/console-setup.service"
I1207 08:34:29.536241   13809 manager.go:932] Added container: "/system.slice/console-setup.service" (aliases: [], namespace: "")
I1207 08:34:29.536399   13809 handler.go:325] Added event &{/system.slice/console-setup.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.536424   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ufw.service"
I1207 08:34:29.536447   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ufw.service: /system.slice/ufw.service not handled by systemd handler
I1207 08:34:29.536473   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ufw.service"
I1207 08:34:29.536502   13809 factory.go:112] Using factory "raw" for container "/system.slice/ufw.service"
I1207 08:34:29.536698   13809 manager.go:932] Added container: "/system.slice/ufw.service" (aliases: [], namespace: "")
I1207 08:34:29.536849   13809 handler.go:325] Added event &{/system.slice/ufw.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.536877   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/nfs-config.service"
I1207 08:34:29.536899   13809 factory.go:105] Error trying to work out if we can handle /system.slice/nfs-config.service: /system.slice/nfs-config.service not handled by systemd handler
I1207 08:34:29.536924   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/nfs-config.service"
I1207 08:34:29.536953   13809 factory.go:112] Using factory "raw" for container "/system.slice/nfs-config.service"
I1207 08:34:29.537148   13809 manager.go:932] Added container: "/system.slice/nfs-config.service" (aliases: [], namespace: "")
I1207 08:34:29.537293   13809 handler.go:325] Added event &{/system.slice/nfs-config.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.537337   13809 factory.go:116] Factory "docker" was unable to handle container "/azure.slice/walinuxagent.service"
I1207 08:34:29.537368   13809 factory.go:105] Error trying to work out if we can handle /azure.slice/walinuxagent.service: /azure.slice/walinuxagent.service not handled by systemd handler
I1207 08:34:29.537454   13809 factory.go:116] Factory "systemd" was unable to handle container "/azure.slice/walinuxagent.service"
I1207 08:34:29.537487   13809 factory.go:112] Using factory "raw" for container "/azure.slice/walinuxagent.service"
I1207 08:34:29.537648   13809 manager.go:932] Added container: "/azure.slice/walinuxagent.service" (aliases: [], namespace: "")
I1207 08:34:29.537762   13809 handler.go:325] Added event &{/azure.slice/walinuxagent.service 2021-11-30 02:07:55.104349238 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.537813   13809 factory.go:116] Factory "docker" was unable to handle container "/WALinuxAgent/WALinuxAgent"
I1207 08:34:29.537846   13809 factory.go:105] Error trying to work out if we can handle /WALinuxAgent/WALinuxAgent: /WALinuxAgent/WALinuxAgent not handled by systemd handler
I1207 08:34:29.537874   13809 factory.go:116] Factory "systemd" was unable to handle container "/WALinuxAgent/WALinuxAgent"
I1207 08:34:29.537903   13809 factory.go:112] Using factory "raw" for container "/WALinuxAgent/WALinuxAgent"
I1207 08:34:29.538101   13809 manager.go:932] Added container: "/WALinuxAgent/WALinuxAgent" (aliases: [], namespace: "")
I1207 08:34:29.538245   13809 handler.go:325] Added event &{/WALinuxAgent/WALinuxAgent 2021-11-30 02:07:55.084347402 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.538272   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ebtables.service"
I1207 08:34:29.538294   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ebtables.service: /system.slice/ebtables.service not handled by systemd handler
I1207 08:34:29.538320   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ebtables.service"
I1207 08:34:29.538350   13809 factory.go:112] Using factory "raw" for container "/system.slice/ebtables.service"
I1207 08:34:29.538561   13809 manager.go:932] Added container: "/system.slice/ebtables.service" (aliases: [], namespace: "")
I1207 08:34:29.538712   13809 handler.go:325] Added event &{/system.slice/ebtables.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.538770   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-sysctl.service"
I1207 08:34:29.538803   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-sysctl.service: /system.slice/systemd-sysctl.service not handled by systemd handler
I1207 08:34:29.538832   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-sysctl.service"
I1207 08:34:29.538861   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-sysctl.service"
I1207 08:34:29.539048   13809 manager.go:932] Added container: "/system.slice/systemd-sysctl.service" (aliases: [], namespace: "")
I1207 08:34:29.539051   13809 container.go:409] Start housekeeping for container "/system.slice"
I1207 08:34:29.539204   13809 handler.go:325] Added event &{/system.slice/systemd-sysctl.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.539237   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/unscd.service"
I1207 08:34:29.539273   13809 factory.go:105] Error trying to work out if we can handle /system.slice/unscd.service: /system.slice/unscd.service not handled by systemd handler
I1207 08:34:29.539302   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/unscd.service"
I1207 08:34:29.539331   13809 factory.go:112] Using factory "raw" for container "/system.slice/unscd.service"
I1207 08:34:29.539587   13809 manager.go:932] Added container: "/system.slice/unscd.service" (aliases: [], namespace: "")
I1207 08:34:29.539766   13809 handler.go:325] Added event &{/system.slice/unscd.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.539798   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.539844   13809 factory.go:105] Error trying to work out if we can handle /kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde: /kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde not handled by systemd handler
I1207 08:34:29.539882   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.539915   13809 factory.go:112] Using factory "raw" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.539989   13809 container.go:409] Start housekeeping for container "/system.slice/lxcfs.service"
I1207 08:34:29.540201   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde" (aliases: [], namespace: "")
I1207 08:34:29.540410   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde 2021-12-07 08:27:16.660211002 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.540438   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/kmod-static-nodes.service"
I1207 08:34:29.540464   13809 factory.go:105] Error trying to work out if we can handle /system.slice/kmod-static-nodes.service: /system.slice/kmod-static-nodes.service not handled by systemd handler
I1207 08:34:29.540493   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/kmod-static-nodes.service"
I1207 08:34:29.540526   13809 factory.go:112] Using factory "raw" for container "/system.slice/kmod-static-nodes.service"
I1207 08:34:29.532572   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c988aa2-5737-11ec-9d1e-002248057bde/b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c"
I1207 08:34:29.540822   13809 manager.go:932] Added container: "/system.slice/kmod-static-nodes.service" (aliases: [], namespace: "")
I1207 08:34:29.541015   13809 handler.go:325] Added event &{/system.slice/kmod-static-nodes.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.541212   13809 container.go:409] Start housekeeping for container "/system.slice/console-setup.service"
I1207 08:34:29.541492   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-update-utmp.service"
I1207 08:34:29.542045   13809 container.go:409] Start housekeeping for container "/system.slice/ufw.service"
I1207 08:34:29.542127   13809 container.go:409] Start housekeeping for container "/system.slice/open-iscsi.service"
I1207 08:34:29.542652   13809 container.go:409] Start housekeeping for container "/system.slice/nfs-config.service"
I1207 08:34:29.544383   13809 container.go:409] Start housekeeping for container "/azure.slice/walinuxagent.service"
I1207 08:34:29.544361   13809 factory.go:112] Using factory "docker" for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863"
I1207 08:34:29.544857   13809 container.go:409] Start housekeeping for container "/WALinuxAgent/WALinuxAgent"
I1207 08:34:29.544993   13809 container.go:409] Start housekeeping for container "/system.slice/unscd.service"
I1207 08:34:29.545352   13809 container.go:409] Start housekeeping for container "/system.slice/ebtables.service"
I1207 08:34:29.546233   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-sysctl.service"
I1207 08:34:29.546322   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:29.546393   13809 container.go:409] Start housekeeping for container "/system.slice/kmod-static-nodes.service"
I1207 08:34:29.550051   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.547711   13809 manager.go:932] Added container: "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863" (aliases: [k8s_infra_caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6_7df95486-5737-11ec-9d1e-002248057bde_0 efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863], namespace: "docker")
I1207 08:34:29.551853   13809 handler.go:325] Added event &{/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863 2021-12-07 08:27:20.89247075 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.552762   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/rsyslog.service"
I1207 08:34:29.553060   13809 factory.go:105] Error trying to work out if we can handle /system.slice/rsyslog.service: /system.slice/rsyslog.service not handled by systemd handler
I1207 08:34:29.553138   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/rsyslog.service"
I1207 08:34:29.553232   13809 factory.go:112] Using factory "raw" for container "/system.slice/rsyslog.service"
I1207 08:34:29.553170   13809 container.go:409] Start housekeeping for container "/kubepods/besteffort/pod7df95486-5737-11ec-9d1e-002248057bde/efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863"
I1207 08:34:29.554932   13809 manager.go:932] Added container: "/system.slice/rsyslog.service" (aliases: [], namespace: "")
I1207 08:34:29.555305   13809 handler.go:325] Added event &{/system.slice/rsyslog.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.555556   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/ssh.service"
I1207 08:34:29.555587   13809 container.go:409] Start housekeeping for container "/system.slice/rsyslog.service"
I1207 08:34:29.555641   13809 factory.go:105] Error trying to work out if we can handle /system.slice/ssh.service: /system.slice/ssh.service not handled by systemd handler
I1207 08:34:29.555756   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/ssh.service"
I1207 08:34:29.555832   13809 factory.go:112] Using factory "raw" for container "/system.slice/ssh.service"
I1207 08:34:29.556594   13809 manager.go:932] Added container: "/system.slice/ssh.service" (aliases: [], namespace: "")
I1207 08:34:29.556981   13809 handler.go:325] Added event &{/system.slice/ssh.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.557092   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:34:29.557171   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:34:29.557196   13809 container.go:409] Start housekeeping for container "/system.slice/ssh.service"
I1207 08:34:29.557207   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:34:29.557313   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
I1207 08:34:29.557428   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-tmpfiles-setup.service: /system.slice/systemd-tmpfiles-setup.service not handled by systemd handler
I1207 08:34:29.557509   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup.service"
I1207 08:34:29.557536   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-tmpfiles-setup.service"
I1207 08:34:29.557862   13809 manager.go:932] Added container: "/system.slice/systemd-tmpfiles-setup.service" (aliases: [], namespace: "")
I1207 08:34:29.558163   13809 handler.go:325] Added event &{/system.slice/systemd-tmpfiles-setup.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.558265   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cron.service"
I1207 08:34:29.558340   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cron.service: /system.slice/cron.service not handled by systemd handler
I1207 08:34:29.558353   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-tmpfiles-setup.service"
I1207 08:34:29.558369   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cron.service"
I1207 08:34:29.558444   13809 factory.go:112] Using factory "raw" for container "/system.slice/cron.service"
I1207 08:34:29.558752   13809 manager.go:932] Added container: "/system.slice/cron.service" (aliases: [], namespace: "")
I1207 08:34:29.559016   13809 handler.go:325] Added event &{/system.slice/cron.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.559058   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/hv-vss-daemon.service"
I1207 08:34:29.559136   13809 factory.go:105] Error trying to work out if we can handle /system.slice/hv-vss-daemon.service: /system.slice/hv-vss-daemon.service not handled by systemd handler
I1207 08:34:29.559198   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/hv-vss-daemon.service"
I1207 08:34:29.559246   13809 factory.go:112] Using factory "raw" for container "/system.slice/hv-vss-daemon.service"
I1207 08:34:29.559154   13809 container.go:409] Start housekeeping for container "/system.slice/cron.service"
I1207 08:34:29.559528   13809 manager.go:932] Added container: "/system.slice/hv-vss-daemon.service" (aliases: [], namespace: "")
I1207 08:34:29.559718   13809 handler.go:325] Added event &{/system.slice/hv-vss-daemon.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.559755   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/grub-common.service"
I1207 08:34:29.559795   13809 factory.go:105] Error trying to work out if we can handle /system.slice/grub-common.service: /system.slice/grub-common.service not handled by systemd handler
I1207 08:34:29.559830   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/grub-common.service"
I1207 08:34:29.559866   13809 factory.go:112] Using factory "raw" for container "/system.slice/grub-common.service"
I1207 08:34:29.560027   13809 container.go:409] Start housekeeping for container "/system.slice/hv-vss-daemon.service"
I1207 08:34:29.560115   13809 manager.go:932] Added container: "/system.slice/grub-common.service" (aliases: [], namespace: "")
I1207 08:34:29.560304   13809 handler.go:325] Added event &{/system.slice/grub-common.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.560356   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
I1207 08:34:29.560391   13809 factory.go:105] Error trying to work out if we can handle /system.slice/system-systemd\x2dfsck.slice: /system.slice/system-systemd\x2dfsck.slice not handled by systemd handler
I1207 08:34:29.560422   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/system-systemd\\x2dfsck.slice"
I1207 08:34:29.560435   13809 factory.go:112] Using factory "raw" for container "/system.slice/system-systemd\\x2dfsck.slice"
I1207 08:34:29.560559   13809 container.go:409] Start housekeeping for container "/system.slice/grub-common.service"
I1207 08:34:29.560705   13809 manager.go:932] Added container: "/system.slice/system-systemd\\x2dfsck.slice" (aliases: [], namespace: "")
I1207 08:34:29.560933   13809 handler.go:325] Added event &{/system.slice/system-systemd\x2dfsck.slice 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.561020   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-user-sessions.service"
I1207 08:34:29.561058   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-user-sessions.service: /system.slice/systemd-user-sessions.service not handled by systemd handler
I1207 08:34:29.561088   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-user-sessions.service"
I1207 08:34:29.561155   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-user-sessions.service"
I1207 08:34:29.561101   13809 container.go:409] Start housekeeping for container "/system.slice/system-systemd\\x2dfsck.slice"
I1207 08:34:29.561491   13809 manager.go:932] Added container: "/system.slice/systemd-user-sessions.service" (aliases: [], namespace: "")
I1207 08:34:29.561737   13809 handler.go:325] Added event &{/system.slice/systemd-user-sessions.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.561813   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/acpid.service"
I1207 08:34:29.561854   13809 factory.go:105] Error trying to work out if we can handle /system.slice/acpid.service: /system.slice/acpid.service not handled by systemd handler
I1207 08:34:29.561887   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/acpid.service"
I1207 08:34:29.561923   13809 factory.go:112] Using factory "raw" for container "/system.slice/acpid.service"
I1207 08:34:29.561987   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-user-sessions.service"
I1207 08:34:29.562224   13809 manager.go:932] Added container: "/system.slice/acpid.service" (aliases: [], namespace: "")
I1207 08:34:29.562430   13809 handler.go:325] Added event &{/system.slice/acpid.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.562467   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cloud-config.service"
I1207 08:34:29.562492   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cloud-config.service: /system.slice/cloud-config.service not handled by systemd handler
I1207 08:34:29.562509   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cloud-config.service"
I1207 08:34:29.562532   13809 factory.go:112] Using factory "raw" for container "/system.slice/cloud-config.service"
I1207 08:34:29.562569   13809 container.go:409] Start housekeeping for container "/system.slice/acpid.service"
I1207 08:34:29.562786   13809 manager.go:932] Added container: "/system.slice/cloud-config.service" (aliases: [], namespace: "")
I1207 08:34:29.562995   13809 handler.go:325] Added event &{/system.slice/cloud-config.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.563052   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-journald.service"
I1207 08:34:29.563094   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-journald.service: /system.slice/systemd-journald.service not handled by systemd handler
I1207 08:34:29.563127   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-journald.service"
I1207 08:34:29.563162   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-journald.service"
I1207 08:34:29.563208   13809 container.go:409] Start housekeeping for container "/system.slice/cloud-config.service"
I1207 08:34:29.563380   13809 manager.go:932] Added container: "/system.slice/systemd-journald.service" (aliases: [], namespace: "")
I1207 08:34:29.563560   13809 handler.go:325] Added event &{/system.slice/systemd-journald.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.563733   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-journald.service"
I1207 08:34:29.564663   13809 factory.go:112] Using factory "docker" for container "/docker/f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c"
I1207 08:34:29.565798   13809 manager.go:932] Added container: "/docker/f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c" (aliases: [laughing_swanson f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c], namespace: "docker")
I1207 08:34:29.565982   13809 handler.go:325] Added event &{/docker/f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c 2021-12-07 08:34:29.3945893 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.566015   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:34:29.566029   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:34:29.566038   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:34:29.566050   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/rc-local.service"
I1207 08:34:29.566090   13809 factory.go:105] Error trying to work out if we can handle /system.slice/rc-local.service: /system.slice/rc-local.service not handled by systemd handler
I1207 08:34:29.566125   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/rc-local.service"
I1207 08:34:29.566159   13809 factory.go:112] Using factory "raw" for container "/system.slice/rc-local.service"
I1207 08:34:29.566144   13809 container.go:409] Start housekeeping for container "/docker/f2ddff49e13546726fb6b2bdbba6bb2926a54f78036684114dc7f99d3838761c"
I1207 08:34:29.567236   13809 manager.go:932] Added container: "/system.slice/rc-local.service" (aliases: [], namespace: "")
I1207 08:34:29.567596   13809 handler.go:325] Added event &{/system.slice/rc-local.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.567705   13809 factory.go:116] Factory "docker" was unable to handle container "/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:29.567754   13809 factory.go:105] Error trying to work out if we can handle /kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde: /kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde not handled by systemd handler
I1207 08:34:29.567786   13809 container.go:409] Start housekeeping for container "/system.slice/rc-local.service"
I1207 08:34:29.567787   13809 factory.go:116] Factory "systemd" was unable to handle container "/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:29.567981   13809 factory.go:112] Using factory "raw" for container "/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:29.568422   13809 manager.go:932] Added container: "/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde" (aliases: [], namespace: "")
I1207 08:34:29.568705   13809 handler.go:325] Added event &{/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde 2021-11-30 02:10:14.083376832 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.568755   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/apport.service"
I1207 08:34:29.568791   13809 factory.go:105] Error trying to work out if we can handle /system.slice/apport.service: /system.slice/apport.service not handled by systemd handler
I1207 08:34:29.568814   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/apport.service"
I1207 08:34:29.568838   13809 factory.go:112] Using factory "raw" for container "/system.slice/apport.service"
I1207 08:34:29.568941   13809 container.go:409] Start housekeeping for container "/kubepods/besteffort/pod83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:29.569074   13809 manager.go:932] Added container: "/system.slice/apport.service" (aliases: [], namespace: "")
I1207 08:34:29.569255   13809 handler.go:325] Added event &{/system.slice/apport.service 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.569307   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-journal-flush.service"
I1207 08:34:29.569430   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-journal-flush.service: /system.slice/systemd-journal-flush.service not handled by systemd handler
I1207 08:34:29.569445   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-journal-flush.service"
I1207 08:34:29.569454   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-journal-flush.service"
I1207 08:34:29.569372   13809 container.go:409] Start housekeeping for container "/system.slice/apport.service"
I1207 08:34:29.570146   13809 manager.go:932] Added container: "/system.slice/systemd-journal-flush.service" (aliases: [], namespace: "")
I1207 08:34:29.570313   13809 handler.go:325] Added event &{/system.slice/systemd-journal-flush.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.570356   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
I1207 08:34:29.570389   13809 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-tmpfiles-setup-dev.service: /system.slice/systemd-tmpfiles-setup-dev.service not handled by systemd handler
I1207 08:34:29.570411   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/systemd-tmpfiles-setup-dev.service"
I1207 08:34:29.570428   13809 factory.go:112] Using factory "raw" for container "/system.slice/systemd-tmpfiles-setup-dev.service"
I1207 08:34:29.570516   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-journal-flush.service"
I1207 08:34:29.570625   13809 manager.go:932] Added container: "/system.slice/systemd-tmpfiles-setup-dev.service" (aliases: [], namespace: "")
I1207 08:34:29.570851   13809 handler.go:325] Added event &{/system.slice/systemd-tmpfiles-setup-dev.service 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.570882   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:34:29.570891   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:34:29.570932   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:34:29.570973   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/snapd.seeded.service"
I1207 08:34:29.571013   13809 factory.go:105] Error trying to work out if we can handle /system.slice/snapd.seeded.service: /system.slice/snapd.seeded.service not handled by systemd handler
I1207 08:34:29.571044   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/snapd.seeded.service"
I1207 08:34:29.571076   13809 factory.go:112] Using factory "raw" for container "/system.slice/snapd.seeded.service"
I1207 08:34:29.571053   13809 container.go:409] Start housekeeping for container "/system.slice/systemd-tmpfiles-setup-dev.service"
I1207 08:34:29.571470   13809 manager.go:932] Added container: "/system.slice/snapd.seeded.service" (aliases: [], namespace: "")
I1207 08:34:29.571746   13809 handler.go:325] Added event &{/system.slice/snapd.seeded.service 2021-11-30 02:07:55.076346668 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.571864   13809 container.go:409] Start housekeeping for container "/system.slice/snapd.seeded.service"
I1207 08:34:29.573105   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"
I1207 08:34:29.574183   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b" (aliases: [k8s_POD_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_0 50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b], namespace: "docker")
I1207 08:34:29.574444   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b 2021-12-07 08:27:21.664518128 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.574475   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/cloud-init.service"
I1207 08:34:29.574505   13809 factory.go:105] Error trying to work out if we can handle /system.slice/cloud-init.service: /system.slice/cloud-init.service not handled by systemd handler
I1207 08:34:29.574512   13809 factory.go:116] Factory "systemd" was unable to handle container "/system.slice/cloud-init.service"
I1207 08:34:29.574519   13809 factory.go:112] Using factory "raw" for container "/system.slice/cloud-init.service"
I1207 08:34:29.574745   13809 manager.go:932] Added container: "/system.slice/cloud-init.service" (aliases: [], namespace: "")
I1207 08:34:29.574926   13809 handler.go:325] Added event &{/system.slice/cloud-init.service 2021-11-30 02:07:55.072346301 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.574957   13809 factory.go:116] Factory "docker" was unable to handle container "/WALinuxAgent"
I1207 08:34:29.574971   13809 factory.go:105] Error trying to work out if we can handle /WALinuxAgent: /WALinuxAgent not handled by systemd handler
I1207 08:34:29.574981   13809 factory.go:116] Factory "systemd" was unable to handle container "/WALinuxAgent"
I1207 08:34:29.574988   13809 factory.go:112] Using factory "raw" for container "/WALinuxAgent"
I1207 08:34:29.575181   13809 manager.go:932] Added container: "/WALinuxAgent" (aliases: [], namespace: "")
I1207 08:34:29.575340   13809 handler.go:325] Added event &{/WALinuxAgent 2021-11-30 02:07:55.080347035 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.575369   13809 factory.go:116] Factory "docker" was unable to handle container "/docker"
I1207 08:34:29.575396   13809 factory.go:105] Error trying to work out if we can handle /docker: /docker not handled by systemd handler
I1207 08:34:29.575413   13809 factory.go:116] Factory "systemd" was unable to handle container "/docker"
I1207 08:34:29.575437   13809 factory.go:112] Using factory "raw" for container "/docker"
I1207 08:34:29.575664   13809 manager.go:932] Added container: "/docker" (aliases: [], namespace: "")
I1207 08:34:29.575890   13809 handler.go:325] Added event &{/docker 2021-11-30 02:07:55.068345933 +0000 UTC containerCreation {<nil>}}
I1207 08:34:29.575915   13809 manager.go:316] Recovery completed
I1207 08:34:29.577444   13809 container.go:409] Start housekeeping for container "/system.slice/cloud-init.service"
I1207 08:34:29.578046   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"
I1207 08:34:29.579693   13809 container.go:409] Start housekeeping for container "/WALinuxAgent"
I1207 08:34:29.588113   13809 container.go:409] Start housekeeping for container "/docker"
I1207 08:34:29.649282   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.679183   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:34:29.679208   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:34:29.679217   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:34:29.679223   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:34:29.679231   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:34:29.679240   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:34:29.679248   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:34:29.679256   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:34:29.679264   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:34:29.679271   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:34:29.679278   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:34:29.679286   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:34:29.679298   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:34:29.679304   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:34:29.679310   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:34:29.679315   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:34:29.679321   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:34:29.679327   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:34:29.679333   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:34:29.679340   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:34:29.679349   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:34:29.679356   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:34:29.679371   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:34:29.679378   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:34:29.679382   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:34:29.679418   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:34:29.679436   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:34:29.679450   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:34:29.679459   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:34:29.679473   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:34:29.679483   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:34:29.679492   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:34:29.679502   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:34:29.679522   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:34:29.679537   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:34:29.679553   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:34:29.679566   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:34:29.679593   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:34:29.679607   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:34:29.679614   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:34:29.679622   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:34:29.679630   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:34:29.679638   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:34:29.679645   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:34:29.679653   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:34:29.679659   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:34:29.679690   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:34:29.679706   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:34:29.679713   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:34:29.679723   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:34:29.679734   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:34:29.679744   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:34:29.679751   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:34:29.679760   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:34:29.679766   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:34:29.679788   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:34:29.679812   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:34:29.679828   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:34:29.679842   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:34:29.679864   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:34:29.679876   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:34:29.679883   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:34:29.679891   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:34:29.679896   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:34:29.679903   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:34:29.679911   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:34:29.679967   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:34:29.696416   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15229140Ki, capacity: 16426204Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:29.696449   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33477192Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:29.696459   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:29.696471   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33477192Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:29.696480   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:29.696493   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16297536Ki, capacity: 16426204Ki
W1207 08:34:29.696509   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:34:29.696532   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:34:29.749256   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.849324   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:29.949353   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.049315   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.149357   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.249312   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.349456   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.443668   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:30.449154   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.549331   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.649370   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.749332   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.849355   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:30.949320   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.049416   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.149280   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.249313   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.349294   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.448015   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:31.449245   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.549294   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.649323   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.749427   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.849406   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:31.949335   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.049370   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.149315   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.249366   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.349356   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.449322   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.454621   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:32.549455   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.649349   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.749468   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.849478   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:32.949328   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.049335   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.149431   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.249358   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.349348   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.449324   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.459607   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:33.549310   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.649325   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.749455   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.849438   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:33.949350   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.049430   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.149300   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.249315   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.349419   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.349619   13809 config.go:101] Looking for [api file], have seen map[]
I1207 08:34:34.349696   13809 kubelet.go:1911] SyncLoop (housekeeping, skipped): sources aren't ready yet.
I1207 08:34:34.349739   13809 kubelet.go:1837] SyncLoop (ADD, "file"): ""
I1207 08:34:34.349803   13809 kubelet.go:1875] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47"}
I1207 08:34:34.349897   13809 kubelet.go:1875] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864"}
I1207 08:34:34.349943   13809 kubelet.go:1875] SyncLoop (PLEG): ignore irrelevant event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731"}
I1207 08:34:34.349999   13809 kubelet.go:1837] SyncLoop (ADD, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde), caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde), kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde), shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.350450   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14"}
I1207 08:34:34.350557   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.350665   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.350496   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.350558   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.350469   13809 kubelet_pods.go:1294] Generating status for "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:34:34.350777   13809 status_manager.go:340] Status Manager: adding pod: "7c988aa2-5737-11ec-9d1e-002248057bde", with status: ('\x01', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.240.67.5 2021-12-07 08:27:16 +0000 UTC [] [{logger {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] Burstable}) to podStatusChannel
I1207 08:34:34.351040   13809 status_manager.go:147] Status Manager: syncing pod: "7c988aa2-5737-11ec-9d1e-002248057bde", with status: (1, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.240.67.5 2021-12-07 08:27:16 +0000 UTC [] [{logger {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] Burstable}) from podStatusChannel
I1207 08:34:34.351269   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.351333   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd
I1207 08:34:34.351349   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.351362   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.351375   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.351492   13809 status_manager.go:340] Status Manager: adding pod: "7df95486-5737-11ec-9d1e-002248057bde", with status: ('\x01', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }]   10.240.67.5 10.244.35.253 2021-12-07 08:27:19 +0000 UTC [] [{infra {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] BestEffort}) to podStatusChannel
E1207 08:34:34.351650   13809 pod_workers.go:182] Error syncing pod 7df95486-5737-11ec-9d1e-002248057bde ("caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"), skipping: network is not ready: [runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR]
I1207 08:34:34.351692   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479"}
I1207 08:34:34.351731   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.351886   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.351881   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"}
I1207 08:34:34.351902   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.351914   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.351929   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.351914   13809 kubelet.go:1871] SyncLoop (PLEG): "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7df95486-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863"}
I1207 08:34:34.351942   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.351958   13809 kubelet.go:1871] SyncLoop (PLEG): "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7df95486-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826"}
I1207 08:34:34.352001   13809 kubelet.go:1871] SyncLoop (PLEG): "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"83d280aa-8ff5-11ea-a482-002248057bde", Type:"ContainerDied", Data:"a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844"}
I1207 08:34:34.352022   13809 kubelet_pods.go:1294] Generating status for "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:34:34.352077   13809 kubelet.go:1871] SyncLoop (PLEG): "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"83d280aa-8ff5-11ea-a482-002248057bde", Type:"ContainerDied", Data:"a9ec477cf43e44591814f19311555d74e884af3490302c58f069cfdfbe31c732"}
I1207 08:34:34.352099   13809 kubelet_pods.go:1294] Generating status for "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
W1207 08:34:34.352139   13809 pod_container_deletor.go:77] Container "a9ec477cf43e44591814f19311555d74e884af3490302c58f069cfdfbe31c732" not found in pod's containers
I1207 08:34:34.352162   13809 kubelet.go:1871] SyncLoop (PLEG): "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c988aa2-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c"}
I1207 08:34:34.352205   13809 kubelet.go:1871] SyncLoop (PLEG): "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c988aa2-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c"}
I1207 08:34:34.352252   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf", UID:"7df95486-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484049", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:34.352288   13809 status_manager.go:340] Status Manager: adding pod: "83d280aa-8ff5-11ea-a482-002248057bde", with status: ('\x01', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 23:58:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-07 00:50:44 +0000 UTC ContainersNotReady containers with unready status: [kube-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 23:58:50 +0000 UTC  }]   10.240.67.5 10.240.67.5 2020-05-06 23:58:43 +0000 UTC [] [{kube-proxy {nil nil &ContainerStateTerminated{ExitCode:255,Signal:0,Reason:Error,Message:,StartedAt:2020-05-07 00:50:44 +0000 UTC,FinishedAt:2020-05-11 00:58:57 +0000 UTC,ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844,}} {nil nil nil} false 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844}] BestEffort}) to podStatusChannel
I1207 08:34:34.352618   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x01', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1 c2 c3]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} {nil nil nil} false 5 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {c2 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {c3 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
E1207 08:34:34.352829   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: network is not ready: [runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR]
I1207 08:34:34.352926   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:34.352974   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:34:34.360381   13809 round_trippers.go:439] Response Status: 200 OK in 8 milliseconds
I1207 08:34:34.360672   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/status
I1207 08:34:34.360691   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.360699   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.360706   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.360720   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.361397   13809 round_trippers.go:439] Response Status: 201 Created in 9 milliseconds
I1207 08:34:34.361532   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.361549   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.361560   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.361567   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.361573   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.363755   13809 config.go:282] Setting pods for source api
I1207 08:34:34.363932   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:34.364575   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.365235   13809 status_manager.go:451] Status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)" updated successfully: (1, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable})
I1207 08:34:34.365367   13809 status_manager.go:147] Status Manager: syncing pod: "7df95486-5737-11ec-9d1e-002248057bde", with status: (1, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }]   10.240.67.5 10.244.35.253 2021-12-07 08:27:19 +0000 UTC [] [{infra {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] BestEffort}) from podStatusChannel
I1207 08:34:34.365516   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf
I1207 08:34:34.365526   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.365531   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.365536   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.369622   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:34.369800   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf/status
I1207 08:34:34.369816   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.369822   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.369828   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.369835   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.371728   13809 round_trippers.go:439] Response Status: 201 Created in 10 milliseconds
I1207 08:34:34.376403   13809 round_trippers.go:439] Response Status: 200 OK in 6 milliseconds
I1207 08:34:34.376503   13809 status_manager.go:451] Status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)" updated successfully: (1, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort})
I1207 08:34:34.376629   13809 config.go:282] Setting pods for source api
I1207 08:34:34.376642   13809 status_manager.go:147] Status Manager: syncing pod: "83d280aa-8ff5-11ea-a482-002248057bde", with status: (1, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 23:58:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-05-07 00:50:44 +0000 UTC ContainersNotReady containers with unready status: [kube-proxy]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-05-06 23:58:50 +0000 UTC  }]   10.240.67.5 10.240.67.5 2020-05-06 23:58:43 +0000 UTC [] [{kube-proxy {nil nil &ContainerStateTerminated{ExitCode:255,Signal:0,Reason:Error,Message:,StartedAt:2020-05-07 00:50:44 +0000 UTC,FinishedAt:2020-05-11 00:58:57 +0000 UTC,ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844,}} {nil nil nil} false 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844}] BestEffort}) from podStatusChannel
I1207 08:34:34.376748   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:34:34.376759   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.376766   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.376773   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.377508   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.378111   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
W1207 08:34:34.378262   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:34:34.378332   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (1, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1 c2 c3]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} {nil nil nil} false 5 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {c2 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {c3 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:34:34.378528   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:34:34.378542   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.378549   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.378556   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.382255   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:34.382534   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:34:34.382550   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.382555   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.382560   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.382564   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.385887   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:34.386380   13809 config.go:282] Setting pods for source api
I1207 08:34:34.386462   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (1, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1 c2 c3]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:5 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {Name:c2 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {Name:c3 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:34:34.387399   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.449343   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:34.463539   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:34.472023   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.472048   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.472229   13809 status_manager.go:325] Ignoring same status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort}
E1207 08:34:34.472335   13809 pod_workers.go:182] Error syncing pod 7df95486-5737-11ec-9d1e-002248057bde ("caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"), skipping: network is not ready: [runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR]
I1207 08:34:34.472411   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf", UID:"7df95486-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484049", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:34.472541   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1 c2 c3]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:5 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {Name:c2 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {Name:c3 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:34:34.472629   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf.16be6c02f42c0c58
I1207 08:34:34.472647   13809 round_trippers.go:421] Request Headers:
E1207 08:34:34.472646   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: network is not ready: [runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: Kubenet does not have netConfig. This is most likely due to lack of PodCIDR]
I1207 08:34:34.472655   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:34.472663   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.472675   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.472705   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:34.478252   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:34:34.478524   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c02f43e049f
I1207 08:34:34.478540   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.478546   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.478550   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.478555   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:34.483426   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:34.549523   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.549593   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ssl-certs-host" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-ssl-certs-host") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.549707   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.549889   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550012   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "logfiles-containers-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-containers-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550196   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "logfiles-pods-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-pods-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550288   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "logfiles-docker-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-docker-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550359   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "certificates" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-certificates") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550458   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-kubeconfig") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.550509   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etc-kubernetes" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-etc-kubernetes") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.550566   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550672   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "mdsd-logs" (UniqueName: "kubernetes.io/empty-dir/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-logs") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550754   13809 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-kubeconfig") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.550783   13809 reconciler.go:159] Desired state of world has been populated with pods, starting reconstruct state function
I1207 08:34:34.551360   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-kubeconfig") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.551416   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "etc-kubernetes" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-etc-kubernetes") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.551484   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551542   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-kubeconfig") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.551556   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "mdsd-logs" (UniqueName: "kubernetes.io/empty-dir/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-logs") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551589   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:34:34.551616   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-kubeconfig") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551677   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "logfiles-docker-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-docker-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551711   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:34.551729   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.551742   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.551751   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "certificates" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-certificates") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551754   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.551768   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.551821   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/mdsd-config
I1207 08:34:34.551838   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.551846   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.551854   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.551862   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:34:34.551826   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.551944   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "ssl-certs-host" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-ssl-certs-host") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.551952   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "kubeconfig" 
I1207 08:34:34.552011   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.552057   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/default-token-74mlg
I1207 08:34:34.552082   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.552086   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552095   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.552108   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.552138   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "logfiles-containers-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-containers-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552151   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-kubeconfig") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552189   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "logfiles-pods-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-pods-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552191   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "kubeconfig" 
I1207 08:34:34.552274   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "logfiles-pods-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-pods-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552311   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "logfiles-docker-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-docker-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552308   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "logfiles-pods-volume" 
I1207 08:34:34.552314   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-logs" (UniqueName: "kubernetes.io/empty-dir/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-logs") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552336   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "mdsd-logs" 
I1207 08:34:34.552360   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:34.552378   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:34.552423   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "certificates" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-certificates") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552452   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "certificates" 
I1207 08:34:34.552484   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "logfiles-docker-volume" 
I1207 08:34:34.552529   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:34:34.552541   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.552552   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.552562   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "logfiles-containers-volume" (UniqueName: "kubernetes.io/host-path/7c988aa2-5737-11ec-9d1e-002248057bde-logfiles-containers-volume") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.552564   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.552585   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "ssl-certs-host" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-ssl-certs-host") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.552606   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "etc-kubernetes" (UniqueName: "kubernetes.io/host-path/83d280aa-8ff5-11ea-a482-002248057bde-etc-kubernetes") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:34.552616   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "ssl-certs-host" 
I1207 08:34:34.552643   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "logfiles-containers-volume" 
I1207 08:34:34.552670   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "etc-kubernetes" 
I1207 08:34:34.552530   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:34.552718   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.552731   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.552744   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.554330   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:34.555025   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c030015177c", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"SuccessfulMountVolume", Message:"MountVolume.SetUp succeeded for volume \"kubeconfig\" ", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:551523196, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:551523196, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
I1207 08:34:34.555265   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.555289   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.555301   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.555320   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.555359   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.555357   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:34.556255   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:34:34.556415   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:34:34.556438   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:34:34.556446   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:34:34.556454   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:34.556514   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:34:34.556518   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:34:34.556568   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.556601   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:34:34.556605   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "mdsd-data" 
I1207 08:34:34.556621   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:34:34.556628   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:34:34.556658   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:34.556717   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.556745   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "aci-metadata-volume" 
I1207 08:34:34.556796   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:34.556864   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:34:34.556976   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:34:34.556986   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:34:34.556991   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:34:34.557051   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:34:34.557083   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:34.557104   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd", UID:"7c988aa2-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094484050", FieldPath:""}): type: 'Normal' reason: 'SuccessfulMountVolume' MountVolume.SetUp succeeded for volume "default-token-74mlg" 
I1207 08:34:34.557351   13809 round_trippers.go:439] Response Status: 403 Forbidden in 4 milliseconds
E1207 08:34:34.557480   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:34.557564   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:34:35.057543461 +0000 UTC (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:34.557584   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:34.568160   13809 round_trippers.go:439] Response Status: 201 Created in 12 milliseconds
I1207 08:34:34.568286   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.568321   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.568330   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.568338   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.568358   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.580450   13809 round_trippers.go:439] Response Status: 201 Created in 12 milliseconds
I1207 08:34:34.580565   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.580581   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.580588   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.580594   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.580647   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.585359   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:34.585524   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.585557   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.585577   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.585589   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.585596   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.590659   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:34.590767   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.590782   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.590790   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.590798   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.590811   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.594875   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:34.595000   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:34.595019   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.595026   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.595032   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.595039   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.603130   13809 round_trippers.go:439] Response Status: 403 Forbidden in 8 milliseconds
E1207 08:34:34.603224   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300253138", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"SuccessfulMountVolume", Message:"MountVolume.SetUp succeeded for volume \"ssl-certs-host\" ", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:552578360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:552578360, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
I1207 08:34:34.603352   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.603369   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.603376   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.603383   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.603389   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.608590   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:34.651507   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.651550   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[cpu:{{200 -3} {<nil>} 200m DecimalSI} memory:{{1 9} {<nil>} 1G DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:34:34.672274   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:34.681117   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:34.797999   13809 request.go:462] Throttling request took 189.295278ms, request: POST:https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:34.798075   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:34.798085   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.798093   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.798104   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:34.798116   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.801129   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:34.801245   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300257d65", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"SuccessfulMountVolume", Message:"MountVolume.SetUp succeeded for volume \"etc-kubernetes\" ", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:552597861, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:552597861, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
I1207 08:34:34.997916   13809 request.go:462] Throttling request took 196.448512ms, request: POST:https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.997991   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:34.998002   13809 round_trippers.go:421] Request Headers:
I1207 08:34:34.998011   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:34.998023   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:34.998035   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.003289   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:35.154629   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:35.154725   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:35.154996   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:35.155020   13809 round_trippers.go:421] Request Headers:
I1207 08:34:35.155031   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:35.155040   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.157067   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:35.157197   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:35.157343   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:34:36.157300426 +0000 UTC (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:35.157425   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:35.197907   13809 request.go:462] Throttling request took 194.463891ms, request: POST:https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:35.198009   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:35.198029   13809 round_trippers.go:421] Request Headers:
I1207 08:34:35.198044   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:35.198065   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:35.198078   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.202608   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:35.349524   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:35.349586   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:35.397983   13809 request.go:462] Throttling request took 195.180334ms, request: POST:https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:35.398056   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:35.398065   13809 round_trippers.go:421] Request Headers:
I1207 08:34:35.398076   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.398089   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:35.398100   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:35.403108   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:35.472312   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:35.478186   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.478190   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.478420   13809 status_manager.go:325] Ignoring same status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable}
I1207 08:34:35.478808   13809 status_manager.go:325] Ignoring same status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort}
I1207 08:34:35.479043   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.479170   13809 kuberuntime_manager.go:428] Syncing Pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,GenerateName:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,UID:7df95486-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484049,Generation:0,CreationTimestamp:2021-12-07 08:27:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{app: caas,deploymenttype: infra,pod-template-hash: 2462294644,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},Annotations:map[string]string{kubernetes.io/config.seen: 2021-12-07T08:34:29.329958181Z,kubernetes.io/config.source: api,kubernetes.io/created-by: {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"caas-20d222f17dd4489fb180b51031b406f6","name":"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88","uid":"7df8c8a0-5737-11ec-9d1e-002248057bde","apiVersion":"extensions","resourceVersion":"1094476293"}}
,localRegistry-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[{extensions/v1beta1 ReplicaSet caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88 7df8c8a0-5737-11ec-9d1e-002248057bde 0xc4202cdbd1 0xc4202cdbd2}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{infra mcr.microsoft.com/aci/infra:master_20211112.1 [/bin/sh -c while true; do echo `date`; sleep 1000000; done] []  [] [] [] {map[] map[]} [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[{[{agentpool NotIn [agentpool1 system]} {node-optimization-needed DoesNotExist []} {kubernetes.io/hostname In [k8s-agentpool4-a14krc0l-17]}]}],},PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAffinity:nil,PodAntiAffinity:&PodAntiAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[{app Exists []}],} [caas-827aef2dda4640159dd2550e4e471cf3 caas-8dca1f0909804431b986fc0cca798ca3 caas-b3d11bb4bae449faa2674fa20dee69cb caas-b516ef0c44d44dba913d83b46a14454c certbootstrap default kube-public kube-system] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.253,StartTime:2021-12-07 08:27:19 +0000 UTC,ContainerStatuses:[{infra {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}],QOSClass:BestEffort,InitContainerStatuses:[],},}
I1207 08:34:35.479733   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.556227   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:34:35.556358   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:34:35.556408   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:34:35.556461   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:34:35.556515   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:34:35.556572   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:34:35.556732   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:34:35.556753   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:34:35.556762   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:34:35.556776   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:34:35.556779   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:34:35.556791   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:34:35.556914   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:34:35.556960   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:35.556971   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:34:35.557005   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:34:35.597848   13809 request.go:462] Throttling request took 194.534695ms, request: POST:https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:35.597947   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:34:35.597974   13809 round_trippers.go:421] Request Headers:
I1207 08:34:35.597987   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:35.598008   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:35.598021   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.599864   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:35.600007   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Warning"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
I1207 08:34:35.779490   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.779549   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[memory:{{1 9} {<nil>} 1G DecimalSI} cpu:{{200 -3} {<nil>} 200m DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:34:35.780521   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:34:35.797893   13809 request.go:462] Throttling request took 197.393068ms, request: PATCH:https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:35.797963   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:35.797978   13809 round_trippers.go:421] Request Headers:
I1207 08:34:35.797987   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:35.798000   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:35.798011   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:35.800731   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:35.800837   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462875, nsec:157272024, loc:(*time.Location)(0x9e22280)}}, Count:2, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:34:36.158841   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:36.158941   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:36.159217   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:36.159244   13809 round_trippers.go:421] Request Headers:
I1207 08:34:36.159258   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:36.159271   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:36.161670   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:36.161769   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:36.161865   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:34:38.161836533 +0000 UTC (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:36.161892   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:36.162180   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:36.162197   13809 round_trippers.go:421] Request Headers:
I1207 08:34:36.162205   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:36.162214   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:36.162223   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:36.164307   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:36.164395   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462876, nsec:161817532, loc:(*time.Location)(0x9e22280)}}, Count:3, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:34:36.478452   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:37.349551   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:37.349619   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:37.485058   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:38.166995   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:38.167082   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:38.167358   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:38.167386   13809 round_trippers.go:421] Request Headers:
I1207 08:34:38.167401   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:38.167414   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:38.169902   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:38.170105   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:38.170266   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:34:42.170223303 +0000 UTC (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:38.170351   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:38.170744   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:38.170776   13809 round_trippers.go:421] Request Headers:
I1207 08:34:38.170789   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:38.170803   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:38.170821   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:38.173096   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:38.173247   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462878, nsec:170192901, loc:(*time.Location)(0x9e22280)}}, Count:4, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:34:38.491167   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:39.349575   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:39.349613   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:39.349642   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:39.349745   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:34:39.349769   13809 round_trippers.go:421] Request Headers:
I1207 08:34:39.349783   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:39.349797   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:39.352063   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:34:39.352239   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:34:39.497746   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:39.505354   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:34:39.505370   13809 round_trippers.go:421] Request Headers:
I1207 08:34:39.505402   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:39.505413   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:39.507456   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:34:39.513304   13809 kubelet_node_status.go:443] Recording NodeReady event message for node k8s-agentpool4-a14krc0l-17
I1207 08:34:39.513458   13809 server.go:227] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeReady' Node k8s-agentpool4-a14krc0l-17 status is now: NodeReady
I1207 08:34:39.513508   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/default/events
I1207 08:34:39.513520   13809 round_trippers.go:421] Request Headers:
I1207 08:34:39.513526   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:39.513533   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:39.513539   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:39.514515   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:34:39.514528   13809 round_trippers.go:421] Request Headers:
I1207 08:34:39.514533   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:39.514537   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:39.514542   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:39.515461   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:39.515895   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-agentpool4-a14krc0l-17.16be6c0427d41e49", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool4-a14krc0l-17", UID:"k8s-agentpool4-a14krc0l-17", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeReady", Message:"Node k8s-agentpool4-a14krc0l-17 status is now: NodeReady", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462879, nsec:513321033, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462879, nsec:513321033, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Normal"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "default"' (will not retry!)
I1207 08:34:39.519968   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:34:39.682539   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:39.696837   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:34:39.713514   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33477192Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:39.713543   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:39.713551   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33477192Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:39.713557   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
I1207 08:34:39.713563   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16297536Ki, capacity: 16426204Ki
I1207 08:34:39.713567   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15229140Ki, capacity: 16426204Ki, time: 2021-12-07 08:34:29.401498119 +0000 UTC
W1207 08:34:39.713575   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:34:39.713589   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:34:40.503091   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:41.349510   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:41.349574   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:41.509431   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:42.182486   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:42.182559   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:42.182801   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:42.182830   13809 round_trippers.go:421] Request Headers:
I1207 08:34:42.182842   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:42.182855   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:42.185007   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:42.185197   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:42.185340   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:34:50.185297825 +0000 UTC (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:42.185665   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:42.185806   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:42.185840   13809 round_trippers.go:421] Request Headers:
I1207 08:34:42.185855   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:42.185872   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:42.185887   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:42.187359   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:42.187455   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462882, nsec:185269224, loc:(*time.Location)(0x9e22280)}}, Count:5, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:34:42.515644   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:43.349556   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:43.349631   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:43.521939   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:44.528258   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:44.684075   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:45.349539   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:45.349603   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:45.534261   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:46.539036   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:47.349529   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:47.349608   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:47.349681   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:47.349763   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:47.350097   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1 c2 c3]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:5 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {Name:c2 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {Name:c3 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:34:47.350704   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:47.402337   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:34:47.402415   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:47.402545   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:34:47.402698   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:34:47.402727   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:34:47.402740   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:34:47.402801   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:47.402844   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:47.545489   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:47.651080   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:47.651116   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1 c2 c3]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} {nil nil nil} false 5 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {c2 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {c3 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:34:47.651579   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:34:47.651619   13809 kuberuntime_manager.go:500] Container {Name:c2 Image:runjivu/hyperkube-amd64:v1.8.4 Command:[/bin/sleep inf] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:600 scale:6} d:{Dec:<nil>} s:600M Format:DecimalSI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:500 scale:6} d:{Dec:<nil>} s:500M Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:34:47.651666   13809 kuberuntime_manager.go:500] Container {Name:c3 Image:runjivu/hyperkube-amd64:v1.8.4 Command:[/bin/sleep inf] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:600 scale:6} d:{Dec:<nil>} s:600M Format:DecimalSI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:500 scale:6} d:{Dec:<nil>} s:500M Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:34:47.651849   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0 1 2] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:34:47.651916   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:47.651986   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:47.652080   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:47.654034   13809 provider.go:119] Refreshing cache for provider: *credentialprovider.defaultDockerConfigProvider
I1207 08:34:47.654026   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hubrepo:5736"
I1207 08:34:47.654077   13809 config.go:131] looking for config.json at /var/lib/kubelet/config.json
I1207 08:34:47.654122   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:47.654125   13809 config.go:131] looking for config.json at /config.json
I1207 08:34:47.654141   13809 round_trippers.go:421] Request Headers:
I1207 08:34:47.654149   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:47.654156   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:47.654162   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:47.654168   13809 config.go:131] looking for config.json at /root/.docker/config.json
I1207 08:34:47.654207   13809 config.go:131] looking for config.json at /.docker/config.json
I1207 08:34:47.654263   13809 config.go:101] looking for .dockercfg at /var/lib/kubelet/.dockercfg
I1207 08:34:47.654289   13809 config.go:101] looking for .dockercfg at /.dockercfg
I1207 08:34:47.654312   13809 config.go:101] looking for .dockercfg at /root/.dockercfg
I1207 08:34:47.654329   13809 config.go:101] looking for .dockercfg at /.dockercfg
I1207 08:34:47.654348   13809 provider.go:89] Unable to parse Docker config file: couldn't find valid .dockercfg after checking in [/var/lib/kubelet  /root /]
I1207 08:34:47.654372   13809 kuberuntime_image.go:46] Pulling image "runjivu/hubrepo:5736" without credentials
I1207 08:34:47.658958   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:48.551502   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:49.349503   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:49.349585   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:49.349605   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:49.349746   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:34:49.349769   13809 round_trippers.go:421] Request Headers:
I1207 08:34:49.349785   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:49.349799   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:49.351821   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:34:49.351964   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:34:49.520570   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:34:49.520602   13809 round_trippers.go:421] Request Headers:
I1207 08:34:49.520613   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:49.520624   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:49.523300   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:34:49.530363   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:34:49.530381   13809 round_trippers.go:421] Request Headers:
I1207 08:34:49.530388   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:49.530395   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:49.530402   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:49.535138   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:49.556394   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:49.685615   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:49.713878   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:34:49.732879   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:42.78595454 +0000 UTC
I1207 08:34:49.732912   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33476572Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:42.78595454 +0000 UTC
I1207 08:34:49.732919   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618532, capacity: 6250Ki, time: 2021-12-07 08:34:42.78595454 +0000 UTC
I1207 08:34:49.732927   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16297536Ki, capacity: 16426204Ki
I1207 08:34:49.732932   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15216496Ki, capacity: 16426204Ki, time: 2021-12-07 08:34:42.78595454 +0000 UTC
I1207 08:34:49.732938   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33476572Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:42.78595454 +0000 UTC
W1207 08:34:49.732948   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:34:49.732964   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:34:49.958060   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hubrepo:5736": "Status: Image is up to date for runjivu/hubrepo:5736"
I1207 08:34:49.960174   13809 kuberuntime_container.go:100] Generating ref for container c1: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}
I1207 08:34:49.960234   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:34:49.960307   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:49.960371   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:34:49.960419   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c1" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:34:49.960439   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:49.960476   13809 round_trippers.go:421] Request Headers:
I1207 08:34:49.960403   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hubrepo:5736"
I1207 08:34:49.960497   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:49.960512   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:49.960523   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:49.965271   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:49.969423   13809 round_trippers.go:439] Response Status: 201 Created in 8 milliseconds
I1207 08:34:50.213280   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:34:50.213344   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:34:50.213567   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:34:50.213588   13809 round_trippers.go:421] Request Headers:
I1207 08:34:50.213596   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:50.213604   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:50.216531   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:34:50.216649   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:34:50.216739   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:35:06.216714597 +0000 UTC (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:50.217004   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:34:50.217020   13809 round_trippers.go:421] Request Headers:
I1207 08:34:50.217025   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:50.217030   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:50.217035   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:50.217150   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:34:50.218877   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:34:50.218942   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:34:50.219133   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462890, nsec:216696096, loc:(*time.Location)(0x9e22280)}}, Count:6, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:34:50.219304   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:50.219318   13809 round_trippers.go:421] Request Headers:
I1207 08:34:50.219326   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:50.219334   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:50.219342   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:50.224674   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:50.352244   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830"
I1207 08:34:50.353183   13809 kuberuntime_manager.go:739] checking backoff for container "c2" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:50.353287   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c2,Image:runjivu/hyperkube-amd64:v1.8.4,Command:[/bin/sleep inf],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{600 6} {<nil>} 600M DecimalSI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{500 6} {<nil>} 500M DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:50.353522   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:50.353536   13809 round_trippers.go:421] Request Headers:
I1207 08:34:50.353541   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:50.353551   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:50.353556   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:50.353664   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Started' Started container
I1207 08:34:50.355255   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c2}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:50.355278   13809 kuberuntime_image.go:46] Pulling image "runjivu/hyperkube-amd64:v1.8.4" without credentials
W1207 08:34:50.356013   13809 container.go:354] Failed to create summary reader for "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830": none of the resources are being tracked.
I1207 08:34:50.356073   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_6 e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830], namespace: "docker")
I1207 08:34:50.356164   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830 0001-01-01 00:00:00 +0000 UTC containerCreation {<nil>}}
I1207 08:34:50.356195   13809 manager.go:989] Destroyed container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_6 e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830], namespace: "docker")
I1207 08:34:50.356209   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830 2021-12-07 08:34:50.356206353 +0000 UTC containerDeletion {<nil>}}
I1207 08:34:50.356231   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830"
I1207 08:34:50.358825   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:50.359118   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:50.359134   13809 round_trippers.go:421] Request Headers:
I1207 08:34:50.359146   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:50.359154   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:50.359161   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:50.365608   13809 round_trippers.go:439] Response Status: 201 Created in 6 milliseconds
I1207 08:34:50.560736   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:50.567560   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830: non-existent -> exited
I1207 08:34:50.569040   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:50.589565   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc422187880), (*container.ContainerStatus)(0xc421a4ae00), (*container.ContainerStatus)(0xc421429340), (*container.ContainerStatus)(0xc421a4aee0), (*container.ContainerStatus)(0xc421429420), (*container.ContainerStatus)(0xc4200f61c0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4212c55e0)}} (err: <nil>)
I1207 08:34:50.589637   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830"}
I1207 08:34:50.589677   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:50.589848   13809 kuberuntime_container.go:807] Removing container "d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47"
I1207 08:34:51.349572   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:51.349635   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:51.589933   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:51.595657   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47: exited -> non-existent
I1207 08:34:51.597176   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:51.614744   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4208ee700), (*container.ContainerStatus)(0xc4200f7180), (*container.ContainerStatus)(0xc4200f7260), (*container.ContainerStatus)(0xc4208ee7e0), (*container.ContainerStatus)(0xc4200f7340)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc421367f90)}} (err: <nil>)
I1207 08:34:52.615072   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:52.692485   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hyperkube-amd64:v1.8.4": "Status: Image is up to date for runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:52.694590   13809 kuberuntime_container.go:100] Generating ref for container c2: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c2}"}
I1207 08:34:52.694648   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:34:52.694740   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:52.694789   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c2 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:34:52.694779   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c2}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:52.694816   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c2" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:34:52.694885   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:52.694905   13809 round_trippers.go:421] Request Headers:
I1207 08:34:52.694913   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:52.694920   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:52.694930   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:52.696739   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:52.700520   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:52.957020   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c2}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:34:52.957084   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:52.957097   13809 round_trippers.go:421] Request Headers:
I1207 08:34:52.957104   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:52.957111   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:52.957116   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:52.962480   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:53.118519   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974"
I1207 08:34:53.119185   13809 kuberuntime_manager.go:739] checking backoff for container "c3" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:53.119323   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:53.119341   13809 round_trippers.go:421] Request Headers:
I1207 08:34:53.119348   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:53.119357   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:53.119363   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:53.119382   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c3,Image:runjivu/hyperkube-amd64:v1.8.4,Command:[/bin/sleep inf],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{600 6} {<nil>} 600M DecimalSI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{500 6} {<nil>} 500M DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:53.119495   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c2}"}): type: 'Normal' reason: 'Started' Started container
I1207 08:34:53.121224   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c3}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:53.121269   13809 kuberuntime_image.go:46] Pulling image "runjivu/hyperkube-amd64:v1.8.4" without credentials
I1207 08:34:53.121265   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974" (aliases: [k8s_c2_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_1 6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974], namespace: "docker")
I1207 08:34:53.121625   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974 2021-12-07 08:34:52.996020166 +0000 UTC containerCreation {<nil>}}
I1207 08:34:53.121712   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974"
I1207 08:34:53.124703   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:53.124926   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:53.124943   13809 round_trippers.go:421] Request Headers:
I1207 08:34:53.124951   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:53.124959   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:53.124967   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:53.129373   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:53.349597   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:53.349673   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:53.621834   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:53.628217   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974: non-existent -> running
I1207 08:34:53.629804   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:53.644116   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4212e3260), (*container.ContainerStatus)(0xc420a63c00), (*container.ContainerStatus)(0xc4212e3340), (*container.ContainerStatus)(0xc420a63dc0), (*container.ContainerStatus)(0xc4212e3420), (*container.ContainerStatus)(0xc420a63ea0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc420a7f450)}} (err: <nil>)
I1207 08:34:53.644184   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974"}
I1207 08:34:54.644429   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:54.687312   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:55.209427   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc4212f3700), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger"}
I1207 08:34:55.217864   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c988aa2-5737-11ec-9d1e-002248057bde/logger_0.log"
I1207 08:34:55.217902   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger?limitBytes=67104768&tailLines=100&timestamps=True: (8.58122ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:34:55.349514   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:34:55.349566   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:55.470871   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hyperkube-amd64:v1.8.4": "Status: Image is up to date for runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:55.472819   13809 kuberuntime_container.go:100] Generating ref for container c3: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c3}"}
I1207 08:34:55.472883   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:34:55.472957   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:55.473006   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c3}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hyperkube-amd64:v1.8.4"
I1207 08:34:55.473157   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:55.473194   13809 round_trippers.go:421] Request Headers:
I1207 08:34:55.473210   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:55.473217   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c3 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:34:55.473230   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:55.473244   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:55.473263   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c3" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:34:55.476544   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:34:55.479353   13809 round_trippers.go:439] Response Status: 201 Created in 6 milliseconds
I1207 08:34:55.649488   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:55.823380   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c3}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:34:55.823501   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:55.823526   13809 round_trippers.go:421] Request Headers:
I1207 08:34:55.823540   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:55.823553   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:55.823571   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:55.829421   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:55.949243   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d"
I1207 08:34:55.949956   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c3}"}): type: 'Normal' reason: 'Started' Started container
I1207 08:34:55.950046   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:55.950060   13809 round_trippers.go:421] Request Headers:
I1207 08:34:55.950066   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:55.950070   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:55.950075   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:55.951743   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d" (aliases: [k8s_c3_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_1 01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d], namespace: "docker")
I1207 08:34:55.951915   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d 2021-12-07 08:34:55.8441928 +0000 UTC containerCreation {<nil>}}
I1207 08:34:55.951966   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d"
I1207 08:34:55.955035   13809 round_trippers.go:439] Response Status: 201 Created in 4 milliseconds
I1207 08:34:56.653912   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:56.659413   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d: non-existent -> running
I1207 08:34:56.660311   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.676222   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc421a4a540), (*container.ContainerStatus)(0xc421428fc0), (*container.ContainerStatus)(0xc421a4a620), (*container.ContainerStatus)(0xc4214290a0), (*container.ContainerStatus)(0xc421a4a700), (*container.ContainerStatus)(0xc421dca8c0), (*container.ContainerStatus)(0xc421429180)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc42214fd10)}} (err: <nil>)
I1207 08:34:56.676298   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d"}
I1207 08:34:56.676317   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.676576   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x02', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} {nil nil nil} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:34:56.676668   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (2, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} {nil nil nil} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:34:56.676872   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:34:56.676878   13809 round_trippers.go:421] Request Headers:
I1207 08:34:56.676882   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:56.676886   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:56.677004   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.680120   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:56.680470   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:34:56.680486   13809 round_trippers.go:421] Request Headers:
I1207 08:34:56.680493   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:56.680499   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:56.680506   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:56.683832   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:56.684073   13809 config.go:282] Setting pods for source api
I1207 08:34:56.684038   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (2, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:6 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:34:56.685094   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.738929   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:34:56.739085   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:56.739332   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:34:56.739366   13809 round_trippers.go:421] Request Headers:
I1207 08:34:56.739377   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:56.739386   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:56.743029   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:56.743169   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:34:56.743386   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:34:56.743412   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:34:56.743425   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:34:56.743488   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:56.743539   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:56.977437   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.977503   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1 c2 c3]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} {nil nil nil} false 5 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {c2 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {c3 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:34:56.978334   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:34:56.978730   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:34:56.978841   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:56.978968   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.979128   13809 kuberuntime_manager.go:749] Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:56.979163   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:34:56.979327   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:56.979347   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:34:56.979420   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events
I1207 08:34:56.979441   13809 round_trippers.go:421] Request Headers:
I1207 08:34:56.979454   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:56.979467   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:56.979480   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:56.979475   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:56.984769   13809 round_trippers.go:439] Response Status: 201 Created in 5 milliseconds
I1207 08:34:56.985099   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c02f43e049f
I1207 08:34:56.985118   13809 round_trippers.go:421] Request Headers:
I1207 08:34:56.985123   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:56.985127   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:56.985134   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:56.990885   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:34:57.349521   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:57.349591   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:57.676590   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:57.683069   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.683336   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x03', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:34:57.683416   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (3, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:34:57.683575   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:34:57.683588   13809 round_trippers.go:421] Request Headers:
I1207 08:34:57.683595   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:57.683602   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:57.683656   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.687162   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:57.687455   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:34:57.687472   13809 round_trippers.go:421] Request Headers:
I1207 08:34:57.687477   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:57.687481   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:57.687486   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:34:57.690789   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:34:57.690988   13809 config.go:282] Setting pods for source api
I1207 08:34:57.691016   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (3, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} Ready:false RestartCount:6 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:34:57.692080   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.743356   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:34:57.743446   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:57.743696   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:34:57.743768   13809 round_trippers.go:421] Request Headers:
I1207 08:34:57.743803   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:57.743851   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:57.746503   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:34:57.746612   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:34:57.746811   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:34:57.746881   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:34:57.746900   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:34:57.746969   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:34:57.747063   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:34:57.984005   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.984053   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1 c2 c3]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:31:37 +0000 UTC,FinishedAt:2021-12-07 08:31:37 +0000 UTC,ContainerID:docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47,}} {nil nil nil} false 5 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d07d23baffe2384c926d8ed86fb82231050b767cc81db37696d46ce35a24af47} {c2 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479} {c3 {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} {nil nil nil} false 0 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:34:57.984880   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:34:57.985290   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:34:57.985433   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:34:57.985574   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.985789   13809 kuberuntime_manager.go:749] Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:34:57.985816   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:34:57.985936   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:34:57.985991   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:34:57.986024   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:34:57.986553   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c0838deaef7
I1207 08:34:57.986593   13809 round_trippers.go:421] Request Headers:
I1207 08:34:57.986607   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:57.986620   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:57.986631   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:57.992550   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:34:57.992861   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c02f43e049f
I1207 08:34:57.992876   13809 round_trippers.go:421] Request Headers:
I1207 08:34:57.992882   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:57.992890   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:57.992896   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:57.999940   13809 round_trippers.go:439] Response Status: 200 OK in 7 milliseconds
I1207 08:34:58.683354   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:59.349458   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:34:59.349550   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:34:59.349587   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:34:59.349783   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:34:59.349830   13809 round_trippers.go:421] Request Headers:
I1207 08:34:59.349862   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:59.349893   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:59.352809   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:34:59.352974   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:34:59.535739   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:34:59.535770   13809 round_trippers.go:421] Request Headers:
I1207 08:34:59.535780   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:59.535788   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:59.538260   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:34:59.546786   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:34:59.547120   13809 round_trippers.go:421] Request Headers:
I1207 08:34:59.547131   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:34:59.547140   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:34:59.547149   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:34:59.551459   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:34:59.688774   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:34:59.689289   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:34:59.733323   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:34:59.756574   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618493, capacity: 6250Ki, time: 2021-12-07 08:34:53.409588633 +0000 UTC
I1207 08:34:59.756604   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:34:59.756610   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15213264Ki, capacity: 16426204Ki, time: 2021-12-07 08:34:53.409588633 +0000 UTC
I1207 08:34:59.756616   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33476332Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:53.409588633 +0000 UTC
I1207 08:34:59.756622   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618493, capacity: 6250Ki, time: 2021-12-07 08:34:53.409588633 +0000 UTC
I1207 08:34:59.756628   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33476332Ki, capacity: 50758760Ki, time: 2021-12-07 08:34:53.409588633 +0000 UTC
W1207 08:34:59.756636   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:34:59.756646   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:00.694160   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:01.349514   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:01.349566   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:01.700712   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:02.705626   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:03.349523   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:03.349590   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:03.712685   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:04.690645   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:04.718421   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:05.206716   13809 server.go:779] POST /stats/container/: (57.853606ms) 200 [[Go-http-client/1.1] 10.244.164.213:32942]
I1207 08:35:05.337840   13809 server.go:779] POST /stats/container/: (17.430256ms) 200 [[Go-http-client/1.1] 10.244.29.185:56956]
I1207 08:35:05.349464   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:05.349500   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:05.722825   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:06.278180   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:35:06.278273   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:35:06.278520   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:35:06.278585   13809 round_trippers.go:421] Request Headers:
I1207 08:35:06.278618   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:06.278653   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:06.283321   13809 round_trippers.go:439] Response Status: 403 Forbidden in 4 milliseconds
E1207 08:35:06.283465   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:35:06.283658   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:35:38.283571184 +0000 UTC (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:35:06.283785   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:35:06.284246   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:35:06.284278   13809 round_trippers.go:421] Request Headers:
I1207 08:35:06.284291   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:06.284306   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:06.284319   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:06.287996   13809 round_trippers.go:439] Response Status: 403 Forbidden in 3 milliseconds
E1207 08:35:06.288081   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462906, nsec:283537282, loc:(*time.Location)(0x9e22280)}}, Count:7, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:35:06.728641   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:07.349523   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:07.349589   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:07.733759   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:08.738848   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:09.349526   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:09.349562   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:09.349593   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:09.349679   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:09.349703   13809 round_trippers.go:421] Request Headers:
I1207 08:35:09.349717   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:09.349730   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:09.351855   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:35:09.351983   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:09.552567   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:09.552601   13809 round_trippers.go:421] Request Headers:
I1207 08:35:09.552611   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:09.552621   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:09.554534   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:35:09.561636   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:09.561657   13809 round_trippers.go:421] Request Headers:
I1207 08:35:09.561664   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:09.561669   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:09.561673   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:09.571636   13809 round_trippers.go:439] Response Status: 200 OK in 9 milliseconds
I1207 08:35:09.692084   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:09.743910   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:09.756945   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:09.764972   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618455, capacity: 6250Ki, time: 2021-12-07 08:35:03.484170036 +0000 UTC
I1207 08:35:09.765000   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33476052Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:03.484170036 +0000 UTC
I1207 08:35:09.765007   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618455, capacity: 6250Ki, time: 2021-12-07 08:35:03.484170036 +0000 UTC
I1207 08:35:09.765013   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:35:09.765018   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15210540Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:03.484170036 +0000 UTC
I1207 08:35:09.765028   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33476052Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:03.484170036 +0000 UTC
W1207 08:35:09.765041   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:09.765059   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:10.748852   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:11.349561   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:11.349630   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:11.755362   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:12.349549   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:12.349696   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:12.350174   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} Ready:false RestartCount:6 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:35:12.350809   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:12.401902   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:12.401961   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:12.402071   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:12.402187   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:12.402210   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:12.402217   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:12.402259   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:12.402293   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:12.651217   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:12.651274   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:12.652057   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:12.652496   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:12.652659   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:12.652803   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:12.652960   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:12.654979   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hubrepo:5736"
I1207 08:35:12.654998   13809 kuberuntime_image.go:46] Pulling image "runjivu/hubrepo:5736" without credentials
I1207 08:35:12.655248   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c060d0c8b09
I1207 08:35:12.655274   13809 round_trippers.go:421] Request Headers:
I1207 08:35:12.655282   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:12.655289   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:12.655303   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:12.661526   13809 round_trippers.go:439] Response Status: 200 OK in 6 milliseconds
I1207 08:35:12.760805   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:13.349556   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:13.349631   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:13.766320   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:14.693837   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:14.772761   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:15.000203   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hubrepo:5736": "Status: Image is up to date for runjivu/hubrepo:5736"
I1207 08:35:15.001619   13809 kuberuntime_container.go:100] Generating ref for container c1: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}
I1207 08:35:15.001668   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:35:15.001721   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:15.001727   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hubrepo:5736"
I1207 08:35:15.001757   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:35:15.001782   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c1" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:35:15.001994   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c069681a017
I1207 08:35:15.002004   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.002009   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.002014   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.002021   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:15.003902   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:35:15.008201   13809 round_trippers.go:439] Response Status: 200 OK in 6 milliseconds
I1207 08:35:15.272628   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:35:15.273044   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c06a5ec4a1a
I1207 08:35:15.273083   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.273097   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.273111   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:15.273131   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.287536   13809 round_trippers.go:439] Response Status: 200 OK in 14 milliseconds
I1207 08:35:15.349492   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:15.349540   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:15.410258   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b"
I1207 08:35:15.411754   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Started' Started container
I1207 08:35:15.412011   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c06adee9a5b
I1207 08:35:15.412026   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.412033   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.412040   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:15.412048   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.413940   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_7 65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b], namespace: "docker")
I1207 08:35:15.414279   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b 2021-12-07 08:35:15.293371439 +0000 UTC containerCreation {<nil>}}
I1207 08:35:15.414348   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b"
I1207 08:35:15.417209   13809 helpers.go:107] Unable to get network stats from pid 14175: couldn't read network stats: failure opening /proc/14175/net/dev: open /proc/14175/net/dev: no such file or directory
I1207 08:35:15.417678   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:35:15.424104   13809 manager.go:989] Destroyed container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_7 65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b], namespace: "docker")
I1207 08:35:15.424162   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b 2021-12-07 08:35:15.424150063 +0000 UTC containerDeletion {<nil>}}
I1207 08:35:15.778967   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:15.784131   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b: non-existent -> exited
I1207 08:35:15.785002   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:15.802875   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4212e28c0), (*container.ContainerStatus)(0xc421428460), (*container.ContainerStatus)(0xc4212e29a0), (*container.ContainerStatus)(0xc42083c2a0), (*container.ContainerStatus)(0xc4212e2b60), (*container.ContainerStatus)(0xc42083c380), (*container.ContainerStatus)(0xc4212e2e00), (*container.ContainerStatus)(0xc42083c460)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4208956d0)}} (err: <nil>)
I1207 08:35:15.802943   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b"}
I1207 08:35:15.802987   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:15.803105   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:15.803126   13809 kuberuntime_container.go:807] Removing container "e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830"
I1207 08:35:15.803335   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x04', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:35:15.803400   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (4, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:35:15.803602   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:35:15.803620   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.803628   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.803635   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.803707   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:15.807278   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:15.807619   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:35:15.807637   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.807642   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.807647   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:35:15.807651   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.811011   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:15.811263   13809 config.go:282] Setting pods for source api
I1207 08:35:15.811247   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (4, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} Ready:false RestartCount:7 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:35:15.812455   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:15.814060   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:15.814096   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:15.814237   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:35:15.814249   13809 round_trippers.go:421] Request Headers:
I1207 08:35:15.814256   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:15.814263   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:15.817281   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:15.817404   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:15.817537   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:15.817559   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:15.817566   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:15.817605   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:15.817637   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:16.104083   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:16.104130   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 10s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 6 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:16.104997   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:16.105526   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:16.105640   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:16.105772   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:16.105986   13809 kuberuntime_manager.go:749] Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:16.106009   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:35:16.106169   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:16.106271   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:35:16.106315   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:35:16.106633   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c0838deaef7
I1207 08:35:16.106672   13809 round_trippers.go:421] Request Headers:
I1207 08:35:16.106686   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:16.106699   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:16.106714   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:16.112743   13809 round_trippers.go:439] Response Status: 200 OK in 6 milliseconds
I1207 08:35:16.113121   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/events/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod.16be6c02f43e049f
I1207 08:35:16.113138   13809 round_trippers.go:421] Request Headers:
I1207 08:35:16.113143   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:16.113148   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:16.113152   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:16.120152   13809 round_trippers.go:439] Response Status: 200 OK in 6 milliseconds
I1207 08:35:16.803251   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:16.809256   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830: exited -> non-existent
I1207 08:35:16.810214   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:16.825910   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc420a62540), (*container.ContainerStatus)(0xc42083d960), (*container.ContainerStatus)(0xc4212e3b20), (*container.ContainerStatus)(0xc420a63180), (*container.ContainerStatus)(0xc4212e3c00), (*container.ContainerStatus)(0xc42083da40), (*container.ContainerStatus)(0xc4212e3ce0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc421001590)}} (err: <nil>)
I1207 08:35:17.349567   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:17.349662   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:17.826230   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:18.833420   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:19.349567   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:35:19.349632   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:19.349641   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:19.349796   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:19.349826   13809 round_trippers.go:421] Request Headers:
I1207 08:35:19.349841   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:19.349854   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:19.351916   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:35:19.352066   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:19.572302   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:19.572339   13809 round_trippers.go:421] Request Headers:
I1207 08:35:19.572354   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:19.572363   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:19.574466   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:19.583018   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:19.583033   13809 round_trippers.go:421] Request Headers:
I1207 08:35:19.583040   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:19.583047   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:19.583054   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:19.586417   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:19.695442   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:19.765339   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:19.788251   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:19.788283   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475900Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:19.788290   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:19.788296   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:35:19.788301   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15206248Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:19.788306   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475900Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
W1207 08:35:19.788314   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:19.788325   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:19.840254   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:20.846125   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:21.349550   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:21.349622   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:21.853206   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:22.860188   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:23.349524   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:23.349586   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:23.866232   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:24.697263   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:24.870943   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:25.349542   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:25.349616   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:25.877886   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:26.884564   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:27.349582   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:27.349653   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:27.349693   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:27.349761   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.350158   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x05', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:35:27.350337   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (5, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:35:27.350655   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:35:27.350681   13809 round_trippers.go:421] Request Headers:
I1207 08:35:27.350695   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:27.350708   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:27.350799   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.354152   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:27.354929   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:35:27.354969   13809 round_trippers.go:421] Request Headers:
I1207 08:35:27.354984   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:27.354998   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:35:27.355011   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:27.359687   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:35:27.359893   13809 config.go:282] Setting pods for source api
I1207 08:35:27.359871   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (5, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} Ready:false RestartCount:7 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:35:27.360150   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:27.360196   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:27.360306   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:27.360398   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:27.360411   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:27.360418   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:27.360461   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:27.360490   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:27.363236   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.651117   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.651174   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:34:50 +0000 UTC,FinishedAt:2021-12-07 08:34:50 +0000 UTC,ContainerID:docker://e4e394c6e52c3210bbe71982cd9f5f4a35c811fd78acf43f4ef96e0820c39830,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:27.652070   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:27.652434   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:27.652563   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:27.652687   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.652907   13809 kuberuntime_manager.go:749] Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:27.652977   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:35:27.653125   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:27.653221   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:35:27.653281   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:35:27.891309   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:28.898172   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:29.343549   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:35:29.349515   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:29.349597   13809 qos_container_manager_linux.go:320] [ContainerManager]: Updated QoS cgroup configuration
I1207 08:35:29.351202   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:29.351284   13809 round_trippers.go:421] Request Headers:
I1207 08:35:29.351294   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:29.351304   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:29.355113   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:29.355157   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:29.357270   13809 kubelet.go:1222] Container garbage collection succeeded
I1207 08:35:29.357505   13809 round_trippers.go:439] Response Status: 403 Forbidden in 6 milliseconds
W1207 08:35:29.357615   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:29.429428   13809 iptables.go:396] running iptables -N [KUBE-MARK-DROP -t nat]
I1207 08:35:29.432792   13809 iptables.go:396] running iptables -C [KUBE-MARK-DROP -t nat -j MARK --set-xmark 0x00008000/0x00008000]
I1207 08:35:29.435904   13809 iptables.go:396] running iptables -N [KUBE-FIREWALL -t filter]
I1207 08:35:29.438763   13809 iptables.go:396] running iptables -C [KUBE-FIREWALL -t filter -m comment --comment kubernetes firewall for dropping marked packets -m mark --mark 0x00008000/0x00008000 -j DROP]
I1207 08:35:29.441705   13809 iptables.go:396] running iptables -C [OUTPUT -t filter -j KUBE-FIREWALL]
I1207 08:35:29.444190   13809 iptables.go:396] running iptables -C [INPUT -t filter -j KUBE-FIREWALL]
I1207 08:35:29.446422   13809 iptables.go:396] running iptables -N [KUBE-MARK-MASQ -t nat]
I1207 08:35:29.448490   13809 iptables.go:396] running iptables -N [KUBE-POSTROUTING -t nat]
I1207 08:35:29.450803   13809 iptables.go:396] running iptables -C [KUBE-MARK-MASQ -t nat -j MARK --set-xmark 0x00004000/0x00004000]
I1207 08:35:29.452975   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]
I1207 08:35:29.455248   13809 iptables.go:396] running iptables -C [KUBE-POSTROUTING -t nat -m comment --comment kubernetes service traffic requiring SNAT -m mark --mark 0x00004000/0x00004000 -j MASQUERADE]
I1207 08:35:29.587848   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:29.588019   13809 round_trippers.go:421] Request Headers:
I1207 08:35:29.588061   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:29.588090   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:29.590260   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:29.597519   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:29.597544   13809 round_trippers.go:421] Request Headers:
I1207 08:35:29.597552   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:29.597558   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:29.597565   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:29.601660   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:35:29.700939   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:29.731238   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:35:29.731262   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:35:29.731271   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:35:29.731276   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:35:29.731282   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:35:29.731289   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:35:29.731294   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:35:29.731307   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:35:29.731325   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:35:29.731337   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:35:29.731353   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:35:29.731369   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:35:29.731380   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:35:29.731401   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:35:29.731415   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:35:29.731451   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:35:29.731466   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:35:29.731477   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:35:29.731498   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:35:29.731511   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:35:29.731524   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:35:29.731529   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:35:29.731535   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:35:29.731541   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:35:29.731546   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:35:29.731556   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:35:29.731589   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:35:29.731599   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:35:29.731621   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:35:29.731640   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:35:29.731656   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:35:29.731670   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:35:29.731680   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:35:29.731686   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:35:29.731695   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:35:29.731704   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:35:29.731730   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:35:29.731753   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:35:29.731776   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:35:29.731788   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:35:29.731801   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:35:29.731808   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:35:29.731812   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:35:29.731824   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:35:29.731832   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:35:29.731838   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:35:29.731868   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:35:29.731896   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:35:29.731923   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:35:29.731952   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:35:29.731972   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:35:29.731981   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:35:29.731989   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:35:29.731997   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:35:29.732018   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:35:29.732030   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:35:29.732039   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:35:29.732046   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:35:29.732064   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:35:29.732081   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:35:29.732095   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:35:29.732102   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:35:29.732111   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:35:29.732123   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:35:29.732131   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:35:29.732139   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:35:29.788583   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:29.810352   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475900Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:29.810385   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:29.810396   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:35:29.810404   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15206248Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:29.810413   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475900Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
I1207 08:35:29.810422   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:17.573837713 +0000 UTC
W1207 08:35:29.810439   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:29.810459   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:29.904039   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:30.909996   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:31.349567   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:31.349638   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:31.917107   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:32.924196   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:33.349554   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:33.349636   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:33.929604   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:34.703002   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:34.934911   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:35.349560   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:35.349639   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:35.940117   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:36.349565   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)
I1207 08:35:36.349710   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:35:36.350025   13809 status_manager.go:325] Ignoring same status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable}
I1207 08:35:36.350461   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:35:36.397113   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:36.397172   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:36.397223   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:35:36.397281   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:35:36.397463   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/mdsd-config?resourceVersion=0
I1207 08:35:36.397484   13809 round_trippers.go:421] Request Headers:
I1207 08:35:36.397493   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:36.397506   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:36.397533   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/default-token-74mlg?resourceVersion=0
I1207 08:35:36.397545   13809 round_trippers.go:421] Request Headers:
I1207 08:35:36.397554   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:36.397563   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:36.399846   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:36.399886   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:36.399993   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:35:36.400125   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:35:36.400152   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:35:36.400185   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:35:36.400195   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:35:36.400268   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:35:36.400282   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:35:36.400289   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:35:36.400294   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:35:36.400319   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:35:36.400375   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:35:36.400407   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:35:36.650905   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:35:36.650950   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[cpu:{{200 -3} {<nil>} 200m DecimalSI} memory:{{1 9} {<nil>} 1G DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:36.651912   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:35:36.945607   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:37.349558   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:37.349630   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:37.951495   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:38.304711   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:35:38.304786   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:35:38.305045   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:35:38.305078   13809 round_trippers.go:421] Request Headers:
I1207 08:35:38.305092   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:38.305103   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:38.307409   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:35:38.307519   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:35:38.307618   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:36:42.307583938 +0000 UTC (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:35:38.307663   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:35:38.308059   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:35:38.308081   13809 round_trippers.go:421] Request Headers:
I1207 08:35:38.308088   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:38.308097   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:38.308106   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:38.309675   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:35:38.309776   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462938, nsec:307568237, loc:(*time.Location)(0x9e22280)}}, Count:8, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:35:38.958725   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:39.349573   13809 kubelet.go:1890] SyncLoop (SYNC): 2 pods; caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde), wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:39.349669   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:39.349755   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:39.349815   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.349885   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.349685   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:39.350091   13809 status_manager.go:325] Ignoring same status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort}
I1207 08:35:39.350391   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:39.350424   13809 round_trippers.go:421] Request Headers:
I1207 08:35:39.350438   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:39.350452   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:39.350523   13809 kuberuntime_manager.go:428] Syncing Pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,GenerateName:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,UID:7df95486-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484049,Generation:0,CreationTimestamp:2021-12-07 08:27:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{app: caas,deploymenttype: infra,pod-template-hash: 2462294644,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},Annotations:map[string]string{kubernetes.io/config.seen: 2021-12-07T08:34:29.329958181Z,kubernetes.io/config.source: api,kubernetes.io/created-by: {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"caas-20d222f17dd4489fb180b51031b406f6","name":"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88","uid":"7df8c8a0-5737-11ec-9d1e-002248057bde","apiVersion":"extensions","resourceVersion":"1094476293"}}
,localRegistry-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[{extensions/v1beta1 ReplicaSet caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88 7df8c8a0-5737-11ec-9d1e-002248057bde 0xc420ae61e0 0xc420ae61e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{infra mcr.microsoft.com/aci/infra:master_20211112.1 [/bin/sh -c while true; do echo `date`; sleep 1000000; done] []  [] [] [] {map[] map[]} [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[{[{agentpool NotIn [system agentpool1]} {node-optimization-needed DoesNotExist []} {kubernetes.io/hostname In [k8s-agentpool4-a14krc0l-17]}]}],},PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAffinity:nil,PodAntiAffinity:&PodAntiAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[{app Exists []}],} [caas-827aef2dda4640159dd2550e4e471cf3 caas-8dca1f0909804431b986fc0cca798ca3 caas-b3d11bb4bae449faa2674fa20dee69cb caas-b516ef0c44d44dba913d83b46a14454c certbootstrap default kube-public kube-system] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.253,StartTime:2021-12-07 08:27:19 +0000 UTC,ContainerStatuses:[{infra {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}],QOSClass:BestEffort,InitContainerStatuses:[],},}
I1207 08:35:39.351083   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.351257   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} Ready:false RestartCount:7 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:35:39.352031   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.352530   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:35:39.352698   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:39.409624   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:39.409715   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:39.409951   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:35:39.409979   13809 round_trippers.go:421] Request Headers:
I1207 08:35:39.409992   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:39.410006   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:39.413169   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:39.413323   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:39.413521   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:39.413568   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:39.413596   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:39.413669   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:39.413715   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:39.602238   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:39.602265   13809 round_trippers.go:421] Request Headers:
I1207 08:35:39.602276   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:39.602286   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:39.604327   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:39.612108   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:39.612131   13809 round_trippers.go:421] Request Headers:
I1207 08:35:39.612139   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:39.612145   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:39.612151   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:39.616161   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:35:39.652451   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.652512   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:39.653318   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:39.653797   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:39.653913   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:39.654030   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:39.654154   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:39.655941   13809 kuberuntime_image.go:46] Pulling image "runjivu/hubrepo:5736" without credentials
I1207 08:35:39.655931   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hubrepo:5736"
I1207 08:35:39.704518   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:39.810769   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:39.838273   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:30.812365034 +0000 UTC
I1207 08:35:39.838305   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:35:39.838312   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15206108Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:30.812365034 +0000 UTC
I1207 08:35:39.838318   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475788Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:30.812365034 +0000 UTC
I1207 08:35:39.838324   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618454, capacity: 6250Ki, time: 2021-12-07 08:35:30.812365034 +0000 UTC
I1207 08:35:39.838330   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475788Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:30.812365034 +0000 UTC
W1207 08:35:39.838338   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:39.838352   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:39.965601   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:40.971198   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:41.349520   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:41.349583   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:41.976499   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:42.191276   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hubrepo:5736": "Status: Image is up to date for runjivu/hubrepo:5736"
I1207 08:35:42.193220   13809 kuberuntime_container.go:100] Generating ref for container c1: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}
I1207 08:35:42.193291   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:35:42.193376   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:42.193515   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:35:42.193562   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c1" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:35:42.193351   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hubrepo:5736"
I1207 08:35:42.195661   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:35:42.508162   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:35:42.641354   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c"
I1207 08:35:42.642403   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Started' Started container
W1207 08:35:42.643956   13809 container.go:354] Failed to create summary reader for "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c": none of the resources are being tracked.
I1207 08:35:42.643999   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_8 c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c], namespace: "docker")
I1207 08:35:42.644089   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c 0001-01-01 00:00:00 +0000 UTC containerCreation {<nil>}}
I1207 08:35:42.644125   13809 manager.go:989] Destroyed container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_8 c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c], namespace: "docker")
I1207 08:35:42.644141   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c 2021-12-07 08:35:42.644137888 +0000 UTC containerDeletion {<nil>}}
I1207 08:35:42.644170   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c"
I1207 08:35:42.982143   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:42.988748   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c: non-existent -> exited
I1207 08:35:42.989693   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.008651   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc421dca8c0), (*container.ContainerStatus)(0xc4208eeee0), (*container.ContainerStatus)(0xc421dcaa80), (*container.ContainerStatus)(0xc4208eefc0), (*container.ContainerStatus)(0xc422334000), (*container.ContainerStatus)(0xc421dcab60), (*container.ContainerStatus)(0xc4223340e0), (*container.ContainerStatus)(0xc4208ef180)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc421e18410)}} (err: <nil>)
I1207 08:35:43.008728   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c"}
I1207 08:35:43.008778   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.008898   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.008936   13809 kuberuntime_container.go:807] Removing container "65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b"
I1207 08:35:43.009194   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\x06', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:35:43.009258   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (6, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:35:43.009525   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:35:43.009538   13809 round_trippers.go:421] Request Headers:
I1207 08:35:43.009545   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:43.009544   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.009554   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:43.012789   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:43.013102   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:35:43.013117   13809 round_trippers.go:421] Request Headers:
I1207 08:35:43.013122   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:43.013129   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:43.013136   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:35:43.016178   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:43.016520   13809 config.go:282] Setting pods for source api
I1207 08:35:43.016498   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (6, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} Ready:false RestartCount:8 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:35:43.017590   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.022802   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:43.022858   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:43.023034   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:35:43.023052   13809 round_trippers.go:421] Request Headers:
I1207 08:35:43.023060   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:43.023068   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:43.025540   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:43.025633   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:43.025763   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:43.025778   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:43.025783   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:43.025813   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:43.025837   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:43.309949   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.310000   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 7 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:43.310863   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:43.311290   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:43.311422   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:43.311548   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.311780   13809 kuberuntime_manager.go:749] Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:43.311799   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:35:43.311925   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:43.312022   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:35:43.312099   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:35:43.349531   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:43.349605   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:44.008962   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:44.014663   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b: exited -> non-existent
I1207 08:35:44.015588   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:44.032148   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4225fcd20), (*container.ContainerStatus)(0xc422334c40), (*container.ContainerStatus)(0xc4225fce00), (*container.ContainerStatus)(0xc422334d20), (*container.ContainerStatus)(0xc4225fcee0), (*container.ContainerStatus)(0xc421dcbc00), (*container.ContainerStatus)(0xc4225fcfc0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4221c84b0)}} (err: <nil>)
I1207 08:35:44.706376   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:45.032530   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:45.349549   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:45.349622   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:46.038912   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:47.045111   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:47.349531   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:47.349592   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:48.050166   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:49.057331   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:49.349548   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:35:49.349611   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:49.349639   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:49.349783   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:49.349851   13809 round_trippers.go:421] Request Headers:
I1207 08:35:49.349885   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:49.349913   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:49.351897   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
W1207 08:35:49.352082   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:49.616819   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:49.616855   13809 round_trippers.go:421] Request Headers:
I1207 08:35:49.616869   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:49.616886   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:49.619037   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:49.627458   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:49.627481   13809 round_trippers.go:421] Request Headers:
I1207 08:35:49.627493   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:49.627505   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:49.627517   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:49.631682   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:35:49.708049   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:49.838662   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:49.858167   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15204464Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:49.858200   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475600Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:49.858212   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:49.858221   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475600Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:49.858230   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:49.858239   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
W1207 08:35:49.858250   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:49.858268   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:35:50.062192   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:51.067781   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:51.349577   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:51.349657   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:52.073072   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:53.078535   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:53.097581   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc4213c2fc0), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger"}
I1207 08:35:53.104315   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c988aa2-5737-11ec-9d1e-002248057bde/logger_0.log"
I1207 08:35:53.104350   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger?limitBytes=67104768&tailLines=100&timestamps=True: (6.829114ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:35:53.349562   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:53.349640   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:54.084237   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:54.710005   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:55.089931   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:55.349545   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:55.349613   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:56.095046   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:56.349593   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:56.349723   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.350189   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\a', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:35:56.350302   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (7, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:35:56.351969   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.351939   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:35:56.352073   13809 round_trippers.go:421] Request Headers:
I1207 08:35:56.352082   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:56.352090   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:56.356898   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:35:56.357281   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:35:56.357297   13809 round_trippers.go:421] Request Headers:
I1207 08:35:56.357303   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:35:56.357309   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:56.357314   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:56.362457   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:35:56.362772   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (7, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} Ready:false RestartCount:8 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:35:56.362963   13809 config.go:282] Setting pods for source api
I1207 08:35:56.364009   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.378386   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:35:56.378429   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:56.378581   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:35:56.378595   13809 round_trippers.go:421] Request Headers:
I1207 08:35:56.378602   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:56.378609   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:56.382499   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:56.382597   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:35:56.382778   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:35:56.382804   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:35:56.382812   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:35:56.382854   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:35:56.382884   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:35:56.652578   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.652639   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:15 +0000 UTC,FinishedAt:2021-12-07 08:35:15 +0000 UTC,ContainerID:docker://65f46ef494f25527e9e756c18dc58d2675ed786f7bd80419546ab6938a30127b,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:35:56.653554   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:35:56.654033   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:35:56.654151   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:35:56.654306   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.654533   13809 kuberuntime_manager.go:749] Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:35:56.654567   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:35:56.654738   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:35:56.654802   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:35:56.654902   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:35:57.100732   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:57.349534   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:57.349601   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:58.106645   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:59.112099   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:35:59.349564   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:35:59.349611   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:35:59.349626   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:35:59.349749   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:35:59.349776   13809 round_trippers.go:421] Request Headers:
I1207 08:35:59.349790   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:59.349802   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:59.352024   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:35:59.352227   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:35:59.632394   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:35:59.632435   13809 round_trippers.go:421] Request Headers:
I1207 08:35:59.632450   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:59.632459   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:59.634997   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:35:59.643989   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:35:59.644004   13809 round_trippers.go:421] Request Headers:
I1207 08:35:59.644012   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:35:59.644018   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:35:59.644025   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:35:59.647954   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:35:59.711691   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:35:59.858483   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:35:59.879157   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475600Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:59.879188   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:59.879195   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475600Ki, capacity: 50758760Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:59.879200   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
I1207 08:35:59.879205   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:35:59.879210   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15204464Ki, capacity: 16426204Ki, time: 2021-12-07 08:35:49.040322852 +0000 UTC
W1207 08:35:59.879217   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:35:59.879229   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:00.117245   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:01.122536   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:01.349542   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:01.349607   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:02.127774   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:03.135279   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:03.349553   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:03.349625   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:04.140720   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:04.713535   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:05.146292   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:05.349533   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:05.349602   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:05.537576   13809 server.go:779] POST /stats/container/: (50.390851ms) 200 [[Go-http-client/1.1] 10.244.164.213:32942]
I1207 08:36:05.696407   13809 server.go:779] POST /stats/container/: (14.162157ms) 200 [[Go-http-client/1.1] 10.244.29.185:56956]
I1207 08:36:06.153296   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:07.158577   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:07.349528   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:07.349597   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:08.163858   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:09.169591   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:09.349529   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:09.349596   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:09.349619   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:09.349761   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:09.349818   13809 round_trippers.go:421] Request Headers:
I1207 08:36:09.349856   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:09.349886   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:09.352566   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:36:09.352742   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:09.648791   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:09.648834   13809 round_trippers.go:421] Request Headers:
I1207 08:36:09.648866   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:09.648875   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:09.651115   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:09.657452   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:09.657484   13809 round_trippers.go:421] Request Headers:
I1207 08:36:09.657493   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:09.657500   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:09.657517   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:09.661758   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:36:09.715033   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:09.879548   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:09.890068   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475508Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:09.890101   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:09.890109   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475508Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:09.890115   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:09.890121   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:09.890127   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15202396Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
W1207 08:36:09.890140   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:09.890153   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:10.176334   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:10.349563   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:10.349672   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:10.350142   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} Ready:false RestartCount:8 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:36:10.350708   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:10.435893   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:10.435961   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:10.436076   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:10.436212   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:10.436231   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:10.436239   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:10.436285   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:10.436317   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:10.651123   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:10.651189   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:10.651946   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:10.652371   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:10.652465   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:10.652544   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:10.652693   13809 kuberuntime_manager.go:749] Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:10.652710   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:36:10.652820   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:10.652881   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:36:10.652918   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:11.182944   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:11.349565   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:11.349641   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:12.189573   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:13.195688   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:13.349522   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:13.349580   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:14.200737   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:14.716886   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:15.207566   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:15.349558   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:15.349634   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:16.212843   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:17.218179   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:17.349527   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:17.349584   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:18.224668   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:19.230544   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:19.349530   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:19.349601   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:19.349618   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:19.349759   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:19.349791   13809 round_trippers.go:421] Request Headers:
I1207 08:36:19.349804   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:19.349827   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:19.352540   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:36:19.352693   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:19.662527   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:19.662575   13809 round_trippers.go:421] Request Headers:
I1207 08:36:19.662586   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:19.662596   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:19.665216   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:19.672836   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:19.672852   13809 round_trippers.go:421] Request Headers:
I1207 08:36:19.672858   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:19.672865   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:19.672871   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:19.676720   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:19.718571   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:19.890437   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:19.910027   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475508Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:19.910068   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:19.910078   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475508Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:19.910087   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
I1207 08:36:19.910095   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:19.910103   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15202396Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:06.717296602 +0000 UTC
W1207 08:36:19.910119   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:19.910134   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:20.236147   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:21.241284   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:21.349500   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:21.349594   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:21.349612   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:21.349662   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:21.350239   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} Ready:false RestartCount:8 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:36:21.350931   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:21.378817   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:21.378884   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:21.379069   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:36:21.379093   13809 round_trippers.go:421] Request Headers:
I1207 08:36:21.379103   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:21.379112   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:21.381266   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:21.381416   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:21.381551   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:21.381579   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:21.381588   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:21.381643   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:21.381685   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:21.651388   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:21.651457   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:21.652179   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:21.652638   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:21.652756   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:21.652882   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:21.653078   13809 kuberuntime_manager.go:749] Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:21.653098   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:36:21.653226   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:21.653294   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:36:21.653325   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:22.246396   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:23.250985   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:23.349522   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:23.349575   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:24.257748   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:24.720424   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:25.264415   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:25.349536   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:36:25.349603   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:26.271345   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:27.277850   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:27.349525   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:27.349572   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:28.283993   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:29.289954   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:29.349446   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:29.349505   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:29.349566   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:29.349702   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:29.349722   13809 round_trippers.go:421] Request Headers:
I1207 08:36:29.349735   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:29.349746   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:29.350342   13809 qos_container_manager_linux.go:320] [ContainerManager]: Updated QoS cgroup configuration
I1207 08:36:29.352255   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:36:29.352413   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:29.357430   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:29.362925   13809 kubelet.go:1222] Container garbage collection succeeded
I1207 08:36:29.457769   13809 iptables.go:396] running iptables -N [KUBE-MARK-DROP -t nat]
I1207 08:36:29.463102   13809 iptables.go:396] running iptables -C [KUBE-MARK-DROP -t nat -j MARK --set-xmark 0x00008000/0x00008000]
I1207 08:36:29.466650   13809 iptables.go:396] running iptables -N [KUBE-FIREWALL -t filter]
I1207 08:36:29.469110   13809 iptables.go:396] running iptables -C [KUBE-FIREWALL -t filter -m comment --comment kubernetes firewall for dropping marked packets -m mark --mark 0x00008000/0x00008000 -j DROP]
I1207 08:36:29.471526   13809 iptables.go:396] running iptables -C [OUTPUT -t filter -j KUBE-FIREWALL]
I1207 08:36:29.473995   13809 iptables.go:396] running iptables -C [INPUT -t filter -j KUBE-FIREWALL]
I1207 08:36:29.476231   13809 iptables.go:396] running iptables -N [KUBE-MARK-MASQ -t nat]
I1207 08:36:29.478500   13809 iptables.go:396] running iptables -N [KUBE-POSTROUTING -t nat]
I1207 08:36:29.480515   13809 iptables.go:396] running iptables -C [KUBE-MARK-MASQ -t nat -j MARK --set-xmark 0x00004000/0x00004000]
I1207 08:36:29.482681   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]
I1207 08:36:29.484869   13809 iptables.go:396] running iptables -C [KUBE-POSTROUTING -t nat -m comment --comment kubernetes service traffic requiring SNAT -m mark --mark 0x00004000/0x00004000 -j MASQUERADE]
I1207 08:36:29.677508   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:29.677531   13809 round_trippers.go:421] Request Headers:
I1207 08:36:29.677537   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:29.677542   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:29.679653   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:29.686988   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:29.687012   13809 round_trippers.go:421] Request Headers:
I1207 08:36:29.687018   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:29.687024   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:29.687034   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:29.691040   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:29.721527   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:29.732108   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:36:29.732132   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:36:29.732144   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:36:29.732150   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:36:29.732158   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:36:29.732169   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:36:29.732176   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:36:29.732188   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:36:29.732198   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:36:29.732208   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:36:29.732215   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:36:29.732224   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:36:29.732235   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:36:29.732242   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:36:29.732250   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:36:29.732257   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:36:29.732285   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:36:29.732297   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:36:29.732304   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:36:29.732314   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:36:29.732337   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:36:29.732357   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:36:29.732367   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:36:29.732376   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:36:29.732394   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:36:29.732405   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:36:29.732417   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:36:29.732435   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:36:29.732452   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:36:29.732476   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:36:29.732494   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:36:29.732516   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:36:29.732528   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:36:29.732540   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:36:29.732558   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:36:29.732582   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:36:29.732592   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:36:29.732604   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:36:29.732637   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:36:29.732665   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:36:29.732681   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:36:29.732699   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:36:29.732706   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:36:29.732713   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:36:29.732721   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:36:29.732727   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:36:29.732735   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:36:29.732750   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:36:29.732757   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:36:29.732767   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:36:29.732783   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:36:29.732793   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:36:29.732804   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:36:29.732827   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:36:29.732840   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:36:29.732855   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:36:29.732869   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:36:29.732899   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:36:29.732909   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:36:29.732917   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:36:29.732925   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:36:29.732933   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:36:29.732944   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:36:29.732951   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:36:29.732961   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:36:29.732970   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:36:29.910444   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:29.930698   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:26.510647467 +0000 UTC
I1207 08:36:29.930730   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:29.930737   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15212324Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:26.510647467 +0000 UTC
I1207 08:36:29.930744   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475400Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:26.510647467 +0000 UTC
I1207 08:36:29.930750   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618453, capacity: 6250Ki, time: 2021-12-07 08:36:26.510647467 +0000 UTC
I1207 08:36:29.930756   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475400Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:26.510647467 +0000 UTC
W1207 08:36:29.930764   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:29.930776   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:30.295685   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:31.302756   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:31.349496   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:31.349570   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:32.308831   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:33.315922   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:33.349523   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:33.349585   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:34.323033   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:34.349509   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:34.349605   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:34.350003   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} Ready:false RestartCount:8 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:36:34.350403   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:34.422108   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:34.422165   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:34.422287   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:34.422417   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:34.422438   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:34.422446   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:34.422488   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:34.422524   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:34.650827   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:34.650885   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:34.651759   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:34.652248   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:34.652389   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:34.652516   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:34.652688   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:34.655363   13809 kuberuntime_image.go:46] Pulling image "runjivu/hubrepo:5736" without credentials
I1207 08:36:34.655430   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hubrepo:5736"
I1207 08:36:34.723217   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:35.329413   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:35.349476   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:35.349540   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:36.336183   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:36.991349   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hubrepo:5736": "Status: Image is up to date for runjivu/hubrepo:5736"
I1207 08:36:36.993193   13809 kuberuntime_container.go:100] Generating ref for container c1: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}
I1207 08:36:36.993260   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:36:36.993342   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:36.993334   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hubrepo:5736"
I1207 08:36:36.993454   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:36:36.993493   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c1" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:36:36.996700   13809 expiration_cache.go:98] Entry version: {key:version obj:0xc4221fd220} has expired
I1207 08:36:36.997684   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:36:37.224000   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:36:37.342495   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:37.349683   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:37.349719   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:37.352802   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838"
E1207 08:36:37.353214   13809 kubelet.go:1628] Unable to mount volumes for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]; skipping pod
E1207 08:36:37.353243   13809 pod_workers.go:182] Error syncing pod 83d280aa-8ff5-11ea-a482-002248057bde ("kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"), skipping: timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]
I1207 08:36:37.353370   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:36:37.353401   13809 round_trippers.go:421] Request Headers:
I1207 08:36:37.353409   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:37.353416   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:36:37.353422   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:37.353603   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:37.353625   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' Unable to mount volumes for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]
I1207 08:36:37.355617   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:36:37.355699   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c1f97a2e2f7", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedSync", Message:"Error syncing pod", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353259767, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353259767, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Warning"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
I1207 08:36:37.355825   13809 round_trippers.go:414] POST https://10.240.255.5:443/api/v1/namespaces/kube-system/events
I1207 08:36:37.355838   13809 round_trippers.go:421] Request Headers:
I1207 08:36:37.355845   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:36:37.355853   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:37.355862   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:37.356167   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Started' Started container
I1207 08:36:37.357438   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838: non-existent -> running
I1207 08:36:37.357638   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:36:37.357716   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c1f97a1e7c7", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"Unable to mount volumes for pod \"kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)\": timeout expired waiting for volumes to attach/mount for pod \"kube-system\"/\"kube-proxy-wqgxc\". list of unattached/unmounted volumes=[default-token-fpq6c]", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353195463, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353195463, loc:(*time.Location)(0x9e22280)}}, Count:1, Type:"Warning"}': 'events is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot create events in the namespace "kube-system"' (will not retry!)
W1207 08:36:37.359335   13809 container.go:354] Failed to create summary reader for "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838": none of the resources are being tracked.
I1207 08:36:37.359360   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_9 cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838], namespace: "docker")
I1207 08:36:37.359452   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838 0001-01-01 00:00:00 +0000 UTC containerCreation {<nil>}}
I1207 08:36:37.359485   13809 manager.go:989] Destroyed container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_9 cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838], namespace: "docker")
I1207 08:36:37.359499   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838 2021-12-07 08:36:37.359496944 +0000 UTC containerDeletion {<nil>}}
I1207 08:36:37.359526   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838"
I1207 08:36:37.362773   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.515737   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc42083c540), (*container.ContainerStatus)(0xc42083c620), (*container.ContainerStatus)(0xc42272cc40), (*container.ContainerStatus)(0xc42083c8c0), (*container.ContainerStatus)(0xc42272cd20), (*container.ContainerStatus)(0xc42083c9a0), (*container.ContainerStatus)(0xc42272ce00), (*container.ContainerStatus)(0xc42083cd20)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc421433680)}} (err: <nil>)
I1207 08:36:37.515809   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerStarted", Data:"cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838"}
I1207 08:36:37.515882   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.516108   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\b', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:36:37.516196   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (8, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:36:37.516412   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:36:37.516428   13809 round_trippers.go:421] Request Headers:
I1207 08:36:37.516435   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:37.516443   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:37.516473   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.519529   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:37.519902   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:36:37.519919   13809 round_trippers.go:421] Request Headers:
I1207 08:36:37.519924   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:37.519928   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:36:37.519933   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:37.523448   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:37.523757   13809 config.go:282] Setting pods for source api
I1207 08:36:37.523851   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (8, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:36:37.524802   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.532941   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:37.532994   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:37.533136   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:36:37.533152   13809 round_trippers.go:421] Request Headers:
I1207 08:36:37.533159   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:37.533166   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:37.538995   13809 round_trippers.go:439] Response Status: 200 OK in 5 milliseconds
I1207 08:36:37.539061   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:37.539183   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:37.539197   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:37.539204   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:37.539238   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:37.539263   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:37.816832   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.816883   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 8 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:37.817762   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:37.818183   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:37.818312   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:37.818449   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.818647   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:37.818687   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:36:37.818836   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:37.818934   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:36:37.818997   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:38.516135   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:38.521186   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838: running -> exited
I1207 08:36:38.522184   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.548065   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc42088c000), (*container.ContainerStatus)(0xc421dca000), (*container.ContainerStatus)(0xc422186000), (*container.ContainerStatus)(0xc42088c1c0), (*container.ContainerStatus)(0xc421dca0e0), (*container.ContainerStatus)(0xc4221860e0), (*container.ContainerStatus)(0xc421dca1c0), (*container.ContainerStatus)(0xc4221861c0)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4227a2190)}} (err: <nil>)
I1207 08:36:38.548135   13809 kubelet_pods.go:1294] Generating status for "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:36:38.548208   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838"}
I1207 08:36:38.548327   13809 status_manager.go:325] Ignoring same status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 23:58:43 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-07 00:50:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 23:58:50 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2020-05-06 23:58:43 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:255,Signal:0,Reason:Error,Message:,StartedAt:2020-05-07 00:50:44 +0000 UTC,FinishedAt:2020-05-11 00:58:57 +0000 UTC,ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844}] QOSClass:BestEffort}
I1207 08:36:38.548553   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.548576   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:36:38.548670   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.548832   13809 kuberuntime_container.go:807] Removing container "c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c"
I1207 08:36:38.548961   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\t', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:36:38.549307   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.549236   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (9, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:36:38.549579   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:36:38.549594   13809 round_trippers.go:421] Request Headers:
I1207 08:36:38.549601   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:38.549640   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:38.552843   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:38.553205   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:36:38.553222   13809 round_trippers.go:421] Request Headers:
I1207 08:36:38.553227   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:38.553233   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:36:38.553240   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:38.557251   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:36:38.558006   13809 config.go:282] Setting pods for source api
I1207 08:36:38.558073   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (9, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:36:38.559036   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.636774   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:38.636872   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:38.637186   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:36:38.637220   13809 round_trippers.go:421] Request Headers:
I1207 08:36:38.637235   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:38.637249   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:38.641227   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:38.641375   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:38.641516   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:38.641533   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:38.641540   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:38.641583   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:38.641613   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:38.849560   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.849677   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:35:42 +0000 UTC,FinishedAt:2021-12-07 08:35:42 +0000 UTC,ContainerID:docker://c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:38.850510   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:38.851005   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:38.851139   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:38.851277   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.851549   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:38.851597   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:36:38.851729   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:38.852057   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:38.852106   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:36:39.349559   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:39.349564   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:39.349630   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:39.349720   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:39.349748   13809 round_trippers.go:421] Request Headers:
I1207 08:36:39.349762   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:39.349775   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:39.351817   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:36:39.351960   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:39.548419   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:39.553260   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/c830d3cac4654d0d7a35834a99616547844aa8664ec5ad2e23fd170a59f0021c: exited -> non-existent
I1207 08:36:39.554172   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:39.570404   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4208ee540), (*container.ContainerStatus)(0xc4221876c0), (*container.ContainerStatus)(0xc4208ee620), (*container.ContainerStatus)(0xc4221877a0), (*container.ContainerStatus)(0xc4208ee700), (*container.ContainerStatus)(0xc422187880), (*container.ContainerStatus)(0xc420a63a40)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4208954a0)}} (err: <nil>)
I1207 08:36:39.691867   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:39.691897   13809 round_trippers.go:421] Request Headers:
I1207 08:36:39.691907   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:39.691919   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:39.693959   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:39.702215   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:39.702236   13809 round_trippers.go:421] Request Headers:
I1207 08:36:39.702243   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:39.702251   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:39.702259   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:39.706262   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:39.724345   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:39.931054   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:39.952570   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:39.952595   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15211972Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:39.4326349 +0000 UTC
I1207 08:36:39.952606   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475160Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:39.4326349 +0000 UTC
I1207 08:36:39.952611   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:39.4326349 +0000 UTC
I1207 08:36:39.952616   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475160Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:39.4326349 +0000 UTC
I1207 08:36:39.952621   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:39.4326349 +0000 UTC
W1207 08:36:39.952628   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:39.952638   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:40.570760   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:41.349567   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:41.349640   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:41.577958   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:42.348952   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:36:42.349049   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:36:42.349594   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:36:42.349622   13809 round_trippers.go:421] Request Headers:
I1207 08:36:42.349634   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:42.349652   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:42.351871   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:36:42.352080   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:36:42.352270   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:38:44.352219972 +0000 UTC (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:36:42.352409   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:36:42.352834   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:36:42.352863   13809 round_trippers.go:421] Request Headers:
I1207 08:36:42.352877   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:42.352891   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:42.352903   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:42.354552   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:36:42.354665   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774463002, nsec:352196771, loc:(*time.Location)(0x9e22280)}}, Count:9, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:36:42.584416   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:43.349546   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:43.349625   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:43.589883   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:44.349534   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)
I1207 08:36:44.349679   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:36:44.349931   13809 status_manager.go:325] Ignoring same status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort}
I1207 08:36:44.350307   13809 kuberuntime_manager.go:428] Syncing Pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,GenerateName:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,UID:7df95486-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484049,Generation:0,CreationTimestamp:2021-12-07 08:27:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{app: caas,deploymenttype: infra,pod-template-hash: 2462294644,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},Annotations:map[string]string{kubernetes.io/config.seen: 2021-12-07T08:34:29.329958181Z,kubernetes.io/config.source: api,kubernetes.io/created-by: {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"caas-20d222f17dd4489fb180b51031b406f6","name":"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88","uid":"7df8c8a0-5737-11ec-9d1e-002248057bde","apiVersion":"extensions","resourceVersion":"1094476293"}}
,localRegistry-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[{extensions/v1beta1 ReplicaSet caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88 7df8c8a0-5737-11ec-9d1e-002248057bde 0xc420ae61e0 0xc420ae61e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{infra mcr.microsoft.com/aci/infra:master_20211112.1 [/bin/sh -c while true; do echo `date`; sleep 1000000; done] []  [] [] [] {map[] map[]} [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[{[{agentpool NotIn [system agentpool1]} {node-optimization-needed DoesNotExist []} {kubernetes.io/hostname In [k8s-agentpool4-a14krc0l-17]}]}],},PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAffinity:nil,PodAntiAffinity:&PodAntiAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[{app Exists []}],} [caas-827aef2dda4640159dd2550e4e471cf3 caas-8dca1f0909804431b986fc0cca798ca3 caas-b3d11bb4bae449faa2674fa20dee69cb caas-b516ef0c44d44dba913d83b46a14454c certbootstrap default kube-public kube-system] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.253,StartTime:2021-12-07 08:27:19 +0000 UTC,ContainerStatuses:[{infra {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}],QOSClass:BestEffort,InitContainerStatuses:[],},}
I1207 08:36:44.350845   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:36:44.595339   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:44.725965   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:45.349543   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:36:45.349611   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:45.600548   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:46.606508   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:47.349579   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:47.349659   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:47.611889   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:48.617000   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:49.349418   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:49.349513   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)
I1207 08:36:49.349560   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:49.349637   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:49.349578   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:49.349769   13809 round_trippers.go:421] Request Headers:
I1207 08:36:49.349801   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:49.349814   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:49.352146   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:36:49.352236   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:49.622045   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:49.706901   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:49.706926   13809 round_trippers.go:421] Request Headers:
I1207 08:36:49.706934   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:49.706941   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:49.709036   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:49.715801   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:49.715826   13809 round_trippers.go:421] Request Headers:
I1207 08:36:49.715834   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:49.715841   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:49.715850   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:49.719843   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:49.726945   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:49.952933   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:49.973641   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475116Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:49.973670   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:49.973677   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:49.973682   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15211988Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:49.973688   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475116Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:49.973693   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
W1207 08:36:49.973701   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:49.973715   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:36:50.627213   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:51.349555   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:51.349629   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:51.633222   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:52.349551   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)
I1207 08:36:52.349713   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:36:52.350006   13809 status_manager.go:325] Ignoring same status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable}
I1207 08:36:52.350420   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:36:52.381148   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:52.381269   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:52.381314   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:36:52.381327   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:36:52.381627   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/mdsd-config?resourceVersion=0
I1207 08:36:52.381647   13809 round_trippers.go:421] Request Headers:
I1207 08:36:52.381656   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:52.381658   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/default-token-74mlg?resourceVersion=0
I1207 08:36:52.381664   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:52.381669   13809 round_trippers.go:421] Request Headers:
I1207 08:36:52.381676   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:52.381683   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:52.383714   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:52.384017   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:36:52.384031   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:36:52.384128   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:36:52.384248   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:36:52.384280   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:36:52.384293   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:36:52.384305   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:36:52.384335   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:36:52.384355   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:36:52.384400   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:36:52.384441   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:36:52.384462   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:36:52.384582   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:36:52.640085   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:52.650756   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:36:52.650784   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[memory:{{1 9} {<nil>} 1G DecimalSI} cpu:{{200 -3} {<nil>} 200m DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:52.651335   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.349618   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:53.349737   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:53.349793   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:53.349945   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.350724   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:36:53.351514   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.384597   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:36:53.384670   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:53.384786   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:36:53.384944   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:36:53.384974   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:36:53.384982   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:36:53.385037   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:36:53.385086   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:36:53.645982   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:53.651883   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.651934   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:36:53.652829   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:36:53.653312   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:36:53.653451   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:36:53.653597   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.653784   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:36:53.653801   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:36:53.653916   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:36:53.654055   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:36:53.654095   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:36:54.654279   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:54.728541   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:55.349531   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:55.349602   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:55.660970   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:56.667289   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:56.820720   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc421d64740), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger"}
I1207 08:36:56.830580   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c988aa2-5737-11ec-9d1e-002248057bde/logger_0.log"
I1207 08:36:56.830614   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger?limitBytes=67104768&tailLines=100&timestamps=True: (10.012205ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:36:57.349556   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:36:57.349647   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:57.674564   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:58.679995   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:59.349556   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:36:59.349578   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:36:59.349645   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:36:59.349717   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:36:59.349765   13809 round_trippers.go:421] Request Headers:
I1207 08:36:59.349802   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:59.349828   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:59.351762   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
W1207 08:36:59.351882   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:36:59.687228   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:36:59.720587   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:36:59.720614   13809 round_trippers.go:421] Request Headers:
I1207 08:36:59.720622   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:59.720629   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:59.722620   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:36:59.729444   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:36:59.730195   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:36:59.730208   13809 round_trippers.go:421] Request Headers:
I1207 08:36:59.730214   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:36:59.730221   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:36:59.730226   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:36:59.734106   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:36:59.974025   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:36:59.995448   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33475116Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:59.995479   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:59.995489   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33475116Ki, capacity: 50758760Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:59.995498   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
I1207 08:36:59.995506   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:36:59.995514   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15211988Ki, capacity: 16426204Ki, time: 2021-12-07 08:36:49.650095355 +0000 UTC
W1207 08:36:59.995525   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:36:59.995547   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:00.693748   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:01.349514   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:01.349586   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:01.700685   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:02.706537   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:03.349503   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:03.349546   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:03.713247   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:04.720651   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:04.730495   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:05.349546   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:05.349601   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:05.727334   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:05.948179   13809 server.go:779] POST /stats/container/: (58.060912ms) 200 [[Go-http-client/1.1] 10.244.29.185:56956]
I1207 08:37:06.520424   13809 server.go:779] POST /stats/container/: (13.005786ms) 200 [[Go-http-client/1.1] 10.244.164.213:32942]
I1207 08:37:06.732418   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:07.349514   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:07.349577   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:07.739220   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:08.349523   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:08.349662   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:08.350158   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:37:08.350760   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:08.445651   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:08.445729   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:08.445961   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:37:08.446030   13809 round_trippers.go:421] Request Headers:
I1207 08:37:08.446062   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:08.446087   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:08.448464   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:08.448596   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:37:08.448811   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:37:08.448843   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:37:08.448852   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:37:08.448928   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:08.448979   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:37:08.651166   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:08.651232   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:37:08.652078   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:37:08.652518   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:37:08.652639   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:37:08.652777   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:08.652991   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:08.653052   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:08.653152   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
E1207 08:37:08.653236   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:08.653315   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:37:08.745900   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:09.349334   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:09.349509   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:09.349539   13809 round_trippers.go:421] Request Headers:
I1207 08:37:09.349552   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:09.349565   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:09.349623   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:09.349723   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:09.352312   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:09.352546   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:09.732264   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:09.734659   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:09.734689   13809 round_trippers.go:421] Request Headers:
I1207 08:37:09.734704   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:09.734718   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:09.737576   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:09.744409   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:09.744424   13809 round_trippers.go:421] Request Headers:
I1207 08:37:09.744431   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:09.744438   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:09.744445   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:09.748489   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:37:09.751931   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:09.995847   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:37:10.008522   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:10.008552   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474976Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:10.008559   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:10.008565   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:37:10.008570   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15208284Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:10.008576   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474976Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
W1207 08:37:10.008583   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:37:10.008598   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:10.755694   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:11.349550   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:11.349615   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:11.762705   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:12.769562   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:13.349542   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:13.349614   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:13.774668   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:14.734024   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:14.780285   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:15.349538   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:15.349601   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:15.787374   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:16.793490   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:17.349499   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:17.349564   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:17.799001   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:18.804398   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:19.349413   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:19.349565   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:19.349589   13809 round_trippers.go:421] Request Headers:
I1207 08:37:19.349569   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:19.349603   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:19.349620   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:19.349625   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:19.352552   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:19.352693   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:19.735620   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:19.749797   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:19.749813   13809 round_trippers.go:421] Request Headers:
I1207 08:37:19.749818   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:19.749822   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:19.751834   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:19.758892   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:19.758905   13809 round_trippers.go:421] Request Headers:
I1207 08:37:19.758910   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:19.758913   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:19.758917   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:19.763320   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:37:19.810755   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:20.008868   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:37:20.030514   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15208284Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:20.030545   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474976Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:20.030552   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:20.030558   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474976Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:20.030564   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:09.547620904 +0000 UTC
I1207 08:37:20.030569   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
W1207 08:37:20.030577   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:37:20.030588   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:20.816715   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:21.349555   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:21.349627   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:21.822137   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:22.349541   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:22.349639   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:22.350101   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:37:22.350661   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:22.400124   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:22.400204   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:22.400330   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:37:22.400547   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:37:22.400583   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:37:22.400597   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:37:22.400663   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:22.400744   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:37:22.651119   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:22.651177   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:37:22.651911   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:37:22.652304   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:37:22.652418   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:37:22.652564   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:22.652780   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:22.652801   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:22.652861   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
E1207 08:37:22.652962   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:22.653041   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:37:22.828504   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:23.349551   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:23.349626   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:23.835301   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:24.737339   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:24.841725   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:25.349506   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:25.349576   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:25.846750   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:26.852385   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:27.349526   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:27.349597   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:27.857604   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:28.863680   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:29.349505   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:29.349622   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:29.349828   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:29.349911   13809 round_trippers.go:421] Request Headers:
I1207 08:37:29.349965   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:29.349990   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:29.349657   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:29.350874   13809 qos_container_manager_linux.go:320] [ContainerManager]: Updated QoS cgroup configuration
I1207 08:37:29.352415   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:29.352556   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:29.363204   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:29.368275   13809 kubelet.go:1222] Container garbage collection succeeded
I1207 08:37:29.487756   13809 iptables.go:396] running iptables -N [KUBE-MARK-DROP -t nat]
I1207 08:37:29.490950   13809 iptables.go:396] running iptables -C [KUBE-MARK-DROP -t nat -j MARK --set-xmark 0x00008000/0x00008000]
I1207 08:37:29.493841   13809 iptables.go:396] running iptables -N [KUBE-FIREWALL -t filter]
I1207 08:37:29.495830   13809 iptables.go:396] running iptables -C [KUBE-FIREWALL -t filter -m comment --comment kubernetes firewall for dropping marked packets -m mark --mark 0x00008000/0x00008000 -j DROP]
I1207 08:37:29.497874   13809 iptables.go:396] running iptables -C [OUTPUT -t filter -j KUBE-FIREWALL]
I1207 08:37:29.499668   13809 iptables.go:396] running iptables -C [INPUT -t filter -j KUBE-FIREWALL]
I1207 08:37:29.501437   13809 iptables.go:396] running iptables -N [KUBE-MARK-MASQ -t nat]
I1207 08:37:29.502863   13809 iptables.go:396] running iptables -N [KUBE-POSTROUTING -t nat]
I1207 08:37:29.504417   13809 iptables.go:396] running iptables -C [KUBE-MARK-MASQ -t nat -j MARK --set-xmark 0x00004000/0x00004000]
I1207 08:37:29.506009   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]
I1207 08:37:29.507721   13809 iptables.go:396] running iptables -C [KUBE-POSTROUTING -t nat -m comment --comment kubernetes service traffic requiring SNAT -m mark --mark 0x00004000/0x00004000 -j MASQUERADE]
I1207 08:37:29.728582   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:37:29.728614   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:37:29.728625   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:37:29.728631   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:37:29.728638   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:37:29.728646   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:37:29.728652   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:37:29.728663   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:37:29.728679   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:37:29.728693   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:37:29.728700   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:37:29.728712   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:37:29.728724   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:37:29.728733   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:37:29.728747   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:37:29.728753   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:37:29.728761   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:37:29.728779   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:37:29.728786   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:37:29.728799   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:37:29.728808   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:37:29.728817   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:37:29.728827   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:37:29.728837   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:37:29.728846   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:37:29.728856   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:37:29.728867   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:37:29.728876   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:37:29.728886   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:37:29.728895   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:37:29.728908   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:37:29.728914   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:37:29.728923   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:37:29.728930   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:37:29.728938   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:37:29.728947   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:37:29.728955   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:37:29.728963   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:37:29.728977   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:37:29.728984   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:37:29.728993   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:37:29.729002   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:37:29.729012   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:37:29.729018   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:37:29.729026   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:37:29.729032   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:37:29.729043   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:37:29.729057   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:37:29.729068   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:37:29.729078   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:37:29.729090   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:37:29.729099   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:37:29.729111   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:37:29.729126   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:37:29.729151   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:37:29.729158   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:37:29.729167   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:37:29.729174   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:37:29.729185   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:37:29.729201   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:37:29.729214   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:37:29.729228   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:37:29.729245   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:37:29.729258   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:37:29.729270   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:37:29.729283   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:37:29.738430   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:29.763971   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:29.763993   13809 round_trippers.go:421] Request Headers:
I1207 08:37:29.764000   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:29.764007   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:29.765910   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:37:29.773292   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:29.773309   13809 round_trippers.go:421] Request Headers:
I1207 08:37:29.773315   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:29.773320   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:29.773324   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:29.777139   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:37:29.870436   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:30.030775   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:37:30.052946   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15207688Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:20.083667979 +0000 UTC
I1207 08:37:30.052975   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474952Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:20.083667979 +0000 UTC
I1207 08:37:30.052982   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:20.083667979 +0000 UTC
I1207 08:37:30.052988   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474952Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:20.083667979 +0000 UTC
I1207 08:37:30.052993   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:20.083667979 +0000 UTC
I1207 08:37:30.053015   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
W1207 08:37:30.053022   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:37:30.053032   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:30.877055   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:31.349519   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:31.349580   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:31.882874   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:32.888058   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:33.349517   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:33.349583   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:33.893358   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:34.649960   13809 reconciler.go:159] Desired state of world has been populated with pods, starting reconstruct state function
I1207 08:37:34.740126   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:34.898506   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:35.349566   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:35.349637   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:35.903843   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:36.349528   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:36.349644   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:36.350159   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:37:36.350714   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:36.356519   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:36.356564   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:36.356723   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:37:36.356758   13809 round_trippers.go:421] Request Headers:
I1207 08:37:36.356766   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:36.356771   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:36.358542   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:37:36.358623   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:37:36.358732   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:37:36.358750   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:37:36.358756   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:37:36.358792   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:36.358822   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:37:36.651076   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:36.651124   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:37:36.651880   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:37:36.652297   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:37:36.652401   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:37:36.652528   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:36.652720   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:36.652748   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:37:36.652864   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:36.652926   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:37:36.652960   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:37:36.909075   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:37.349551   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:37.349619   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:37.914568   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:38.921227   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:39.349533   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:39.349634   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:39.349671   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:39.349800   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:39.349826   13809 round_trippers.go:421] Request Headers:
I1207 08:37:39.349840   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:39.349854   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:39.352123   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:39.352268   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:39.741875   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:39.778238   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:39.778266   13809 round_trippers.go:421] Request Headers:
I1207 08:37:39.778277   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:39.778288   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:39.780387   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:39.788232   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:39.788258   13809 round_trippers.go:421] Request Headers:
I1207 08:37:39.788265   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:39.788272   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:39.788280   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:39.792581   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:37:39.927876   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:40.053348   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:37:40.074924   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474868Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:30.133003034 +0000 UTC
I1207 08:37:40.074955   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:30.133003034 +0000 UTC
I1207 08:37:40.074962   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:37:40.074968   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15207604Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:30.133003034 +0000 UTC
I1207 08:37:40.074973   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474868Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:30.133003034 +0000 UTC
I1207 08:37:40.074979   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:30.133003034 +0000 UTC
W1207 08:37:40.074987   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:37:40.075001   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:40.933411   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:41.349520   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:41.349575   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:41.939269   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:42.945909   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:43.349492   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:43.349541   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:43.951377   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:44.743626   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:44.956474   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:45.349536   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:45.349607   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:45.961821   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:46.966933   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:47.349509   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:37:47.349550   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:47.973073   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:48.979900   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:49.349375   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:49.349527   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:49.349580   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:49.349593   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:49.349597   13809 round_trippers.go:421] Request Headers:
I1207 08:37:49.349615   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:49.349630   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:49.351803   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:49.352010   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:49.745532   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:49.793363   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:49.793423   13809 round_trippers.go:421] Request Headers:
I1207 08:37:49.793438   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:49.793451   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:49.795772   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:49.803375   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:49.803390   13809 round_trippers.go:421] Request Headers:
I1207 08:37:49.803395   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:49.803401   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:49.803405   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:49.822203   13809 round_trippers.go:439] Response Status: 200 OK in 18 milliseconds
I1207 08:37:49.984958   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:50.075310   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:37:50.096931   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:37:50.096964   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474808Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:37:50.096971   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:37:50.096977   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:37:50.096982   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15207584Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:37:50.096989   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474808Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
W1207 08:37:50.096997   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:37:50.097008   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:37:50.991520   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:51.349523   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:51.349599   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:51.349642   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:51.349766   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:51.350252   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:37:51.350861   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:51.414322   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:51.414403   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:51.414659   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:37:51.414690   13809 round_trippers.go:421] Request Headers:
I1207 08:37:51.414716   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:51.414729   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:51.416983   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:51.417111   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:37:51.417271   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:37:51.417296   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:37:51.417308   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:37:51.417365   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:37:51.417436   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:37:51.651270   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:51.651332   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:37:51.652072   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:37:51.652495   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:37:51.652615   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:37:51.652727   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:51.652919   13809 kuberuntime_manager.go:749] Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:37:51.652941   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:37:51.653079   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:37:51.653148   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:37:51.653219   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:37:51.997356   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:53.004118   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:53.349568   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)
I1207 08:37:53.349671   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:53.349749   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:53.349916   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:37:53.350311   13809 status_manager.go:325] Ignoring same status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable}
I1207 08:37:53.350783   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:37:53.422459   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:53.422558   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:37:53.422624   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:37:53.422712   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:37:53.422946   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/mdsd-config?resourceVersion=0
I1207 08:37:53.423005   13809 round_trippers.go:421] Request Headers:
I1207 08:37:53.423041   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:53.423058   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:53.422969   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/default-token-74mlg?resourceVersion=0
I1207 08:37:53.423198   13809 round_trippers.go:421] Request Headers:
I1207 08:37:53.423234   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:53.423248   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:53.425337   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:53.425342   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:53.425695   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:37:53.425739   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:37:53.425906   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:37:53.425930   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:37:53.425935   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:37:53.425943   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:37:53.425955   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:37:53.425965   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:37:53.426064   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:37:53.426081   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:37:53.426100   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:37:53.426124   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:37:53.651249   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:37:53.651310   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[cpu:{{200 -3} {<nil>} 200m DecimalSI} memory:{{1 9} {<nil>} 1G DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:37:53.652245   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:37:54.010511   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:54.747278   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:55.017177   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:55.349524   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:55.349590   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:55.574335   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc421d341c0), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger"}
I1207 08:37:55.585296   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c988aa2-5737-11ec-9d1e-002248057bde/logger_0.log"
I1207 08:37:55.585340   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger?limitBytes=67104768&tailLines=100&timestamps=True: (11.104671ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:37:56.024244   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:57.030194   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:57.349520   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:57.349583   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:58.037243   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:59.043680   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:37:59.349507   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:37:59.349564   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:37:59.349671   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:37:59.349837   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:37:59.349860   13809 round_trippers.go:421] Request Headers:
I1207 08:37:59.349874   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:59.349922   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:59.352271   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:37:59.352557   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:37:59.748995   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:37:59.825525   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:37:59.825562   13809 round_trippers.go:421] Request Headers:
I1207 08:37:59.825573   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:59.825581   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:59.827817   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:37:59.835188   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:37:59.835204   13809 round_trippers.go:421] Request Headers:
I1207 08:37:59.835211   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:37:59.835217   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:37:59.835224   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:37:59.838783   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:00.049931   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:00.097276   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:00.116771   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474808Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:38:00.116803   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:38:00.116811   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:38:00.116816   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15207584Ki, capacity: 16426204Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:38:00.116822   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474808Ki, capacity: 50758760Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
I1207 08:38:00.116828   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618452, capacity: 6250Ki, time: 2021-12-07 08:37:43.790309366 +0000 UTC
W1207 08:38:00.116838   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:00.116853   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:01.055487   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:01.349516   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:01.349601   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:02.061354   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:03.066275   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:03.349496   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:03.349549   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:04.073004   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:04.750811   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:05.079496   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:05.349528   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:05.349591   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:05.840999   13809 server.go:779] POST /stats/container/: (54.610901ms) 200 [[Go-http-client/1.1] 10.244.29.185:56956]
I1207 08:38:05.968376   13809 server.go:779] POST /stats/container/: (11.999525ms) 200 [[Go-http-client/1.1] 10.244.164.213:32942]
I1207 08:38:06.084843   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:06.282815   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc422756ac0), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1"}
I1207 08:38:06.291838   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c8e9787-5737-11ec-9d1e-002248057bde/c1_9.log"
I1207 08:38:06.291878   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1?limitBytes=67104768&tailLines=2000&timestamps=False: (9.153353ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:38:07.090273   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:07.349527   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:07.349599   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:07.349671   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:07.349816   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:07.350330   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:9 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:38:07.350923   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:07.379485   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:07.379544   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:07.379748   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:38:07.379791   13809 round_trippers.go:421] Request Headers:
I1207 08:38:07.379814   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:07.379832   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:07.382734   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:07.382828   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:38:07.383039   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:38:07.383064   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:38:07.383073   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:38:07.383116   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:07.383152   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:38:07.651339   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:07.651397   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[memory:{{6 9} {<nil>} 6G DecimalSI} cpu:{{2 0} {<nil>} 2 DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:07.652175   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:38:07.652800   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:38:07.652913   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:07.653032   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:07.653182   13809 kuberuntime_manager.go:706] Creating container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:07.655821   13809 kuberuntime_image.go:46] Pulling image "runjivu/hubrepo:5736" without credentials
I1207 08:38:07.655865   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulling' pulling image "runjivu/hubrepo:5736"
I1207 08:38:08.096605   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:09.102348   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:09.349564   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:09.349627   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:09.349688   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:38:09.349826   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:38:09.349857   13809 round_trippers.go:421] Request Headers:
I1207 08:38:09.349871   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:09.349883   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:09.353373   13809 round_trippers.go:439] Response Status: 403 Forbidden in 3 milliseconds
W1207 08:38:09.353511   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:38:09.752558   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:09.839641   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:38:09.839675   13809 round_trippers.go:421] Request Headers:
I1207 08:38:09.839688   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:09.839701   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:09.841855   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:09.850345   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:38:09.850367   13809 round_trippers.go:421] Request Headers:
I1207 08:38:09.850374   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:09.850380   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:09.850387   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:09.862196   13809 round_trippers.go:439] Response Status: 200 OK in 11 milliseconds
I1207 08:38:09.976113   13809 kube_docker_client.go:333] Stop pulling image "runjivu/hubrepo:5736": "Status: Image is up to date for runjivu/hubrepo:5736"
I1207 08:38:09.978007   13809 kuberuntime_container.go:100] Generating ref for container c1: &v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}
I1207 08:38:09.978069   13809 container_manager_linux.go:634] Calling devicePluginHandler AllocateDevices
E1207 08:38:09.978144   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:09.978202   13809 kubelet_pods.go:123] container: caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c1 podIP: "10.244.35.254" creating hosts mount: true
I1207 08:38:09.978247   13809 kubelet_pods.go:199] Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" container "c1" mount "aci-metadata-volume" has propagation "PROPAGATION_HOST_TO_CONTAINER"
I1207 08:38:09.978163   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Pulled' Successfully pulled image "runjivu/hubrepo:5736"
I1207 08:38:09.981150   13809 expiration_cache.go:98] Entry version: {key:version obj:0xc421e57cc0} has expired
I1207 08:38:09.981582   13809 docker_service.go:408] Setting cgroup parent to: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde"
I1207 08:38:10.108184   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:10.117015   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:10.124106   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:38:10.124127   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15207608Ki, capacity: 16426204Ki, time: 2021-12-07 08:38:03.178076871 +0000 UTC
I1207 08:38:10.124138   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474788Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:03.178076871 +0000 UTC
I1207 08:38:10.124144   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618468, capacity: 6250Ki, time: 2021-12-07 08:38:03.178076871 +0000 UTC
I1207 08:38:10.124150   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474788Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:03.178076871 +0000 UTC
I1207 08:38:10.124161   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618468, capacity: 6250Ki, time: 2021-12-07 08:38:03.178076871 +0000 UTC
W1207 08:38:10.124169   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:10.124188   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:10.323850   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Created' Created container
I1207 08:38:10.447969   13809 factory.go:112] Using factory "docker" for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429"
I1207 08:38:10.448628   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Normal' reason: 'Started' Started container
W1207 08:38:10.450347   13809 container.go:354] Failed to create summary reader for "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429": none of the resources are being tracked.
I1207 08:38:10.450375   13809 manager.go:932] Added container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_10 d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429], namespace: "docker")
I1207 08:38:10.450460   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429 0001-01-01 00:00:00 +0000 UTC containerCreation {<nil>}}
I1207 08:38:10.450508   13809 manager.go:989] Destroyed container: "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429" (aliases: [k8s_c1_wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6_7c8e9787-5737-11ec-9d1e-002248057bde_10 d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429], namespace: "docker")
I1207 08:38:10.450530   13809 handler.go:325] Added event &{/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429 2021-12-07 08:38:10.450527164 +0000 UTC containerDeletion {<nil>}}
I1207 08:38:10.450557   13809 container.go:409] Start housekeeping for container "/kubepods/burstable/pod7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429"
I1207 08:38:11.114362   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:11.121754   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429: non-existent -> exited
I1207 08:38:11.126137   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.144609   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4212e2000), (*container.ContainerStatus)(0xc42083c2a0), (*container.ContainerStatus)(0xc4212e20e0), (*container.ContainerStatus)(0xc42083c380), (*container.ContainerStatus)(0xc4212e21c0), (*container.ContainerStatus)(0xc42083c460), (*container.ContainerStatus)(0xc42083c540), (*container.ContainerStatus)(0xc422186700)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4215cc6e0)}} (err: <nil>)
I1207 08:38:11.144667   13809 kubelet.go:1871] SyncLoop (PLEG): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", event: &pleg.PodLifecycleEvent{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Type:"ContainerDied", Data:"d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429"}
I1207 08:38:11.144695   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.144822   13809 kuberuntime_container.go:807] Removing container "cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838"
I1207 08:38:11.144750   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.145264   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\n', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:38:11.145341   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (10, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:38:11.145520   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:38:11.145529   13809 round_trippers.go:421] Request Headers:
I1207 08:38:11.145535   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:11.145540   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:11.145604   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.153247   13809 round_trippers.go:439] Response Status: 200 OK in 7 milliseconds
I1207 08:38:11.153544   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:38:11.153560   13809 round_trippers.go:421] Request Headers:
I1207 08:38:11.153568   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:11.153574   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:38:11.153581   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:11.156971   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:11.157217   13809 config.go:282] Setting pods for source api
I1207 08:38:11.157283   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (10, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} Ready:false RestartCount:10 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:38:11.158170   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.193553   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:11.193640   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:11.193818   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:38:11.193841   13809 round_trippers.go:421] Request Headers:
I1207 08:38:11.193850   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:11.193859   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:11.197315   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:11.197556   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:38:11.197690   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:38:11.197713   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:38:11.197722   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:38:11.197773   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:11.197811   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:38:11.349527   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:11.349589   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:11.445362   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc421d4ad80), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c4"}
I1207 08:38:11.445863   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.445912   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 1m20s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 9 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:11.446669   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:38:11.447072   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:38:11.447164   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:11.447269   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.447453   13809 kuberuntime_manager.go:749] Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:11.447467   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:38:11.447591   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:11.447638   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:38:11.447683   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:38:11.450601   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c8e9787-5737-11ec-9d1e-002248057bde/c4_0.log"
I1207 08:38:11.450651   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c4?limitBytes=67104768&tailLines=2000&timestamps=False: (5.390826ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:38:12.144846   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:12.151394   13809 generic.go:146] GenericPLEG: 7c8e9787-5737-11ec-9d1e-002248057bde/cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838: exited -> non-existent
I1207 08:38:12.152742   13809 kuberuntime_manager.go:834] getSandboxIDByPodUID got sandbox IDs ["50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b"] for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:12.169509   13809 generic.go:345] PLEG: Write status for wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/caas-20d222f17dd4489fb180b51031b406f6: &container.PodStatus{ID:"7c8e9787-5737-11ec-9d1e-002248057bde", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", IP:"10.244.35.254", ContainerStatuses:[]*container.ContainerStatus{(*container.ContainerStatus)(0xc4212e3dc0), (*container.ContainerStatus)(0xc42083d960), (*container.ContainerStatus)(0xc420a621c0), (*container.ContainerStatus)(0xc4212e3ea0), (*container.ContainerStatus)(0xc420a622a0), (*container.ContainerStatus)(0xc4208ee1c0), (*container.ContainerStatus)(0xc42083da40)}, SandboxStatuses:[]*runtime.PodSandboxStatus{(*runtime.PodSandboxStatus)(0xc4221c9ea0)}} (err: <nil>)
I1207 08:38:12.349503   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)
I1207 08:38:12.349627   13809 kubelet_pods.go:1294] Generating status for "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:38:12.349888   13809 status_manager.go:325] Ignoring same status for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:19 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.253 StartTime:2021-12-07 08:27:19 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:infra State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/infra:master_20211112.1 ImageID:docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 ContainerID:docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}] QOSClass:BestEffort}
I1207 08:38:12.350313   13809 kuberuntime_manager.go:428] Syncing Pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,GenerateName:caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf,UID:7df95486-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484049,Generation:0,CreationTimestamp:2021-12-07 08:27:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{app: caas,deploymenttype: infra,pod-template-hash: 2462294644,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},Annotations:map[string]string{kubernetes.io/config.seen: 2021-12-07T08:34:29.329958181Z,kubernetes.io/config.source: api,kubernetes.io/created-by: {"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"caas-20d222f17dd4489fb180b51031b406f6","name":"caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88","uid":"7df8c8a0-5737-11ec-9d1e-002248057bde","apiVersion":"extensions","resourceVersion":"1094476293"}}
,localRegistry-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[{extensions/v1beta1 ReplicaSet caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88 7df8c8a0-5737-11ec-9d1e-002248057bde 0xc420ae61e0 0xc420ae61e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{infra mcr.microsoft.com/aci/infra:master_20211112.1 [/bin/sh -c while true; do echo `date`; sleep 1000000; done] []  [] [] [] {map[] map[]} [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[{[{agentpool NotIn [system agentpool1]} {node-optimization-needed DoesNotExist []} {kubernetes.io/hostname In [k8s-agentpool4-a14krc0l-17]}]}],},PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAffinity:nil,PodAntiAffinity:&PodAntiAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[{app Exists []}],} [caas-827aef2dda4640159dd2550e4e471cf3 caas-8dca1f0909804431b986fc0cca798ca3 caas-b3d11bb4bae449faa2674fa20dee69cb caas-b516ef0c44d44dba913d83b46a14454c certbootstrap default kube-public kube-system] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:19 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.253,StartTime:2021-12-07 08:27:19 +0000 UTC,ContainerStatuses:[{infra {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:21 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/infra:master_20211112.1 docker-pullable://mcr.microsoft.com/aci/infra@sha256:cf37a34cf9e1fd7590f9742bcbfd04cf299960ba0d868d597937bed8d98583b5 docker://efe4f3f3bbffb67bfde434e9f9e15b10431a89017dc3030323935ba868a4b863}],QOSClass:BestEffort,InitContainerStatuses:[],},}
I1207 08:38:12.350753   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "caas-20d222f17dd4489fb180b51031b406f6-68b66f8b88-ns7sf_caas-20d222f17dd4489fb180b51031b406f6(7df95486-5737-11ec-9d1e-002248057bde)"
I1207 08:38:13.169869   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:13.349524   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:13.349598   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:14.175894   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:14.754212   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:15.182313   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:15.349521   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:15.349623   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:16.188722   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:17.195433   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:17.349518   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:17.349582   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:18.202253   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:19.210856   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:19.349329   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:38:19.349496   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:38:19.349553   13809 round_trippers.go:421] Request Headers:
I1207 08:38:19.349578   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:19.349591   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:19.349597   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:19.349886   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:19.351671   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:38:19.351854   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:38:19.755882   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:19.862924   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:38:19.862955   13809 round_trippers.go:421] Request Headers:
I1207 08:38:19.862970   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:19.862984   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:19.865085   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:19.876321   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:38:19.876345   13809 round_trippers.go:421] Request Headers:
I1207 08:38:19.876350   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:19.876354   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:19.876359   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:19.880223   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:20.107269   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc4221d5840), Verb:"create", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/exec/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c4"}
I1207 08:38:20.107322   13809 httpstream.go:57] Access to exec with tty and stderr is not supported, bypassing stderr
I1207 08:38:20.112422   13809 server.go:779] POST /exec/caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/c4?command=%2Fbin%2Fbash&error=1&input=1&output=1&tty=1: (5.166112ms) 302 [[Go-http-client/1.1] 10.244.29.230:49076]
I1207 08:38:20.119343   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc4225ac200), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/cri/exec/Tl1tYWrg"}
I1207 08:38:20.124329   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:20.141523   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:20.141554   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16296608Ki, capacity: 16426204Ki
I1207 08:38:20.141560   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15206300Ki, capacity: 16426204Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:20.141566   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474628Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:20.141571   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:20.141577   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474628Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
W1207 08:38:20.141584   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:20.141598   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:20.216349   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:21.220177   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:21.349517   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:38:21.349573   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:22.225007   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:23.231587   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:23.349555   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:23.349620   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:24.238275   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:24.757740   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:25.243445   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:25.349511   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:25.349562   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:26.249347   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:26.349494   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:26.349568   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.350004   13809 status_manager.go:340] Status Manager: adding pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: ('\v', {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) to podStatusChannel
I1207 08:38:26.350118   13809 status_manager.go:147] Status Manager: syncing pod: "7c8e9787-5737-11ec-9d1e-002248057bde", with status: (11, {Running [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }]   10.240.67.5 10.244.35.254 2021-12-07 08:27:16 +0000 UTC [] [{c1 {&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] Burstable}) from podStatusChannel
I1207 08:38:26.350367   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod
I1207 08:38:26.350382   13809 round_trippers.go:421] Request Headers:
I1207 08:38:26.350394   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:26.350405   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:26.350486   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.351065   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:26.351133   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:26.351314   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret?resourceVersion=0
I1207 08:38:26.351375   13809 round_trippers.go:421] Request Headers:
I1207 08:38:26.351417   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:26.351434   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:26.353253   13809 round_trippers.go:439] Response Status: 200 OK in 1 milliseconds
I1207 08:38:26.353424   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:38:26.353613   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:38:26.353639   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:38:26.353693   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:38:26.353774   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:26.353827   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:38:26.355332   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:38:26.355852   13809 round_trippers.go:414] PUT https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod/status
I1207 08:38:26.355880   13809 round_trippers.go:421] Request Headers:
I1207 08:38:26.355893   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:26.355907   13809 round_trippers.go:424]     Content-Type: application/vnd.kubernetes.protobuf
I1207 08:38:26.355920   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:26.359005   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:26.359321   13809 status_manager.go:451] Status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)" updated successfully: (11, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} Ready:false RestartCount:10 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable})
I1207 08:38:26.359586   13809 config.go:282] Setting pods for source api
I1207 08:38:26.360566   13809 kubelet.go:1850] SyncLoop (RECONCILE, "api"): "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.650853   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.650909   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {nil nil ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:36:37 +0000 UTC,FinishedAt:2021-12-07 08:36:37 +0000 UTC,ContainerID:docker://cb0ce1a6667b6c3ef223c83ad2c694106c9b9e6862250f2851c639c17d749838,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:26.651723   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:38:26.652121   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:38:26.652246   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:26.652351   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.652549   13809 kuberuntime_manager.go:749] Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:26.652574   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:38:26.652701   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:26.652780   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:38:26.652988   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:38:27.255972   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:27.349516   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:27.349571   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:28.262464   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:29.269046   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:29.349455   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:38:29.349552   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:29.349618   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:29.349650   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:38:29.349682   13809 round_trippers.go:421] Request Headers:
I1207 08:38:29.349697   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:29.349753   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:29.351732   13809 qos_container_manager_linux.go:320] [ContainerManager]: Updated QoS cgroup configuration
I1207 08:38:29.352937   13809 round_trippers.go:439] Response Status: 403 Forbidden in 3 milliseconds
W1207 08:38:29.353102   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:38:29.368549   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:29.374361   13809 kubelet.go:1222] Container garbage collection succeeded
I1207 08:38:29.509879   13809 iptables.go:396] running iptables -N [KUBE-MARK-DROP -t nat]
I1207 08:38:29.514192   13809 iptables.go:396] running iptables -C [KUBE-MARK-DROP -t nat -j MARK --set-xmark 0x00008000/0x00008000]
I1207 08:38:29.517890   13809 iptables.go:396] running iptables -N [KUBE-FIREWALL -t filter]
I1207 08:38:29.520539   13809 iptables.go:396] running iptables -C [KUBE-FIREWALL -t filter -m comment --comment kubernetes firewall for dropping marked packets -m mark --mark 0x00008000/0x00008000 -j DROP]
I1207 08:38:29.522751   13809 iptables.go:396] running iptables -C [OUTPUT -t filter -j KUBE-FIREWALL]
I1207 08:38:29.524660   13809 iptables.go:396] running iptables -C [INPUT -t filter -j KUBE-FIREWALL]
I1207 08:38:29.526552   13809 iptables.go:396] running iptables -N [KUBE-MARK-MASQ -t nat]
I1207 08:38:29.528261   13809 iptables.go:396] running iptables -N [KUBE-POSTROUTING -t nat]
I1207 08:38:29.529931   13809 iptables.go:396] running iptables -C [KUBE-MARK-MASQ -t nat -j MARK --set-xmark 0x00004000/0x00004000]
I1207 08:38:29.531873   13809 iptables.go:396] running iptables -C [POSTROUTING -t nat -m comment --comment kubernetes postrouting rules -j KUBE-POSTROUTING]
I1207 08:38:29.533775   13809 iptables.go:396] running iptables -C [KUBE-POSTROUTING -t nat -m comment --comment kubernetes service traffic requiring SNAT -m mark --mark 0x00004000/0x00004000 -j MASQUERADE]
I1207 08:38:29.746888   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
I1207 08:38:29.746921   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
I1207 08:38:29.746930   13809 manager.go:901] ignoring container "/system.slice/dev-hugepages.mount"
I1207 08:38:29.746935   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/mnt.mount"
I1207 08:38:29.746939   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/mnt.mount", but ignoring.
I1207 08:38:29.746945   13809 manager.go:901] ignoring container "/system.slice/mnt.mount"
I1207 08:38:29.746948   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
I1207 08:38:29.746953   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
I1207 08:38:29.746958   13809 manager.go:901] ignoring container "/system.slice/-.mount"
I1207 08:38:29.746961   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
I1207 08:38:29.746965   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
I1207 08:38:29.746970   13809 manager.go:901] ignoring container "/system.slice/var-lib-lxcfs.mount"
I1207 08:38:29.746974   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:38:29.746979   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/etc-kubernetes-volumeplugins.mount", but ignoring.
I1207 08:38:29.746984   13809 manager.go:901] ignoring container "/system.slice/etc-kubernetes-volumeplugins.mount"
I1207 08:38:29.746988   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:38:29.746997   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount", but ignoring.
I1207 08:38:29.747004   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c8e9787\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-aci\\x2dmetadata\\x2dvolume.mount"
I1207 08:38:29.747012   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
I1207 08:38:29.747016   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
I1207 08:38:29.747021   13809 manager.go:901] ignoring container "/system.slice/dev-mqueue.mount"
I1207 08:38:29.747025   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:38:29.747031   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount", but ignoring.
I1207 08:38:29.747041   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-mdsd\\x2ddata.mount"
I1207 08:38:29.747056   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:38:29.747066   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount", but ignoring.
I1207 08:38:29.747076   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-83d280aa\\x2d8ff5\\x2d11ea\\x2da482\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2dfpq6c.mount"
I1207 08:38:29.747094   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
I1207 08:38:29.747101   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
I1207 08:38:29.747110   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-debug.mount"
I1207 08:38:29.747116   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:38:29.747133   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
I1207 08:38:29.747159   13809 manager.go:901] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
I1207 08:38:29.747165   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:38:29.747185   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount", but ignoring.
I1207 08:38:29.747206   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet-pods-7c988aa2\\x2d5737\\x2d11ec\\x2d9d1e\\x2d002248057bde-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d74mlg.mount"
I1207 08:38:29.747227   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:38:29.747237   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount", but ignoring.
I1207 08:38:29.747250   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-e737d65f4ec7243252afa002c81dfd5b5e0204a6da132f7720c8db02e0cdd826-shm.mount"
I1207 08:38:29.747261   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
I1207 08:38:29.747269   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
I1207 08:38:29.747279   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-default.mount"
I1207 08:38:29.747286   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-config.mount"
I1207 08:38:29.747294   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-config.mount", but ignoring.
I1207 08:38:29.747306   13809 manager.go:901] ignoring container "/system.slice/sys-kernel-config.mount"
I1207 08:38:29.747313   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet.mount"
I1207 08:38:29.747320   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet.mount", but ignoring.
I1207 08:38:29.747332   13809 manager.go:901] ignoring container "/system.slice/var-lib-kubelet.mount"
I1207 08:38:29.747339   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:38:29.747350   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-2854f367b9d4.mount", but ignoring.
I1207 08:38:29.747363   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-2854f367b9d4.mount"
I1207 08:38:29.747370   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:38:29.747378   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay2.mount", but ignoring.
I1207 08:38:29.747390   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-overlay2.mount"
I1207 08:38:29.747397   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:38:29.747411   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-409635be03c0.mount", but ignoring.
I1207 08:38:29.747421   13809 manager.go:901] ignoring container "/system.slice/run-docker-netns-409635be03c0.mount"
I1207 08:38:29.747429   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:38:29.747442   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount", but ignoring.
I1207 08:38:29.747457   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b-shm.mount"
I1207 08:38:29.747468   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-rpc_pipefs.mount"
I1207 08:38:29.747476   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/run-rpc_pipefs.mount", but ignoring.
I1207 08:38:29.747486   13809 manager.go:901] ignoring container "/system.slice/run-rpc_pipefs.mount"
I1207 08:38:29.747495   13809 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:38:29.747505   13809 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount", but ignoring.
I1207 08:38:29.747520   13809 manager.go:901] ignoring container "/system.slice/var-lib-docker-containers-b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c-shm.mount"
I1207 08:38:29.758865   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:29.880857   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:38:29.880894   13809 round_trippers.go:421] Request Headers:
I1207 08:38:29.880904   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:29.880913   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:29.882954   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:29.890634   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:38:29.890649   13809 round_trippers.go:421] Request Headers:
I1207 08:38:29.890655   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:29.890661   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:29.890667   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:29.900039   13809 round_trippers.go:439] Response Status: 200 OK in 9 milliseconds
I1207 08:38:30.141912   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:30.161978   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15206300Ki, capacity: 16426204Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:30.162010   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474628Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:30.162018   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:30.162025   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474628Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:30.162031   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:19.974154976 +0000 UTC
I1207 08:38:30.162036   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16295620Ki, capacity: 16426204Ki
W1207 08:38:30.162043   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:30.162058   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:30.274735   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:31.279304   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:31.349487   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:31.349529   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:32.286005   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:33.293031   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:33.349485   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:38:33.349536   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:34.299038   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:34.760671   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:35.304396   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:35.349484   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:35.349537   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:36.309409   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:37.315082   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:37.349481   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:37.349518   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:38.321820   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:38.349488   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:38.349550   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:38.349869   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} Ready:false RestartCount:10 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:38:38.350228   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:38.397595   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:38.397680   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:38.397920   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/aci-metadata-secret
I1207 08:38:38.397947   13809 round_trippers.go:421] Request Headers:
I1207 08:38:38.397961   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:38.397975   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:38.408229   13809 round_trippers.go:439] Response Status: 200 OK in 10 milliseconds
I1207 08:38:38.408304   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:38:38.408427   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:38:38.408443   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:38:38.408447   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:38:38.408480   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:38.408506   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:38:38.650508   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:38.650575   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[memory:{{500 6} {<nil>} 500M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:38.651335   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI} cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:38:38.651776   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:38:38.651889   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:38.652019   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:38.652224   13809 kuberuntime_manager.go:749] Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:38.652254   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:38.652386   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
E1207 08:38:38.652418   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:38.652516   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:38:39.327426   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:39.349308   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:38:39.349418   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:38:39.349429   13809 round_trippers.go:421] Request Headers:
I1207 08:38:39.349437   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:39.349452   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:39.349499   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:39.349521   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:39.351745   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:38:39.351839   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:38:39.762440   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:39.900713   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:38:39.900746   13809 round_trippers.go:421] Request Headers:
I1207 08:38:39.900760   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:39.900773   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:39.902928   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:39.914541   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:38:39.914557   13809 round_trippers.go:421] Request Headers:
I1207 08:38:39.914564   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:39.914570   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:39.914575   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:39.918468   13809 round_trippers.go:439] Response Status: 200 OK in 3 milliseconds
I1207 08:38:40.162373   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:40.182105   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15205104Ki, capacity: 16426204Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:40.182139   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474516Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:40.182146   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:40.182153   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474516Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:40.182159   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:40.182165   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16295620Ki, capacity: 16426204Ki
W1207 08:38:40.182172   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:40.182185   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:40.334383   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:41.340178   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:41.349448   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:41.349477   13809 kubelet.go:1913] SyncLoop (housekeeping)
E1207 08:38:41.549043   13809 kubelet.go:1628] Unable to mount volumes for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]; skipping pod
E1207 08:38:41.549080   13809 pod_workers.go:182] Error syncing pod 83d280aa-8ff5-11ea-a482-002248057bde ("kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"), skipping: timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]
I1207 08:38:41.549191   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:38:41.549253   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' Unable to mount volumes for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-proxy-wqgxc". list of unattached/unmounted volumes=[default-token-fpq6c]
I1207 08:38:41.549556   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c1f97a2e2f7
I1207 08:38:41.549612   13809 round_trippers.go:421] Request Headers:
I1207 08:38:41.549637   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:41.549645   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:41.549655   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:41.551539   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:38:41.551674   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c1f97a2e2f7", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedSync", Message:"Error syncing pod", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353259767, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774463121, nsec:549099143, loc:(*time.Location)(0x9e22280)}}, Count:2, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c1f97a2e2f7" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:38:41.552224   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c1f97a1e7c7
I1207 08:38:41.552250   13809 round_trippers.go:421] Request Headers:
I1207 08:38:41.552263   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:41.552276   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:41.552300   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:41.553954   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:38:41.554068   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c1f97a1e7c7", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"Unable to mount volumes for pod \"kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)\": timeout expired waiting for volumes to attach/mount for pod \"kube-system\"/\"kube-proxy-wqgxc\". list of unattached/unmounted volumes=[default-token-fpq6c]", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462997, nsec:353195463, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774463121, nsec:549008438, loc:(*time.Location)(0x9e22280)}}, Count:2, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c1f97a1e7c7" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:38:42.346493   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:42.351646   13809 kubelet_pods.go:1294] Generating status for "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:38:42.351776   13809 status_manager.go:325] Ignoring same status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 23:58:43 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-07 00:50:44 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [kube-proxy]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-06 23:58:50 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2020-05-06 23:58:43 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:255,Signal:0,Reason:Error,Message:,StartedAt:2020-05-07 00:50:44 +0000 UTC,FinishedAt:2020-05-11 00:58:57 +0000 UTC,ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://a226eb51fb6d6750fc140c235e91283d31254a53f4f4963b5dbe6e34e57f9844}] QOSClass:BestEffort}
I1207 08:38:42.351970   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)"
I1207 08:38:43.349511   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:43.349570   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:43.351809   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:44.357281   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:44.422079   13809 reconciler.go:257] operationExecutor.MountVolume started for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") 
I1207 08:38:44.422152   13809 secret.go:186] Setting up volume default-token-fpq6c for pod 83d280aa-8ff5-11ea-a482-002248057bde at /var/lib/kubelet/pods/83d280aa-8ff5-11ea-a482-002248057bde/volumes/kubernetes.io~secret/default-token-fpq6c
I1207 08:38:44.422374   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/secrets/default-token-fpq6c
I1207 08:38:44.422401   13809 round_trippers.go:421] Request Headers:
I1207 08:38:44.422530   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:44.422559   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:44.424951   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
E1207 08:38:44.425112   13809 secret.go:201] Couldn't get secret kube-system/default-token-fpq6c: secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
E1207 08:38:44.425272   13809 nestedpendingoperations.go:264] Operation for "\"kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c\" (\"83d280aa-8ff5-11ea-a482-002248057bde\")" failed. No retries permitted until 2021-12-07 08:40:46.425226723 +0000 UTC (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "default-token-fpq6c" (UniqueName: "kubernetes.io/secret/83d280aa-8ff5-11ea-a482-002248057bde-default-token-fpq6c") pod "kube-proxy-wqgxc" (UID: "83d280aa-8ff5-11ea-a482-002248057bde") : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:38:44.425504   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}): type: 'Warning' reason: 'FailedMount' MountVolume.SetUp failed for volume "default-token-fpq6c" : secrets "default-token-fpq6c" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get secrets in the namespace "kube-system"
I1207 08:38:44.425729   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/namespaces/kube-system/events/kube-proxy-wqgxc.16be6c0300709dd0
I1207 08:38:44.425760   13809 round_trippers.go:421] Request Headers:
I1207 08:38:44.425773   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:44.425785   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:44.425798   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:44.427385   13809 round_trippers.go:439] Response Status: 403 Forbidden in 1 milliseconds
E1207 08:38:44.427490   13809 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy-wqgxc.16be6c0300709dd0", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-proxy-wqgxc", UID:"83d280aa-8ff5-11ea-a482-002248057bde", APIVersion:"v1", ResourceVersion:"7110174", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"default-token-fpq6c\" : secrets \"default-token-fpq6c\" is forbidden: User \"62c48211-a4e1-45c2-8886-2097e33c93b9\" cannot get secrets in the namespace \"kube-system\"", Source:v1.EventSource{Component:"kubelet", Host:"k8s-agentpool4-a14krc0l-17"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63774462874, nsec:557521360, loc:(*time.Location)(0x9e22280)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63774463124, nsec:425178520, loc:(*time.Location)(0x9e22280)}}, Count:10, Type:"Warning"}': 'events "kube-proxy-wqgxc.16be6c0300709dd0" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot patch events in the namespace "kube-system"' (will not retry!)
I1207 08:38:44.764167   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:45.349503   13809 config.go:101] Looking for [api file], have seen map[api:{} file:{}]
I1207 08:38:45.349563   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:45.362524   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:46.365841   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:47.349515   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:47.349580   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:47.372393   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:48.376035   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:49.349503   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:49.349570   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:49.349612   13809 status_manager.go:410] Status Manager: syncPod in syncbatch. pod UID: "83d280aa-8ff5-11ea-a482-002248057bde"
I1207 08:38:49.349752   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/kube-system/pods/kube-proxy-wqgxc
I1207 08:38:49.349778   13809 round_trippers.go:421] Request Headers:
I1207 08:38:49.349792   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:49.349805   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:49.351906   13809 round_trippers.go:439] Response Status: 403 Forbidden in 2 milliseconds
W1207 08:38:49.352015   13809 status_manager.go:431] Failed to get status for pod "kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)": pods "kube-proxy-wqgxc" is forbidden: User "62c48211-a4e1-45c2-8886-2097e33c93b9" cannot get pods in the namespace "kube-system"
I1207 08:38:49.381792   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:49.765635   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:49.919043   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17?resourceVersion=0
I1207 08:38:49.919076   13809 round_trippers.go:421] Request Headers:
I1207 08:38:49.919089   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:49.919102   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:49.921245   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:49.929791   13809 round_trippers.go:414] PATCH https://10.240.255.5:443/api/v1/nodes/k8s-agentpool4-a14krc0l-17/status
I1207 08:38:49.929806   13809 round_trippers.go:421] Request Headers:
I1207 08:38:49.929814   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:49.929821   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:49.929827   13809 round_trippers.go:424]     Content-Type: application/strategic-merge-patch+json
I1207 08:38:49.934022   13809 round_trippers.go:439] Response Status: 200 OK in 4 milliseconds
I1207 08:38:50.182484   13809 eviction_manager.go:221] eviction manager: synchronize housekeeping
I1207 08:38:50.204019   13809 helpers.go:875] eviction manager: observations: signal=memory.available, available: 15205104Ki, capacity: 16426204Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:50.204054   13809 helpers.go:875] eviction manager: observations: signal=nodefs.available, available: 33474516Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:50.204065   13809 helpers.go:875] eviction manager: observations: signal=nodefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:50.204075   13809 helpers.go:875] eviction manager: observations: signal=imagefs.available, available: 33474516Ki, capacity: 50758760Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:50.204083   13809 helpers.go:875] eviction manager: observations: signal=imagefs.inodesFree, available: 5618467, capacity: 6250Ki, time: 2021-12-07 08:38:37.822571773 +0000 UTC
I1207 08:38:50.204092   13809 helpers.go:877] eviction manager: observations: signal=allocatableMemory.available, available: 16295652Ki, capacity: 16426204Ki
W1207 08:38:50.204103   13809 helpers.go:847] eviction manager: no observation found for eviction signal allocatableNodeFs.available
I1207 08:38:50.204122   13809 eviction_manager.go:325] eviction manager: no resources are starved
I1207 08:38:50.386647   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:51.349525   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:51.349608   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:51.349651   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:51.349908   13809 kubelet_pods.go:1294] Generating status for "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:51.350422   13809 status_manager.go:325] Ignoring same status for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:31:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [c1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.244.35.254 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:c1 State:{Waiting:&ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} Ready:false RestartCount:10 Image:runjivu/hubrepo:5736 ImageID:docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {Name:c2 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {Name:c3 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} Ready:true RestartCount:1 Image:k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 ImageID:docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded ContainerID:docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {Name:c4 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.9.10 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 ContainerID:docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {Name:c5 State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:runjivu/hyperkube-amd64:v1.10.9 ImageID:docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 ContainerID:docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}] QOSClass:Burstable}
I1207 08:38:51.350974   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:51.393003   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:51.449905   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:51.449977   13809 secret.go:186] Setting up volume aci-metadata-volume for pod 7c8e9787-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:51.450095   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/aci-metadata-secret containing (1) pieces of data, 36 total bytes
I1207 08:38:51.450255   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: current paths:   [correlationId]
I1207 08:38:51.450361   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: new paths:       [correlationId]
I1207 08:38:51.450376   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume: paths to remove: map[]
I1207 08:38:51.450442   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod volume aci-metadata-volume: no update required for target directory /var/lib/kubelet/pods/7c8e9787-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/aci-metadata-volume
I1207 08:38:51.450487   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "aci-metadata-volume" (UniqueName: "kubernetes.io/secret/7c8e9787-5737-11ec-9d1e-002248057bde-aci-metadata-volume") pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" (UID: "7c8e9787-5737-11ec-9d1e-002248057bde") 
I1207 08:38:51.651239   13809 volume_manager.go:366] All volumes are attached and mounted for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:51.651293   13809 kuberuntime_manager.go:428] Syncing Pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod,UID:7c8e9787-5737-11ec-9d1e-002248057bde,ResourceVersion:1094482440,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{correlationid: F16F721AB21B4D2DB7705C2B7E273DBD_96D07D44BBF6070E,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,worker: caas,},Annotations:map[string]string{azure-resource-id: /subscriptions/f9a866c6-a367-4fb7-872c-6ddac39a3f4b/resourceGroups/escape/providers/Microsoft.ContainerInstance/containerGroups/pocContainergroup,kubernetes.io/config.seen: 2021-12-07T08:34:29.32994448Z,kubernetes.io/config.source: api,msi-enabled: False,networkContainerID: 86a48576-53f7-855d-da3b-ea24163f5a79,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{aci-metadata-volume {nil nil nil nil nil SecretVolumeSource{SecretName:aci-metadata-secret,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{c1 runjivu/hubrepo:5736 [] []  [] [] [] {map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}] map[cpu:{{2 0} {<nil>} 2 DecimalSI} memory:{{6 9} {<nil>} 6G DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c2 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[memory:{{600 6} {<nil>} 600M DecimalSI} cpu:{{100 -3} {<nil>} 100m DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c3 runjivu/hyperkube-amd64:v1.8.4 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c4 runjivu/hyperkube-amd64:v1.9.10 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false} {c5 runjivu/hyperkube-amd64:v1.10.9 [/bin/sleep inf] []  [] [] [] {map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{600 6} {<nil>} 600M DecimalSI}] map[cpu:{{100 -3} {<nil>} 100m DecimalSI} memory:{{500 6} {<nil>} 500M DecimalSI}]} [{aci-metadata-volume true /var/aci_metadata  <nil>}] nil nil nil /dev/termination-log File Always &SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:&Affinity{NodeAffinity:nil,PodAffinity:&PodAffinity{RequiredDuringSchedulingIgnoredDuringExecution:[{LabelSelector{MatchLabels:map[string]string{app: caas,deploymenttype: infra,resourcegroup: ESCAPE_ECD9C6863EC3E03A,resourcename: POCCONTAINERGROUP_4324F3DA79994CC5,subscription: F9A866C6A3674FB7872C6DDAC39A3F4B_82D97523CE8F4C4F,},MatchExpressions:[],} [caas-20d222f17dd4489fb180b51031b406f6] kubernetes.io/hostname}],PreferredDuringSchedulingIgnoredDuringExecution:[],},PodAntiAffinity:nil,},SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:*false,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:31:38 +0000 UTC ContainersNotReady containers with unready status: [c1]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.244.35.254,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{c1 {ContainerStateWaiting{Reason:CrashLoopBackOff,Message:Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde),} nil nil} {nil nil &ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2021-12-07 08:38:10 +0000 UTC,FinishedAt:2021-12-07 08:38:10 +0000 UTC,ContainerID:docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429,}} false 10 runjivu/hubrepo:5736 docker-pullable://runjivu/hubrepo@sha256:59951986d22f34a2468255a5a0c76645e34384b3a109f3472aedb649ee3752fc docker://d254747224cf534d977c62148bae444266de66913f06e24de5c88bb2d60a7429} {c2 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:53 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:52 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://d397d5cbd5e7855ededd6063c392be5e9706ccce203f2d7c9c687b4f09a7b479,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://6126707f6775ae4ac9a78d982d26006e9d5940134b7b785f8cdbfafda4d38974} {c3 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:34:55 +0000 UTC,} nil} {nil nil &ContainerStateTerminated{ExitCode:137,Signal:0,Reason:Error,Message:,StartedAt:2021-12-07 08:27:55 +0000 UTC,FinishedAt:2021-12-07 08:32:23 +0000 UTC,ContainerID:docker://0342615ed9eb364eb24f9f10c048a50c9199ddcaee013a56904fab63e439bb14,}} true 1 k8s-gcrio.azureedge.net/hyperkube-amd64:v1.8.4 docker-pullable://k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:a68d3ebeb5d8a8b4b9697a5370ea1b7ad921c403cd7429c7ac859193901c9ded docker://01c75cc3bbdb881410d938713360473d79dcb60b13e54d378f7d29164e03825d} {c4 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:27:59 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.9.10 docker-pullable://runjivu/hyperkube-amd64@sha256:0a32acce69c8f7fb3b5e38fedad9f85d2d1d8d67cea8141ab85dacf47db5e668 docker://1a24c8aee57c832a5dee501aeb39f370e87dd50cc1ffdd0fb60071c885a0b731} {c5 {nil &ContainerStateRunning{StartedAt:2021-12-07 08:28:28 +0000 UTC,} nil} {nil nil nil} true 0 runjivu/hyperkube-amd64:v1.10.9 docker-pullable://runjivu/hyperkube-amd64@sha256:30592b4a36b1f9b2a1267923a8800d88f0c664501324d30a8d9afc697295a5f0 docker://6fb4d9f603aecc7a5ae6393ad5147379687febc0068d8d27947380e421346864}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:51.652047   13809 kuberuntime_manager.go:500] Container {Name:c1 Image:runjivu/hubrepo:5736 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}] Requests:map[cpu:{i:{value:2 scale:0} d:{Dec:<nil>} s:2 Format:DecimalSI} memory:{i:{value:6 scale:9} d:{Dec:<nil>} s:6G Format:DecimalSI}]} VolumeMounts:[{Name:aci-metadata-volume ReadOnly:true MountPath:/var/aci_metadata SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I1207 08:38:51.652505   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:50d130e4b5a92e2d13ca25cbf2ff0d2010c8c4783f163f0e57b20b5a5114dd6b Attempt:0 NextInitContainerToStart:nil ContainersToStart:[0] ContainersToKill:map[]} for pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
E1207 08:38:51.652642   13809 kubelet_pods.go:335] hostname for pod:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod" was longer than 63. Truncated hostname to :"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e79"
I1207 08:38:51.652766   13809 kuberuntime_manager.go:739] checking backoff for container "c1" in pod "wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:51.653020   13809 kuberuntime_manager.go:749] Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
I1207 08:38:51.653055   13809 kuberuntime_manager.go:702] Backing Off restarting container &Container{Name:c1,Image:runjivu/hubrepo:5736,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},Requests:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},memory: {{6 9} {<nil>} 6G DecimalSI},},},VolumeMounts:[{aci-metadata-volume true /var/aci_metadata  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,} in pod wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)
E1207 08:38:51.653224   13809 pod_workers.go:182] Error syncing pod 7c8e9787-5737-11ec-9d1e-002248057bde ("wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"), skipping: failed to "StartContainer" for "c1" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=c1 pod=wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod_caas-20d222f17dd4489fb180b51031b406f6(7c8e9787-5737-11ec-9d1e-002248057bde)"
I1207 08:38:51.653292   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:"spec.containers{c1}"}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
I1207 08:38:51.653440   13809 server.go:227] Event(v1.ObjectReference{Kind:"Pod", Namespace:"caas-20d222f17dd4489fb180b51031b406f6", Name:"wk-caas-20d222f17dd4489fb180b51031b406f6-2da3a09bc7538b3a651e793051998713-pod", UID:"7c8e9787-5737-11ec-9d1e-002248057bde", APIVersion:"v1", ResourceVersion:"1094482440", FieldPath:""}): type: 'Warning' reason: 'FailedSync' Error syncing pod
I1207 08:38:52.397538   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:53.349555   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:53.349632   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:53.403958   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:54.409433   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:54.755663   13809 auth.go:111] Node request attributes: attrs=authorizer.AttributesRecord{User:(*user.DefaultInfo)(0xc421e4fac0), Verb:"get", Namespace:"", APIGroup:"", APIVersion:"v1", Resource:"nodes", Subresource:"proxy", Name:"k8s-agentpool4-a14krc0l-17", ResourceRequest:true, Path:"/containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger"}
I1207 08:38:54.766769   13809 kubelet.go:2092] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
I1207 08:38:54.767420   13809 kuberuntime_logs.go:158] Finish parsing log file "/var/log/pods/7c988aa2-5737-11ec-9d1e-002248057bde/logger_0.log"
I1207 08:38:54.767461   13809 server.go:779] GET /containerLogs/caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd/logger?limitBytes=67104768&tailLines=100&timestamps=True: (11.900118ms) 200 [[Go-http-client/2.0] 10.240.255.5:41106]
I1207 08:38:55.349557   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:55.349623   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:55.416625   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:56.349513   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; kube-proxy-wqgxc_kube-system(83d280aa-8ff5-11ea-a482-002248057bde)
I1207 08:38:56.422287   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:57.349569   13809 kubelet.go:1890] SyncLoop (SYNC): 1 pods; shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)
I1207 08:38:57.349672   13809 config.go:101] Looking for [api file], have seen map[file:{} api:{}]
I1207 08:38:57.349723   13809 kubelet.go:1913] SyncLoop (housekeeping)
I1207 08:38:57.349979   13809 kubelet_pods.go:1294] Generating status for "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:38:57.350571   13809 status_manager.go:325] Ignoring same status for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:34:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 08:27:16 +0000 UTC Reason: Message:}] Message: Reason: HostIP:10.240.67.5 PodIP:10.240.67.5 StartTime:2021-12-07 08:27:16 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:logger State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 ImageID:docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd ContainerID:docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}] QOSClass:Burstable}
I1207 08:38:57.351205   13809 volume_manager.go:337] Waiting for volumes to attach and mount for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:38:57.372785   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:57.372877   13809 reconciler.go:259] operationExecutor.MountVolume started for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") Volume is already mounted to pod, but remount was requested.
I1207 08:38:57.372902   13809 secret.go:186] Setting up volume mdsd-data for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:38:57.372943   13809 secret.go:186] Setting up volume default-token-74mlg for pod 7c988aa2-5737-11ec-9d1e-002248057bde at /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:38:57.373117   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/default-token-74mlg?resourceVersion=0
I1207 08:38:57.373131   13809 round_trippers.go:421] Request Headers:
I1207 08:38:57.373136   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:57.373143   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:57.373146   13809 round_trippers.go:414] GET https://10.240.255.5:443/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/secrets/mdsd-config?resourceVersion=0
I1207 08:38:57.373157   13809 round_trippers.go:421] Request Headers:
I1207 08:38:57.373180   13809 round_trippers.go:424]     Accept: application/vnd.kubernetes.protobuf, */*
I1207 08:38:57.373204   13809 round_trippers.go:424]     User-Agent: hyperkube/v1.8.4 (linux/amd64) kubernetes/9befc2b
I1207 08:38:57.375242   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:57.375476   13809 round_trippers.go:439] Response Status: 200 OK in 2 milliseconds
I1207 08:38:57.375520   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/mdsd-config containing (3) pieces of data, 3863 total bytes
I1207 08:38:57.375571   13809 secret.go:217] Received secret caas-20d222f17dd4489fb180b51031b406f6/default-token-74mlg containing (3) pieces of data, 3024 total bytes
I1207 08:38:57.375657   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: current paths:   [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:38:57.375674   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: new paths:       [gcscert.pem gcskey.pem mdsd.xml]
I1207 08:38:57.375683   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data: paths to remove: map[]
I1207 08:38:57.375721   13809 atomic_writer.go:329] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: current paths:   [ca.crt namespace token]
I1207 08:38:57.375734   13809 atomic_writer.go:341] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: new paths:       [ca.crt namespace token]
I1207 08:38:57.375741   13809 atomic_writer.go:344] /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg: paths to remove: map[]
I1207 08:38:57.375762   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume mdsd-data: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/mdsd-data
I1207 08:38:57.375777   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "mdsd-data" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-mdsd-data") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:38:57.375819   13809 atomic_writer.go:142] pod caas-20d222f17dd4489fb180b51031b406f6/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd volume default-token-74mlg: no update required for target directory /var/lib/kubelet/pods/7c988aa2-5737-11ec-9d1e-002248057bde/volumes/kubernetes.io~secret/default-token-74mlg
I1207 08:38:57.375872   13809 operation_generator.go:484] MountVolume.SetUp succeeded for volume "default-token-74mlg" (UniqueName: "kubernetes.io/secret/7c988aa2-5737-11ec-9d1e-002248057bde-default-token-74mlg") pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd" (UID: "7c988aa2-5737-11ec-9d1e-002248057bde") 
I1207 08:38:57.427754   13809 generic.go:182] GenericPLEG: Relisting
I1207 08:38:57.651719   13809 volume_manager.go:366] All volumes are attached and mounted for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
I1207 08:38:57.651765   13809 kuberuntime_manager.go:428] Syncing Pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)": &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,GenerateName:,Namespace:caas-20d222f17dd4489fb180b51031b406f6,SelfLink:/api/v1/namespaces/caas-20d222f17dd4489fb180b51031b406f6/pods/shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd,UID:7c988aa2-5737-11ec-9d1e-002248057bde,ResourceVersion:1094484050,Generation:0,CreationTimestamp:2021-12-07 08:27:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agent-image-hash: 094a04102a9c1e599c0d8bfcc0d4a5cd,deploymenttype: system,sa-pod-type: shareloggingagent-linux,shareloggingagent: true,},Annotations:map[string]string{agent-image: mcr.microsoft.com/aci/shareloggingagent:master_20201012.1,kubernetes.io/config.seen: 2021-12-07T08:34:29.329988082Z,kubernetes.io/config.source: api,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{logfiles-containers-volume {HostPathVolumeSource{Path:/var/log/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-pods-volume {&HostPathVolumeSource{Path:/var/log/pods,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {logfiles-docker-volume {&HostPathVolumeSource{Path:/var/lib/docker/containers,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-data {nil nil nil nil nil &SecretVolumeSource{SecretName:mdsd-config,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {mdsd-logs {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {kubeconfig {&HostPathVolumeSource{Path:/var/lib/kubelet,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {certificates {&HostPathVolumeSource{Path:/etc/kubernetes/certs,Type:*,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {default-token-74mlg {nil nil nil nil nil &SecretVolumeSource{SecretName:default-token-74mlg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{logger mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 [/bin/sh -c ./shareLoggingAgent -kubecfg=/var/lib/kubelet/kubeconfig  cp /tmp/geneva_mdsd/mdsd.xml /tmp/geneva_logs && ./start_mdsd.sh] []  [] [] [{CLUSTER_NODE_NAME k8s-agentpool4-a14krc0l-17 nil} {POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {TENANT caas-prod-koreacentral-linux-14 nil} {ROLE shareloggingagent nil} {ROLEINSTANCE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MONITORING_GCS_ENVIRONMENT DiagnosticsProd nil} {MONITORING_GCS_ACCOUNT ContainerInstanceGSM nil} {MONITORING_GCS_REGION koreacentral nil} {MACHINENAME  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {MDSD_CONFIG_DIR /tmp/geneva_logs nil} {MDSD_LOG_DIR /tmp/geneva_logs nil} {MDSD_AUTH_DIR /tmp/geneva_mdsd nil} {MONITORING_GCS_CERT_CERTFILE /tmp/geneva_mdsd/gcscert.pem nil} {MONITORING_GCS_CERT_KEYFILE /tmp/geneva_mdsd/gcskey.pem nil} {MDSD_DJSON_ACK 0 nil}] {map[cpu:{{200 -3} {<nil>} 200m DecimalSI} memory:{{1 9} {<nil>} 1G DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI} memory:{{0 0} {<nil>} 0 DecimalSI}]} [{kubeconfig false /var/lib/kubelet  <nil>} {certificates false /etc/kubernetes/certs  <nil>} {logfiles-containers-volume false /var/log/containers  <nil>} {logfiles-pods-volume false /var/log/pods  <nil>} {logfiles-docker-volume false /var/lib/docker/containers  <nil>} {mdsd-data false /tmp/geneva_mdsd  <nil>} {mdsd-logs false /tmp/geneva_logs  <nil>} {default-token-74mlg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] nil nil nil /dev/termination-log File Always nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:Default,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-agentpool4-a14krc0l-17,HostNetwork:true,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:34:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 08:27:16 +0000 UTC  }],Message:,Reason:,HostIP:10.240.67.5,PodIP:10.240.67.5,StartTime:2021-12-07 08:27:16 +0000 UTC,ContainerStatuses:[{logger {nil ContainerStateRunning{StartedAt:2021-12-07 08:27:23 +0000 UTC,} nil} {nil nil nil} true 0 mcr.microsoft.com/aci/shareloggingagent:master_20201012.1 docker-pullable://mcr.microsoft.com/aci/shareloggingagent@sha256:3453a50851d1828547c446517a8c64d4499ccf62ec13d424e7d3928e1e0399cd docker://74739cf5699ce530c7f84ba24cab4c0c08ad0be1cb818e487f16237165d4263c}],QOSClass:Burstable,InitContainerStatuses:[],},}
I1207 08:38:57.652681   13809 kuberuntime_manager.go:557] computePodActions got {KillPod:false CreateSandbox:false SandboxID:b78c2424978891792ba2da922f96d36fe7d3fcaa3e29f046439e4440f0cd895c Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "shareloggingagent-094a04102a9c1e599c0d8bfcc0d4a5cd_caas-20d222f17dd4489fb180b51031b406f6(7c988aa2-5737-11ec-9d1e-002248057bde)"
